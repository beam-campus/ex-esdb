searchData={"items":[{"type":"module","doc":"ExESDB is a wrapper around the khepri library. \n  Its intention is to provide an interface to khepri,\n  with a focus on event sourcing.","title":"ExESDB","ref":"ExESDB.html"},{"type":"module","doc":"Aggregates events from an event stream using tagged rules:\n  GIVEN: an Event of roughly this format:\n   %{\n    event_id: \"1234567890\",\n    event_type: \"user.birthday_celebrated:v1\",\n    stream_id: \"celebrate-user-birthday-john\",\n    version: 1,\n    data: %{\n      name: \"John\",\n      age: {:sum, 1},\n      venue: {:overwrite, \"New York\"}\n    },\n    timestamp: ~U[2022-01-01 12:00:00Z],\n    epoch: 1641013200,\n    metadata: %{\n      source_id: \"1234567890\"\n    }\n  }","title":"ExESDB.Aggregator","ref":"ExESDB.Aggregator.html"},{"type":"function","doc":"","title":"ExESDB.Aggregator.finalize_map/1","ref":"ExESDB.Aggregator.html#finalize_map/1"},{"type":"function","doc":"Folds a list of events into a single map.","title":"ExESDB.Aggregator.foldl/2","ref":"ExESDB.Aggregator.html#foldl/2"},{"type":"module","doc":"Supervisor for cluster coordination components.\n\nThis supervisor manages cluster-specific coordination components:\n- ClusterCoordinator: Handles coordination logic and split-brain prevention\n- NodeMonitor: Monitors node health and handles failures\n\nNote: KhepriCluster is managed at the System level since it's mode-aware.","title":"ExESDB.ClusterSystem","ref":"ExESDB.ClusterSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.ClusterSystem.child_spec/1","ref":"ExESDB.ClusterSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.ClusterSystem.start_link/1","ref":"ExESDB.ClusterSystem.html#start_link/1"},{"type":"module","doc":"Provides tools for verifying consistency across ExESDB cluster stores.\n\nThis module leverages Khepri and Ra APIs to verify that stores are consistent\nacross cluster nodes, detect split-brain scenarios, and ensure data integrity.","title":"ExESDB.ConsistencyChecker","ref":"ExESDB.ConsistencyChecker.html"},{"type":"function","doc":"Checks Raft log consistency across cluster members.\nThis is a more intensive check that examines log indices and terms.","title":"ExESDB.ConsistencyChecker.check_raft_log_consistency/1","ref":"ExESDB.ConsistencyChecker.html#check_raft_log_consistency/1"},{"type":"type","doc":"","title":"ExESDB.ConsistencyChecker.check_result/0","ref":"ExESDB.ConsistencyChecker.html#t:check_result/0"},{"type":"type","doc":"","title":"ExESDB.ConsistencyChecker.consistency_result/0","ref":"ExESDB.ConsistencyChecker.html#t:consistency_result/0"},{"type":"type","doc":"","title":"ExESDB.ConsistencyChecker.node_name/0","ref":"ExESDB.ConsistencyChecker.html#t:node_name/0"},{"type":"function","doc":"Quick health check to verify if a store is accessible and responsive across nodes.","title":"ExESDB.ConsistencyChecker.quick_health_check/1","ref":"ExESDB.ConsistencyChecker.html#quick_health_check/1"},{"type":"function","doc":"Monitors consistency over time and reports any deviations.","title":"ExESDB.ConsistencyChecker.start_consistency_monitoring/2","ref":"ExESDB.ConsistencyChecker.html#start_consistency_monitoring/2"},{"type":"type","doc":"","title":"ExESDB.ConsistencyChecker.store_id/0","ref":"ExESDB.ConsistencyChecker.html#t:store_id/0"},{"type":"function","doc":"Performs a comprehensive consistency check across all cluster nodes for a given store.","title":"ExESDB.ConsistencyChecker.verify_cluster_consistency/1","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1"},{"type":"function","doc":"- `{:ok, report}` - Detailed consistency report\n- `{:error, reason}` - Error occurred during check","title":"Returns - ExESDB.ConsistencyChecker.verify_cluster_consistency/1","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1-returns"},{"type":"function","doc":"iex> ExESDB.ConsistencyChecker.verify_cluster_consistency(:my_store)\n    {:ok, %{\n      status: :consistent,\n      nodes_checked: 3,\n      leader: :\"node1@host\",\n      members: [:\"node1@host\", :\"node2@host\", :\"node3@host\"],\n      raft_status: :healthy,\n      potential_issues: []\n    }}","title":"Example - ExESDB.ConsistencyChecker.verify_cluster_consistency/1","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1-example"},{"type":"function","doc":"Verifies that all nodes agree on cluster membership.","title":"ExESDB.ConsistencyChecker.verify_membership_consensus/1","ref":"ExESDB.ConsistencyChecker.html#verify_membership_consensus/1"},{"type":"module","doc":"Critical infrastructure supervisor that manages core ExESDB components.\n\nThis supervisor uses :one_for_all strategy because these components are \ntightly coupled and must restart together to maintain consistency.\n\nStartup order:\n1. PersistenceSystem: Manages streams, snapshots, and subscriptions (foundation)\n2. NotificationSystem: Manages leadership and event emission (depends on persistence)\n3. StoreSystem: Manages store lifecycle and clustering (depends on persistence & notification)\n\nThe NotificationSystem includes:\n- LeaderSystem: Leadership responsibilities and subscription management\n- EmitterSystem: Event emission and distribution\n\nThis ensures that leadership and event distribution are core capabilities\navailable in both single-node and cluster modes.","title":"ExESDB.CoreSystem","ref":"ExESDB.CoreSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.CoreSystem.child_spec/1","ref":"ExESDB.CoreSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.CoreSystem.start_link/1","ref":"ExESDB.CoreSystem.html#start_link/1"},{"type":"module","doc":"Comprehensive debugging and inspection API for ExESDB systems.\n\nThis module provides REPL-friendly functions to investigate all aspects\nof an ExESDB Event Sourcing Database by delegating to specialized subsystems:\n\n- Process supervision tree inspection (Inspector)\n- Health and performance monitoring (Monitor)\n- Scenario testing and simulation (ScenarioManager)\n- Real-time observation (Observer)","title":"ExESDB.Debugger","ref":"ExESDB.Debugger.html"},{"type":"module","doc":"iex> ExESDB.Debugger.overview()\n    iex> ExESDB.Debugger.start_scenario(:high_load, intensity: :medium)\n    iex> ExESDB.Debugger.observe(:system_metrics)","title":"Usage in REPL - ExESDB.Debugger","ref":"ExESDB.Debugger.html#module-usage-in-repl"},{"type":"function","doc":"Benchmark a function.","title":"ExESDB.Debugger.benchmark/2","ref":"ExESDB.Debugger.html#benchmark/2"},{"type":"function","doc":"Show detailed configuration for the store.","title":"ExESDB.Debugger.config/1","ref":"ExESDB.Debugger.html#config/1"},{"type":"function","doc":"Perform comprehensive health check.","title":"ExESDB.Debugger.health/1","ref":"ExESDB.Debugger.html#health/1"},{"type":"function","doc":"Show help information for all available debugging commands.","title":"ExESDB.Debugger.help/0","ref":"ExESDB.Debugger.html#help/0"},{"type":"function","doc":"List all active observations.","title":"ExESDB.Debugger.list_observations/0","ref":"ExESDB.Debugger.html#list_observations/0"},{"type":"function","doc":"List all running scenarios.","title":"ExESDB.Debugger.list_scenarios/0","ref":"ExESDB.Debugger.html#list_scenarios/0"},{"type":"function","doc":"Get observation data for analysis.","title":"ExESDB.Debugger.observation_data/1","ref":"ExESDB.Debugger.html#observation_data/1"},{"type":"function","doc":"Start real-time observation of system components.\n\nAvailable targets:\n- `:system_metrics` - Overall system metrics\n- `:process_metrics` - Specific process metrics (requires :target_process opt)\n- `:memory_usage` - Memory usage patterns","title":"ExESDB.Debugger.observe/2","ref":"ExESDB.Debugger.html#observe/2"},{"type":"function","doc":"ExESDB.Debugger.observe(:system_metrics, interval: 1000)\n    ExESDB.Debugger.observe(:process_metrics, target_process: :my_process, interval: 2000)","title":"Examples - ExESDB.Debugger.observe/2","ref":"ExESDB.Debugger.html#observe/2-examples"},{"type":"function","doc":"Display a comprehensive overview of the ExESDB system.","title":"ExESDB.Debugger.overview/1","ref":"ExESDB.Debugger.html#overview/1"},{"type":"function","doc":"Show performance metrics.","title":"ExESDB.Debugger.performance/1","ref":"ExESDB.Debugger.html#performance/1"},{"type":"function","doc":"List all ExESDB-related processes with formatted output.","title":"ExESDB.Debugger.processes/1","ref":"ExESDB.Debugger.html#processes/1"},{"type":"function","doc":"Start a test scenario.\n\nAvailable scenarios:\n- `:high_load` - Simulate high CPU/memory load\n- `:node_failure` - Simulate node failures\n- `:custom` - Load custom scenario from config","title":"ExESDB.Debugger.start_scenario/2","ref":"ExESDB.Debugger.html#start_scenario/2"},{"type":"function","doc":"ExESDB.Debugger.start_scenario(:high_load, intensity: :medium, duration: 30_000)\n    ExESDB.Debugger.start_scenario(:custom, config_path: \"scenarios/custom.json\")","title":"Examples - ExESDB.Debugger.start_scenario/2","ref":"ExESDB.Debugger.html#start_scenario/2-examples"},{"type":"function","doc":"Stop a running observation.","title":"ExESDB.Debugger.stop_observation/1","ref":"ExESDB.Debugger.html#stop_observation/1"},{"type":"function","doc":"Stop a running scenario.","title":"ExESDB.Debugger.stop_scenario/1","ref":"ExESDB.Debugger.html#stop_scenario/1"},{"type":"function","doc":"Display the supervision tree for ExESDB.","title":"ExESDB.Debugger.supervision_tree/1","ref":"ExESDB.Debugger.html#supervision_tree/1"},{"type":"function","doc":"Show top processes by memory/CPU usage.","title":"ExESDB.Debugger.top/1","ref":"ExESDB.Debugger.html#top/1"},{"type":"function","doc":"Trace function calls for debugging.","title":"ExESDB.Debugger.trace/3","ref":"ExESDB.Debugger.html#trace/3"},{"type":"module","doc":"Supervisor for the ExESDB Debugger system.\n\nManages all debugger subsystems including inspection, monitoring,\nscenario management, and observation.","title":"ExESDB.DebuggerSystem","ref":"ExESDB.DebuggerSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.DebuggerSystem.child_spec/1","ref":"ExESDB.DebuggerSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.DebuggerSystem.start_link/1","ref":"ExESDB.DebuggerSystem.html#start_link/1"},{"type":"module","doc":"Logging worker responsible for handling EmitterPool logging events.\n\nThis worker subscribes to emitter_pool logging events and processes them\naccording to configured logging policies.","title":"ExESDB.EmitterPoolLoggingWorker","ref":"ExESDB.EmitterPoolLoggingWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EmitterPoolLoggingWorker.child_spec/1","ref":"ExESDB.EmitterPoolLoggingWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EmitterPoolLoggingWorker.start_link/1","ref":"ExESDB.EmitterPoolLoggingWorker.html#start_link/1"},{"type":"module","doc":"Supervisor for event emission components.\n\nThis supervisor manages the emitter pools that handle event distribution\nto subscribers. Only active when this node is the cluster leader.\n\nComponents:\n- EmitterPools: PartitionSupervisor managing dynamic emitter pools","title":"ExESDB.EmitterSystem","ref":"ExESDB.EmitterSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EmitterSystem.child_spec/1","ref":"ExESDB.EmitterSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EmitterSystem.start_link/1","ref":"ExESDB.EmitterSystem.html#start_link/1"},{"type":"module","doc":"Logging worker responsible for handling EmitterSystem logging events.\n\nThis worker subscribes to emitter_system logging events and processes them\naccording to configured logging policies.","title":"ExESDB.EmitterSystemLoggingWorker","ref":"ExESDB.EmitterSystemLoggingWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EmitterSystemLoggingWorker.child_spec/1","ref":"ExESDB.EmitterSystemLoggingWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EmitterSystemLoggingWorker.start_link/1","ref":"ExESDB.EmitterSystemLoggingWorker.html#start_link/1"},{"type":"module","doc":"As part of the ExESDB.System, \n  the EmitterWorker is responsible for managing the communication \n  between the Event Store and the PubSub mechanism.","title":"ExESDB.EmitterWorker","ref":"ExESDB.EmitterWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EmitterWorker.child_spec/1","ref":"ExESDB.EmitterWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EmitterWorker.start_link/1","ref":"ExESDB.EmitterWorker.html#start_link/1"},{"type":"module","doc":"Logging worker responsible for handling EmitterWorker logging events.\n\nThis worker subscribes to emitter_worker logging events and processes them\naccording to configured logging policies. Since EmitterWorkers can be very\nchatty, this worker provides more fine-grained control over what gets logged.","title":"ExESDB.EmitterWorkerLoggingWorker","ref":"ExESDB.EmitterWorkerLoggingWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EmitterWorkerLoggingWorker.child_spec/1","ref":"ExESDB.EmitterWorkerLoggingWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EmitterWorkerLoggingWorker.start_link/1","ref":"ExESDB.EmitterWorkerLoggingWorker.html#start_link/1"},{"type":"module","doc":"As part of the ExESDB.System, ExESDB.Emitters is responsible for managing the\n  lifetime of the Emitter processes.","title":"ExESDB.Emitters","ref":"ExESDB.Emitters.html"},{"type":"function","doc":"","title":"ExESDB.Emitters.start_emitter_pool/3","ref":"ExESDB.Emitters.html#start_emitter_pool/3"},{"type":"function","doc":"Stops an EmitterPool for a given subscription.","title":"ExESDB.Emitters.stop_emitter_pool/2","ref":"ExESDB.Emitters.html#stop_emitter_pool/2"},{"type":"function","doc":"Updates an EmitterPool with new subscription data.\nThis is typically called when the subscriber PID changes.","title":"ExESDB.Emitters.update_emitter_pool/2","ref":"ExESDB.Emitters.html#update_emitter_pool/2"},{"type":"module","doc":"This module contains the environment variables that are used by ExESDB","title":"ExESDB.EnVars","ref":"ExESDB.EnVars.html"},{"type":"function","doc":"","title":"ExESDB.EnVars.cluster_secret/0","ref":"ExESDB.EnVars.html#cluster_secret/0"},{"type":"function","doc":"Returns the data directory. default: `/data`","title":"ExESDB.EnVars.data_dir/0","ref":"ExESDB.EnVars.html#data_dir/0"},{"type":"function","doc":"Returns the db type. `single` or `cluster`. default: `single`","title":"ExESDB.EnVars.db_type/0","ref":"ExESDB.EnVars.html#db_type/0"},{"type":"function","doc":"Returns the gossip multicast address. default: `255.255.255.255`","title":"ExESDB.EnVars.gossip_multicast_addr/0","ref":"ExESDB.EnVars.html#gossip_multicast_addr/0"},{"type":"function","doc":"Returns whether persistence is enabled. default: `true`","title":"ExESDB.EnVars.persistence_enabled/0","ref":"ExESDB.EnVars.html#persistence_enabled/0"},{"type":"function","doc":"Returns the persistence interval in milliseconds. default: `5_000`","title":"ExESDB.EnVars.persistence_interval/0","ref":"ExESDB.EnVars.html#persistence_interval/0"},{"type":"function","doc":"Returns the idle readers timeout in milliseconds. default: `10_000`","title":"ExESDB.EnVars.reader_idle_ms/0","ref":"ExESDB.EnVars.html#reader_idle_ms/0"},{"type":"function","doc":"Returns the store description. default: `nil`","title":"ExESDB.EnVars.store_description/0","ref":"ExESDB.EnVars.html#store_description/0"},{"type":"function","doc":"Returns the khepri store id. default: `ex_esdb_store`","title":"ExESDB.EnVars.store_id/0","ref":"ExESDB.EnVars.html#store_id/0"},{"type":"function","doc":"Returns the store tags as comma-separated values. default: `nil`","title":"ExESDB.EnVars.store_tags/0","ref":"ExESDB.EnVars.html#store_tags/0"},{"type":"function","doc":"Returns the timeout in milliseconds. default: `10_000`","title":"ExESDB.EnVars.timeout/0","ref":"ExESDB.EnVars.html#timeout/0"},{"type":"function","doc":"Returns the idle writers timeout in milliseconds. default: `10_000`","title":"ExESDB.EnVars.writer_idle_ms/0","ref":"ExESDB.EnVars.html#writer_idle_ms/0"},{"type":"module","doc":"This module contains the event projector functionality","title":"ExESDB.EventProjector","ref":"ExESDB.EventProjector.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.EventProjector.child_spec/1","ref":"ExESDB.EventProjector.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.EventProjector.start_link/1","ref":"ExESDB.EventProjector.html#start_link/1"},{"type":"module","doc":"Event definitions and schemas for ExESDB subsystems.\n\nThis module provides centralized event type definitions and helper functions\nfor building event payloads. Each module is responsible for publishing its\nown events using Phoenix.PubSub directly.","title":"ExESDB.Events","ref":"ExESDB.Events.html"},{"type":"module","doc":"- `:system` - System lifecycle and initialization events\n- `:cluster` - Cluster membership and coordination events\n- `:leadership` - Leadership changes and responsibilities\n- `:persistence` - Data persistence and storage events\n- `:gateway` - External interface and API events\n- `:coordination` - General coordination events\n- `:subscriptions` - Subscription management events","title":"Event Categories - ExESDB.Events","ref":"ExESDB.Events.html#module-event-categories"},{"type":"module","doc":"```elixir\n# Each module publishes its own events:\n\n# In PersistenceWorker:\nevent = Events.build_event(:events_persisted, %{stream_id: \"orders\", event_count: 5})\nPhoenix.PubSub.broadcast(ExESDB.PubSub, \"exesdb:control:store_id:persistence\", \n  {:persistence_event, event})\n\n# In StoreCluster:\nevent = Events.build_event(:cluster_joined, %{via_node: target_node})\nPhoenix.PubSub.broadcast(ExESDB.PubSub, \"exesdb:control:store_id:cluster\", \n  {:cluster_event, event})\n```","title":"Usage - ExESDB.Events","ref":"ExESDB.Events.html#module-usage"},{"type":"function","doc":"Returns all event types organized by category","title":"ExESDB.Events.all_events/0","ref":"ExESDB.Events.html#all_events/0"},{"type":"function","doc":"Builds a standard event map with common fields.","title":"ExESDB.Events.build_event/3","ref":"ExESDB.Events.html#build_event/3"},{"type":"function","doc":"- `event_type` - The event type (atom)\n- `data` - Event-specific data (map)\n- `opts` - Optional fields like `:store_id` (keyword list)","title":"Parameters - ExESDB.Events.build_event/3","ref":"ExESDB.Events.html#build_event/3-parameters"},{"type":"function","doc":"Events.build_event(:cluster_joined, %{via_node: node1})\n    Events.build_event(:events_persisted, %{stream_id: \"orders\", count: 5}, store_id: :my_store)","title":"Examples - ExESDB.Events.build_event/3","ref":"ExESDB.Events.html#build_event/3-examples"},{"type":"function","doc":"Helper to build event payload with standardized event wrapper.","title":"ExESDB.Events.build_payload/4","ref":"ExESDB.Events.html#build_payload/4"},{"type":"function","doc":"Events.build_payload(:cluster_event, :cluster_joined, %{via_node: node1})\n    # => {:cluster_event, %{event_type: :cluster_joined, ...}}","title":"Examples - ExESDB.Events.build_payload/4","ref":"ExESDB.Events.html#build_payload/4-examples"},{"type":"function","doc":"Builds a topic string for a given store and category.","title":"ExESDB.Events.build_topic/2","ref":"ExESDB.Events.html#build_topic/2"},{"type":"function","doc":"Events.build_topic(:my_store, :cluster)\n    # => \"exesdb:control:my_store:cluster\"","title":"Examples - ExESDB.Events.build_topic/2","ref":"ExESDB.Events.html#build_topic/2-examples"},{"type":"function","doc":"Returns all defined cluster events","title":"ExESDB.Events.cluster_events/0","ref":"ExESDB.Events.html#cluster_events/0"},{"type":"function","doc":"Returns all defined coordination events","title":"ExESDB.Events.coordination_events/0","ref":"ExESDB.Events.html#coordination_events/0"},{"type":"function","doc":"Returns all defined gateway events","title":"ExESDB.Events.gateway_events/0","ref":"ExESDB.Events.html#gateway_events/0"},{"type":"function","doc":"Returns all defined leadership events","title":"ExESDB.Events.leadership_events/0","ref":"ExESDB.Events.html#leadership_events/0"},{"type":"function","doc":"Returns all defined persistence events","title":"ExESDB.Events.persistence_events/0","ref":"ExESDB.Events.html#persistence_events/0"},{"type":"function","doc":"Returns all defined subscription events","title":"ExESDB.Events.subscription_events/0","ref":"ExESDB.Events.html#subscription_events/0"},{"type":"function","doc":"Returns all defined system events","title":"ExESDB.Events.system_events/0","ref":"ExESDB.Events.html#system_events/0"},{"type":"function","doc":"Checks if an event type is valid for a given category","title":"ExESDB.Events.valid_event?/2","ref":"ExESDB.Events.html#valid_event?/2"},{"type":"function","doc":"Validates that an event has all required fields","title":"ExESDB.Events.valid_event_structure?/1","ref":"ExESDB.Events.html#valid_event_structure?/1"},{"type":"module","doc":"@deprecated \"Use ExESDB.GatewaySystem instead\"\n  \n  The GatewaySupervisor is responsible for starting and supervising the\n  GatewayWorkers.\n  \n  This module is deprecated in favor of ExESDB.GatewaySystem which provides\n  improved fault tolerance with pooled workers.","title":"ExESDB.GatewaySupervisor","ref":"ExESDB.GatewaySupervisor.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.GatewaySupervisor.child_spec/1","ref":"ExESDB.GatewaySupervisor.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.GatewaySupervisor.start_link/1","ref":"ExESDB.GatewaySupervisor.html#start_link/1"},{"type":"module","doc":"Supervisor for gateway components providing external interface.\n\nThis supervisor manages a pool of gateway workers for high availability\nand load distribution.\n\nComponents:\n- GatewayWorkers: Pool of GatewayWorkers via PartitionSupervisor\n# PubSub: External communication now handled externally","title":"ExESDB.GatewaySystem","ref":"ExESDB.GatewaySystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.GatewaySystem.child_spec/1","ref":"ExESDB.GatewaySystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.GatewaySystem.start_link/1","ref":"ExESDB.GatewaySystem.html#start_link/1"},{"type":"module","doc":"GatewayWorker processes are started on each node in the cluster,\n  and contain the implementation functions for the Gater.API.","title":"ExESDB.GatewayWorker","ref":"ExESDB.GatewayWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.GatewayWorker.child_spec/1","ref":"ExESDB.GatewayWorker.html#child_spec/1"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.error/0","ref":"ExESDB.GatewayWorker.html#t:error/0"},{"type":"function","doc":"","title":"ExESDB.GatewayWorker.gateway_worker_name/1","ref":"ExESDB.GatewayWorker.html#gateway_worker_name/1"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.selector_type/0","ref":"ExESDB.GatewayWorker.html#t:selector_type/0"},{"type":"function","doc":"","title":"ExESDB.GatewayWorker.start_link/1","ref":"ExESDB.GatewayWorker.html#start_link/1"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.store/0","ref":"ExESDB.GatewayWorker.html#t:store/0"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.stream/0","ref":"ExESDB.GatewayWorker.html#t:stream/0"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.subscription_name/0","ref":"ExESDB.GatewayWorker.html#t:subscription_name/0"},{"type":"type","doc":"","title":"ExESDB.GatewayWorker.subscription_type/0","ref":"ExESDB.GatewayWorker.html#t:subscription_type/0"},{"type":"module","doc":"Provides functions for distributing load over the cluster.","title":"ExESDB.HashRing","ref":"ExESDB.HashRing.html"},{"type":"function","doc":"","title":"ExESDB.HashRing.get_core_for_stream/1","ref":"ExESDB.HashRing.html#get_core_for_stream/1"},{"type":"function","doc":"","title":"ExESDB.HashRing.get_node_for_stream/1","ref":"ExESDB.HashRing.html#get_node_for_stream/1"},{"type":"module","doc":"Inspects configuration settings for the ExESDB system.\n\nProvides functionality to retrieve and analyze system configuration\nacross different stores and environments.","title":"ExESDB.Inspection.ConfigInspector","ref":"ExESDB.Inspection.ConfigInspector.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Inspection.ConfigInspector.child_spec/1","ref":"ExESDB.Inspection.ConfigInspector.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Inspection.ConfigInspector.get_config/1","ref":"ExESDB.Inspection.ConfigInspector.html#get_config/1"},{"type":"function","doc":"","title":"ExESDB.Inspection.ConfigInspector.get_environment_config/0","ref":"ExESDB.Inspection.ConfigInspector.html#get_environment_config/0"},{"type":"function","doc":"","title":"ExESDB.Inspection.ConfigInspector.start_link/1","ref":"ExESDB.Inspection.ConfigInspector.html#start_link/1"},{"type":"module","doc":"Inspects processes related to the ExESDB system.\n\nProvides functionality to list, categorize, and analyze processes\nfor health monitoring and debugging.","title":"ExESDB.Inspection.ProcessInspector","ref":"ExESDB.Inspection.ProcessInspector.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Inspection.ProcessInspector.child_spec/1","ref":"ExESDB.Inspection.ProcessInspector.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Inspection.ProcessInspector.list_processes/0","ref":"ExESDB.Inspection.ProcessInspector.html#list_processes/0"},{"type":"function","doc":"","title":"ExESDB.Inspection.ProcessInspector.start_link/1","ref":"ExESDB.Inspection.ProcessInspector.html#start_link/1"},{"type":"module","doc":"Inspects the supervision tree of the ExESDB system.\n\nProvides visualization and analysis of the system's supervision\nhierarchy to aid in debugging and monitoring.","title":"ExESDB.Inspection.TreeInspector","ref":"ExESDB.Inspection.TreeInspector.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Inspection.TreeInspector.child_spec/1","ref":"ExESDB.Inspection.TreeInspector.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Inspection.TreeInspector.start_link/1","ref":"ExESDB.Inspection.TreeInspector.html#start_link/1"},{"type":"function","doc":"","title":"ExESDB.Inspection.TreeInspector.view_supervision_tree/1","ref":"ExESDB.Inspection.TreeInspector.html#view_supervision_tree/1"},{"type":"module","doc":"Supervisor for the Inspection subsystem.\n\nManages all process, configuration, and supervision tree inspection components.","title":"ExESDB.InspectionSystem","ref":"ExESDB.InspectionSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.InspectionSystem.child_spec/1","ref":"ExESDB.InspectionSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.InspectionSystem.start_link/1","ref":"ExESDB.InspectionSystem.html#start_link/1"},{"type":"module","doc":"This module supervises the Leader Subsystem.","title":"ExESDB.LeaderSystem","ref":"ExESDB.LeaderSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.LeaderSystem.child_spec/1","ref":"ExESDB.LeaderSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.LeaderSystem.start_link/1","ref":"ExESDB.LeaderSystem.html#start_link/1"},{"type":"module","doc":"As part of the ExESDB.System, the SubscriptionsTracker is responsible for\n  observing the subscriptions that are maintained in the Store.\n\n  Since Khepri triggers are executed on the leader node, the SubscriptionsTracker\n  will be instructed to start the Emitters system on the leader node whenever a new subscription\n  is registered.\n\n  When a Subscription is deleted, the SubscriptionsTracker will instruct the Emitters system to stop \n  the associated EmitterPool.","title":"ExESDB.LeaderTracker","ref":"ExESDB.LeaderTracker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.LeaderTracker.child_spec/1","ref":"ExESDB.LeaderTracker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.LeaderTracker.start_link/1","ref":"ExESDB.LeaderTracker.html#start_link/1"},{"type":"module","doc":"This module contains the leader's reponsibilities for the cluster.","title":"ExESDB.LeaderWorker","ref":"ExESDB.LeaderWorker.html"},{"type":"function","doc":"Activates the LeaderWorker for the given store.\n\nThis function is called when this node becomes the cluster leader.","title":"ExESDB.LeaderWorker.activate/1","ref":"ExESDB.LeaderWorker.html#activate/1"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.LeaderWorker.child_spec/1","ref":"ExESDB.LeaderWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.LeaderWorker.start_link/1","ref":"ExESDB.LeaderWorker.html#start_link/1"},{"type":"module","doc":"Custom logger filters to reduce noise from Ra, Khepri, and other verbose components","title":"ExESDB.LoggerFilters","ref":"ExESDB.LoggerFilters.html"},{"type":"function","doc":"Filter Khepri noise - reduce verbose operational messages","title":"ExESDB.LoggerFilters.filter_khepri/1","ref":"ExESDB.LoggerFilters.html#filter_khepri/1"},{"type":"function","doc":"Filter libcluster noise - reduce cluster formation chatter","title":"ExESDB.LoggerFilters.filter_libcluster/1","ref":"ExESDB.LoggerFilters.html#filter_libcluster/1"},{"type":"function","doc":"Filter Ra consensus library noise - only show errors and warnings","title":"ExESDB.LoggerFilters.filter_ra/1","ref":"ExESDB.LoggerFilters.html#filter_ra/1"},{"type":"function","doc":"Filter Swarm noise - already provided by BCUtils but adding our own","title":"ExESDB.LoggerFilters.filter_swarm/1","ref":"ExESDB.LoggerFilters.html#filter_swarm/1"},{"type":"module","doc":"LoggerWorker subscribes to events for a specific subsystem and logs events for monitoring and debugging.\n\nEach subsystem should supervise its own LoggerWorker to provide dedicated logging\nfor that subsystem's events and operations.","title":"ExESDB.LoggerWorker","ref":"ExESDB.LoggerWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.LoggerWorker.child_spec/1","ref":"ExESDB.LoggerWorker.html#child_spec/1"},{"type":"function","doc":"Starts the LoggerWorker for a specific subsystem.","title":"ExESDB.LoggerWorker.start_link/1","ref":"ExESDB.LoggerWorker.html#start_link/1"},{"type":"function","doc":"- opts: Configuration options including store_id and subsystem_name","title":"Parameters - ExESDB.LoggerWorker.start_link/1","ref":"ExESDB.LoggerWorker.html#start_link/1-parameters"},{"type":"module","doc":"Helper module for publishing structured logging events to the :ex_esdb_logging PubSub topic.\n\nThis module provides a clean API for components to publish logging events instead of\nusing direct terminal output.","title":"ExESDB.LoggingPublisher","ref":"ExESDB.LoggingPublisher.html"},{"type":"function","doc":"","title":"ExESDB.LoggingPublisher.action/4","ref":"ExESDB.LoggingPublisher.html#action/4"},{"type":"function","doc":"","title":"ExESDB.LoggingPublisher.error/4","ref":"ExESDB.LoggingPublisher.html#error/4"},{"type":"function","doc":"","title":"ExESDB.LoggingPublisher.health/4","ref":"ExESDB.LoggingPublisher.html#health/4"},{"type":"function","doc":"Publishes a logging event to the appropriate topic.","title":"ExESDB.LoggingPublisher.publish/5","ref":"ExESDB.LoggingPublisher.html#publish/5"},{"type":"function","doc":"- `component`: The component type (e.g., :emitter_pool, :emitter_worker, :emitter_system)\n- `event_type`: The type of event (e.g., :startup, :shutdown, :action, :health, :error)\n- `store_id`: The store identifier\n- `message`: The log message\n- `metadata`: Additional metadata (default: %{})","title":"Parameters - ExESDB.LoggingPublisher.publish/5","ref":"ExESDB.LoggingPublisher.html#publish/5-parameters"},{"type":"function","doc":"iex> LoggingPublisher.publish(:emitter_pool, :startup, :my_store, \"Pool started\", %{pool_size: 3})\n    :ok\n    \n    iex> LoggingPublisher.publish(:emitter_worker, :action, :my_store, \"Processing event\", %{event_id: \"123\"})\n    :ok","title":"Examples - ExESDB.LoggingPublisher.publish/5","ref":"ExESDB.LoggingPublisher.html#publish/5-examples"},{"type":"function","doc":"","title":"ExESDB.LoggingPublisher.shutdown/4","ref":"ExESDB.LoggingPublisher.html#shutdown/4"},{"type":"function","doc":"Convenience functions for common logging scenarios","title":"ExESDB.LoggingPublisher.startup/4","ref":"ExESDB.LoggingPublisher.html#startup/4"},{"type":"module","doc":"LoggingSystem supervisor that manages individual logging workers for different ExESDB components.\n\nThis system replaces direct terminal output with PubSub-based logging events that can be\nconsumed by various logging workers and processed appropriately.\n\nThe system publishes logging events to the :ex_esdb_logging PubSub topic with the following structure:\n\n```elixir\n%{\n  component: :emitter_pool | :emitter_worker | :emitter_system,\n  event_type: :startup | :shutdown | :action | :health | :error,\n  store_id: atom(),\n  pid: pid(),\n  timestamp: DateTime.t(),\n  message: String.t(),\n  metadata: map()\n}\n```","title":"ExESDB.LoggingSystem","ref":"ExESDB.LoggingSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.LoggingSystem.child_spec/1","ref":"ExESDB.LoggingSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.LoggingSystem.start_link/1","ref":"ExESDB.LoggingSystem.html#start_link/1"},{"type":"module","doc":"Provides metrics for the event store.","title":"ExESDB.Metrics","ref":"ExESDB.Metrics.html"},{"type":"module","doc":"Performs comprehensive health checks on the ExESDB system.\n\nMonitors system processes, configuration validity, resource usage,\nand overall system health to detect potential issues.","title":"ExESDB.Monitoring.HealthChecker","ref":"ExESDB.Monitoring.HealthChecker.html"},{"type":"function","doc":"","title":"ExESDB.Monitoring.HealthChecker.check_resource_usage/0","ref":"ExESDB.Monitoring.HealthChecker.html#check_resource_usage/0"},{"type":"function","doc":"","title":"ExESDB.Monitoring.HealthChecker.check_system_processes/1","ref":"ExESDB.Monitoring.HealthChecker.html#check_system_processes/1"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Monitoring.HealthChecker.child_spec/1","ref":"ExESDB.Monitoring.HealthChecker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Monitoring.HealthChecker.perform_health_check/1","ref":"ExESDB.Monitoring.HealthChecker.html#perform_health_check/1"},{"type":"function","doc":"","title":"ExESDB.Monitoring.HealthChecker.start_link/1","ref":"ExESDB.Monitoring.HealthChecker.html#start_link/1"},{"type":"module","doc":"Supervisor for the Monitoring subsystem.\n\nManages health checking, performance tracking, and system monitoring components.","title":"ExESDB.MonitoringSystem","ref":"ExESDB.MonitoringSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.MonitoringSystem.child_spec/1","ref":"ExESDB.MonitoringSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.MonitoringSystem.start_link/1","ref":"ExESDB.MonitoringSystem.html#start_link/1"},{"type":"module","doc":"Provides fast failure detection and cluster health monitoring to handle hard node crashes.\n\nThis module implements a multi-layer approach:\n1. Active health probing of cluster nodes\n2. Fast detection of unresponsive nodes\n3. Proactive cleanup of Swarm registrations\n4. Coordination with Khepri cluster management","title":"ExESDB.NodeMonitor","ref":"ExESDB.NodeMonitor.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.NodeMonitor.child_spec/1","ref":"ExESDB.NodeMonitor.html#child_spec/1"},{"type":"function","doc":"Get current cluster health status","title":"ExESDB.NodeMonitor.health_status/1","ref":"ExESDB.NodeMonitor.html#health_status/1"},{"type":"function","doc":"Force probe a specific node","title":"ExESDB.NodeMonitor.probe_node/2","ref":"ExESDB.NodeMonitor.html#probe_node/2"},{"type":"function","doc":"Start the node monitor","title":"ExESDB.NodeMonitor.start_link/1","ref":"ExESDB.NodeMonitor.html#start_link/1"},{"type":"module","doc":"Supervisor for event notification and distribution components.\n\nThis supervisor manages the core event notification functionality:\n- LeaderSystem: Leadership responsibilities and subscription management\n- EmitterSystem: Event emission and distribution\n\nThis is a core component that runs in both single-node and cluster modes.\nThe leadership determination happens at the store level, not the clustering level.","title":"ExESDB.NotificationSystem","ref":"ExESDB.NotificationSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.NotificationSystem.child_spec/1","ref":"ExESDB.NotificationSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.NotificationSystem.start_link/1","ref":"ExESDB.NotificationSystem.html#start_link/1"},{"type":"module","doc":"Collects various system metrics for observation and analysis.\n\nHandles collection of CPU, memory, process count, and other\nsystem-wide metrics for real-time monitoring.","title":"ExESDB.Observation.MetricsCollector","ref":"ExESDB.Observation.MetricsCollector.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Observation.MetricsCollector.child_spec/1","ref":"ExESDB.Observation.MetricsCollector.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Observation.MetricsCollector.collect_memory_metrics/0","ref":"ExESDB.Observation.MetricsCollector.html#collect_memory_metrics/0"},{"type":"function","doc":"","title":"ExESDB.Observation.MetricsCollector.collect_process_metrics/1","ref":"ExESDB.Observation.MetricsCollector.html#collect_process_metrics/1"},{"type":"function","doc":"","title":"ExESDB.Observation.MetricsCollector.collect_system_metrics/0","ref":"ExESDB.Observation.MetricsCollector.html#collect_system_metrics/0"},{"type":"function","doc":"","title":"ExESDB.Observation.MetricsCollector.start_link/1","ref":"ExESDB.Observation.MetricsCollector.html#start_link/1"},{"type":"module","doc":"Supervisor for the Observation subsystem.\n\nManages real-time monitoring, metrics collection, and observation components.","title":"ExESDB.ObservationSystem","ref":"ExESDB.ObservationSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.ObservationSystem.child_spec/1","ref":"ExESDB.ObservationSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.ObservationSystem.start_link/1","ref":"ExESDB.ObservationSystem.html#start_link/1"},{"type":"module","doc":"This module contains the options helper functions for ExESDB\n  \n  Modified to support umbrella configuration patterns where configurations\n  are stored under individual app names instead of the global :ex_esdb key.","title":"ExESDB.Options","ref":"ExESDB.Options.html"},{"type":"function","doc":"","title":"ExESDB.Options.app_env/1","ref":"ExESDB.Options.html#app_env/1"},{"type":"function","doc":"","title":"ExESDB.Options.app_env/3","ref":"ExESDB.Options.html#app_env/3"},{"type":"function","doc":"","title":"ExESDB.Options.data_dir/0","ref":"ExESDB.Options.html#data_dir/0"},{"type":"function","doc":"","title":"ExESDB.Options.data_dir/1","ref":"ExESDB.Options.html#data_dir/1"},{"type":"function","doc":"","title":"ExESDB.Options.db_type/0","ref":"ExESDB.Options.html#db_type/0"},{"type":"function","doc":"","title":"ExESDB.Options.db_type/1","ref":"ExESDB.Options.html#db_type/1"},{"type":"function","doc":"","title":"ExESDB.Options.get_context/0","ref":"ExESDB.Options.html#get_context/0"},{"type":"function","doc":"","title":"ExESDB.Options.get_context_or_discover/0","ref":"ExESDB.Options.html#get_context_or_discover/0"},{"type":"function","doc":"","title":"ExESDB.Options.persistence_enabled/0","ref":"ExESDB.Options.html#persistence_enabled/0"},{"type":"function","doc":"","title":"ExESDB.Options.persistence_enabled/1","ref":"ExESDB.Options.html#persistence_enabled/1"},{"type":"function","doc":"","title":"ExESDB.Options.persistence_interval/0","ref":"ExESDB.Options.html#persistence_interval/0"},{"type":"function","doc":"","title":"ExESDB.Options.persistence_interval/1","ref":"ExESDB.Options.html#persistence_interval/1"},{"type":"function","doc":"","title":"ExESDB.Options.reader_idle_ms/0","ref":"ExESDB.Options.html#reader_idle_ms/0"},{"type":"function","doc":"","title":"ExESDB.Options.reader_idle_ms/1","ref":"ExESDB.Options.html#reader_idle_ms/1"},{"type":"function","doc":"","title":"ExESDB.Options.set_context/1","ref":"ExESDB.Options.html#set_context/1"},{"type":"function","doc":"","title":"ExESDB.Options.store_description/0","ref":"ExESDB.Options.html#store_description/0"},{"type":"function","doc":"","title":"ExESDB.Options.store_description/1","ref":"ExESDB.Options.html#store_description/1"},{"type":"function","doc":"","title":"ExESDB.Options.store_id/0","ref":"ExESDB.Options.html#store_id/0"},{"type":"function","doc":"","title":"ExESDB.Options.store_id/1","ref":"ExESDB.Options.html#store_id/1"},{"type":"function","doc":"","title":"ExESDB.Options.store_tags/0","ref":"ExESDB.Options.html#store_tags/0"},{"type":"function","doc":"","title":"ExESDB.Options.store_tags/1","ref":"ExESDB.Options.html#store_tags/1"},{"type":"function","doc":"","title":"ExESDB.Options.sys_env/1","ref":"ExESDB.Options.html#sys_env/1"},{"type":"function","doc":"","title":"ExESDB.Options.timeout/0","ref":"ExESDB.Options.html#timeout/0"},{"type":"function","doc":"","title":"ExESDB.Options.timeout/1","ref":"ExESDB.Options.html#timeout/1"},{"type":"function","doc":"","title":"ExESDB.Options.topologies/0","ref":"ExESDB.Options.html#topologies/0"},{"type":"function","doc":"","title":"ExESDB.Options.with_context/2","ref":"ExESDB.Options.html#with_context/2"},{"type":"function","doc":"","title":"ExESDB.Options.writer_idle_ms/0","ref":"ExESDB.Options.html#writer_idle_ms/0"},{"type":"function","doc":"","title":"ExESDB.Options.writer_idle_ms/1","ref":"ExESDB.Options.html#writer_idle_ms/1"},{"type":"module","doc":"Simplified ExESDB configuration module using automatic OTP app discovery.\n\nThis module dramatically reduces complexity by:\n1. Automatically discovering the calling application\n2. Using a macro to generate config functions with less duplication\n3. Providing sensible defaults and environment variable overrides\n\nUsage:\n  # Automatic discovery (recommended)\n  ExESDB.Options.data_dir()      # Discovers calling app automatically\n  \n  # Explicit app specification\n  ExESDB.Options.data_dir(:my_app)\n  \n  # Context-based (for umbrella apps)\n  ExESDB.Options.with_context(:my_app, fn ->\n    ExESDB.Options.data_dir()    # Uses :my_app context\n  end)","title":"ExESDB.Options.Simplified","ref":"ExESDB.Options.Simplified.html"},{"type":"function","doc":"Get complete configuration for an OTP app","title":"ExESDB.Options.Simplified.app_env/1","ref":"ExESDB.Options.Simplified.html#app_env/1"},{"type":"function","doc":"Get a specific config value from app environment with default.","title":"ExESDB.Options.Simplified.app_env/3","ref":"ExESDB.Options.Simplified.html#app_env/3"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.data_dir/1","ref":"ExESDB.Options.Simplified.html#data_dir/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.db_type/1","ref":"ExESDB.Options.Simplified.html#db_type/1"},{"type":"function","doc":"Discovers the current OTP application name using multiple strategies.","title":"ExESDB.Options.Simplified.discover_app_name/0","ref":"ExESDB.Options.Simplified.html#discover_app_name/0"},{"type":"function","doc":"Generic configuration value getter with automatic app discovery.\n\nPriority order:\n1. Environment variable\n2. Application configuration\n3. Default value","title":"ExESDB.Options.Simplified.get_config_value/2","ref":"ExESDB.Options.Simplified.html#get_config_value/2"},{"type":"function","doc":"Get current application context","title":"ExESDB.Options.Simplified.get_context/0","ref":"ExESDB.Options.Simplified.html#get_context/0"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.persistence_enabled/1","ref":"ExESDB.Options.Simplified.html#persistence_enabled/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.persistence_interval/1","ref":"ExESDB.Options.Simplified.html#persistence_interval/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.pub_sub/1","ref":"ExESDB.Options.Simplified.html#pub_sub/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.reader_idle_ms/1","ref":"ExESDB.Options.Simplified.html#reader_idle_ms/1"},{"type":"function","doc":"Set application context for configuration lookup","title":"ExESDB.Options.Simplified.set_context/1","ref":"ExESDB.Options.Simplified.html#set_context/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.store_description/1","ref":"ExESDB.Options.Simplified.html#store_description/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.store_id/1","ref":"ExESDB.Options.Simplified.html#store_id/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.store_tags/1","ref":"ExESDB.Options.Simplified.html#store_tags/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.timeout/1","ref":"ExESDB.Options.Simplified.html#timeout/1"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.topologies/0","ref":"ExESDB.Options.Simplified.html#topologies/0"},{"type":"function","doc":"Execute function within specific application context","title":"ExESDB.Options.Simplified.with_context/2","ref":"ExESDB.Options.Simplified.html#with_context/2"},{"type":"function","doc":"","title":"ExESDB.Options.Simplified.writer_idle_ms/1","ref":"ExESDB.Options.Simplified.html#writer_idle_ms/1"},{"type":"module","doc":"Supervisor for persistence layer components.\n\nThis supervisor manages all data persistence components that can\noperate independently of each other.\n\nComponents:\n- PersistenceWorker: Asynchronous disk persistence management\n- Streams: Stream read/write operations\n- Snapshots: Snapshot management\n- Subscriptions: Subscription management","title":"ExESDB.PersistenceSystem","ref":"ExESDB.PersistenceSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.PersistenceSystem.child_spec/1","ref":"ExESDB.PersistenceSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.PersistenceSystem.start_link/1","ref":"ExESDB.PersistenceSystem.html#start_link/1"},{"type":"module","doc":"A GenServer that handles periodic disk persistence operations.\n\nThis worker batches and schedules fence operations to ensure data is\npersisted to disk without blocking event append operations.\n\nFeatures:\n- Configurable persistence interval (default: 5 seconds)\n- Batching of fence operations to reduce disk I/O\n- Graceful shutdown with final persistence\n- Per-store persistence workers","title":"ExESDB.PersistenceWorker","ref":"ExESDB.PersistenceWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.PersistenceWorker.child_spec/1","ref":"ExESDB.PersistenceWorker.html#child_spec/1"},{"type":"function","doc":"Forces immediate persistence of all pending stores.\nThis is a synchronous call that blocks until persistence is complete.","title":"ExESDB.PersistenceWorker.force_persistence/1","ref":"ExESDB.PersistenceWorker.html#force_persistence/1"},{"type":"function","doc":"Requests that a store's data be persisted to disk.\nThis is a non-blocking call that queues the store for persistence.","title":"ExESDB.PersistenceWorker.request_persistence/1","ref":"ExESDB.PersistenceWorker.html#request_persistence/1"},{"type":"function","doc":"Starts a persistence worker for a specific store.","title":"ExESDB.PersistenceWorker.start_link/1","ref":"ExESDB.PersistenceWorker.html#start_link/1"},{"type":"module","doc":"Loads and validates scenario configurations from various sources.\n\nSupports loading from JSON files, YAML files, and Elixir configuration\nfor custom test scenarios.","title":"ExESDB.Scenario.ConfigLoader","ref":"ExESDB.Scenario.ConfigLoader.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Scenario.ConfigLoader.child_spec/1","ref":"ExESDB.Scenario.ConfigLoader.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Scenario.ConfigLoader.list_builtin_scenarios/0","ref":"ExESDB.Scenario.ConfigLoader.html#list_builtin_scenarios/0"},{"type":"function","doc":"","title":"ExESDB.Scenario.ConfigLoader.load_config/1","ref":"ExESDB.Scenario.ConfigLoader.html#load_config/1"},{"type":"function","doc":"","title":"ExESDB.Scenario.ConfigLoader.start_link/1","ref":"ExESDB.Scenario.ConfigLoader.html#start_link/1"},{"type":"function","doc":"","title":"ExESDB.Scenario.ConfigLoader.validate_config/1","ref":"ExESDB.Scenario.ConfigLoader.html#validate_config/1"},{"type":"module","doc":"Supervisor for the Scenario subsystem.\n\nManages scenario execution, configuration loading, and testing orchestration.","title":"ExESDB.ScenarioSystem","ref":"ExESDB.ScenarioSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.ScenarioSystem.child_spec/1","ref":"ExESDB.ScenarioSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.ScenarioSystem.start_link/1","ref":"ExESDB.ScenarioSystem.html#start_link/1"},{"type":"module","doc":"The ExESDB Snapshots SubSystem.","title":"ExESDB.Snapshots","ref":"ExESDB.Snapshots.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Snapshots.child_spec/1","ref":"ExESDB.Snapshots.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Snapshots.path/3","ref":"ExESDB.Snapshots.html#path/3"},{"type":"function","doc":"Returns the key for a snapshot as a Khepri Path.","title":"Description - ExESDB.Snapshots.path/3","ref":"ExESDB.Snapshots.html#path/3-description"},{"type":"function","doc":"iex> ExESDB.Snapshots.path(\"source_uuid\", \"stream_uuid\", 1)\n    [:snapshots, \"source_uuid\", \"stream_uuid\", \"0000000001\"]","title":"Examples - ExESDB.Snapshots.path/3","ref":"ExESDB.Snapshots.html#path/3-examples"},{"type":"function","doc":"","title":"ExESDB.Snapshots.start_link/1","ref":"ExESDB.Snapshots.html#start_link/1"},{"type":"module","doc":"Provides functions for reading snapshots","title":"ExESDB.SnapshotsReader","ref":"ExESDB.SnapshotsReader.html"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReader.cluster_id/3","ref":"ExESDB.SnapshotsReader.html#cluster_id/3"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReader.hr_snapshots_reader_name/3","ref":"ExESDB.SnapshotsReader.html#hr_snapshots_reader_name/3"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReader.list_snapshots/3","ref":"ExESDB.SnapshotsReader.html#list_snapshots/3"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReader.read_snapshot/4","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4"},{"type":"function","doc":"Reads a snapshot version from the store \n  for the given source and stream uuids","title":"Description - ExESDB.SnapshotsReader.read_snapshot/4","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-description"},{"type":"function","doc":"* `store` - the store to read from\n   * `source_uuid` - the source uuid\n   * `stream_uuid` - the stream uuid\n   * `version` - the version of the snapshot to read","title":"Parameters - ExESDB.SnapshotsReader.read_snapshot/4","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-parameters"},{"type":"function","doc":"* `{:ok, map()}` - the snapshot","title":"Returns - ExESDB.SnapshotsReader.read_snapshot/4","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-returns"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReader.start_worker/3","ref":"ExESDB.SnapshotsReader.html#start_worker/3"},{"type":"module","doc":"A pool of `ExESDB.SnapshotsReaderWorker` processes.","title":"ExESDB.SnapshotsReaderPool","ref":"ExESDB.SnapshotsReaderPool.html"},{"type":"module","doc":"A worker process for reading snapshots from the event store.","title":"ExESDB.SnapshotsReaderWorker","ref":"ExESDB.SnapshotsReaderWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SnapshotsReaderWorker.child_spec/1","ref":"ExESDB.SnapshotsReaderWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.SnapshotsReaderWorker.start_link/1","ref":"ExESDB.SnapshotsReaderWorker.html#start_link/1"},{"type":"module","doc":"The API for interacting with ExESDB Snapshots Writers.\n It functions as an API for SnapshotsWriterWorkers, \n by requesting a worker from the Cluster. \n If no worker is available for the specific combination of store, source_uuid, and stream_uuid,\n then a new worker is started.","title":"ExESDB.SnapshotsWriter","ref":"ExESDB.SnapshotsWriter.html"},{"type":"function","doc":"","title":"ExESDB.SnapshotsWriter.cluster_id/3","ref":"ExESDB.SnapshotsWriter.html#cluster_id/3"},{"type":"function","doc":"","title":"ExESDB.SnapshotsWriter.delete_snapshot/4","ref":"ExESDB.SnapshotsWriter.html#delete_snapshot/4"},{"type":"function","doc":"","title":"ExESDB.SnapshotsWriter.hr_snapshots_writer_name/3","ref":"ExESDB.SnapshotsWriter.html#hr_snapshots_writer_name/3"},{"type":"function","doc":"","title":"ExESDB.SnapshotsWriter.record_snapshot/5","ref":"ExESDB.SnapshotsWriter.html#record_snapshot/5"},{"type":"module","doc":"A pool of `ExESDB.SnapshotsWriterWorker` processes.","title":"ExESDB.SnapshotsWriterPool","ref":"ExESDB.SnapshotsWriterPool.html"},{"type":"module","doc":"A worker process for writing snapshots to the event store.","title":"ExESDB.SnapshotsWriterWorker","ref":"ExESDB.SnapshotsWriterWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SnapshotsWriterWorker.child_spec/1","ref":"ExESDB.SnapshotsWriterWorker.html#child_spec/1"},{"type":"function","doc":"Starts a new `ExESDB.SnapshotsWriterWorker` process.","title":"ExESDB.SnapshotsWriterWorker.start_link/1","ref":"ExESDB.SnapshotsWriterWorker.html#start_link/1"},{"type":"module","doc":"A GenServer wrapper around :khepri to act as a distributed event store.","title":"ExESDB.Store","ref":"ExESDB.Store.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Store.child_spec/1","ref":"ExESDB.Store.html#child_spec/1"},{"type":"function","doc":"Get the current state of the store.","title":"ExESDB.Store.get_state/1","ref":"ExESDB.Store.html#get_state/1"},{"type":"function","doc":"- `{:ok, state}`  if successful.\n    - `{:error, reason}` if unsuccessful.","title":"Returns - ExESDB.Store.get_state/1","ref":"ExESDB.Store.html#get_state/1-returns"},{"type":"function","doc":"","title":"ExESDB.Store.start_link/1","ref":"ExESDB.Store.html#start_link/1"},{"type":"function","doc":"Get the store-specific GenServer name.\n\nThis function returns the name used to register this store GenServer,\nallowing multiple stores to run on the same node.","title":"ExESDB.Store.store_name/1","ref":"ExESDB.Store.html#store_name/1"},{"type":"function","doc":"* `store_id` - The store identifier (optional)","title":"Parameters - ExESDB.Store.store_name/1","ref":"ExESDB.Store.html#store_name/1-parameters"},{"type":"function","doc":"iex> ExESDB.Store.store_name(\"my_store\")\n    {:ex_esdb_store, \"my_store\"}\n    \n    iex> ExESDB.Store.store_name(nil)\n    ExESDB.Store","title":"Examples - ExESDB.Store.store_name/1","ref":"ExESDB.Store.html#store_name/1-examples"},{"type":"module","doc":"GenServer responsible for coordinating Khepri cluster formation and preventing split-brain scenarios.\n\nThis module handles:\n- Detecting existing clusters\n- Coordinator election\n- Coordinated cluster joining\n- Split-brain prevention","title":"ExESDB.StoreCoordinator","ref":"ExESDB.StoreCoordinator.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StoreCoordinator.child_spec/1","ref":"ExESDB.StoreCoordinator.html#child_spec/1"},{"type":"function","doc":"Attempts to join a Khepri cluster using coordinated approach to prevent split-brain.\nReturns one of: :ok, :coordinator, :no_nodes, :waiting, :failed","title":"ExESDB.StoreCoordinator.join_cluster/1","ref":"ExESDB.StoreCoordinator.html#join_cluster/1"},{"type":"function","doc":"Checks if this node should handle nodeup events (i.e., not already in a cluster)","title":"ExESDB.StoreCoordinator.should_handle_nodeup?/1","ref":"ExESDB.StoreCoordinator.html#should_handle_nodeup?/1"},{"type":"function","doc":"","title":"ExESDB.StoreCoordinator.start_link/1","ref":"ExESDB.StoreCoordinator.html#start_link/1"},{"type":"module","doc":"This module provides functions to get information about the EXESDB event store.","title":"ExESDB.StoreInfo","ref":"ExESDB.StoreInfo.html"},{"type":"function","doc":"Returns the list of streams in the store.","title":"ExESDB.StoreInfo.get_streams!/1","ref":"ExESDB.StoreInfo.html#get_streams!/1"},{"type":"function","doc":"","title":"ExESDB.StoreInfo.get_streams_raw/1","ref":"ExESDB.StoreInfo.html#get_streams_raw/1"},{"type":"module","doc":"Supervisor for store-related components.\n\nThis supervisor manages the store lifecycle and clustering components.\nUses :rest_for_one strategy to ensure proper startup order.\n\nStartup order (critical for distributed coordination):\n1. Store: Core store GenServer - must be fully operational first\n2. StoreCluster: Clustering coordination - depends on Store being ready\n3. StoreRegistry: Distributed store registry - starts after Store system is stable\n\nThis order ensures StoreRegistry only announces a store that is actually ready\nto handle requests, preventing race conditions in distributed environments.","title":"ExESDB.StoreSystem","ref":"ExESDB.StoreSystem.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StoreSystem.child_spec/1","ref":"ExESDB.StoreSystem.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.StoreSystem.start_link/1","ref":"ExESDB.StoreSystem.html#start_link/1"},{"type":"module","doc":"A GenServer wrapper around :khepri to act as a distributed event store.\n  Inspired by EventStoreDB's API.","title":"ExESDB.StoreWorker","ref":"ExESDB.StoreWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StoreWorker.child_spec/1","ref":"ExESDB.StoreWorker.html#child_spec/1"},{"type":"function","doc":"Get the current state of the store.","title":"ExESDB.StoreWorker.get_state/1","ref":"ExESDB.StoreWorker.html#get_state/1"},{"type":"function","doc":"- `{:ok, state}`  if successful.\n    - `{:error, reason}` if unsuccessful.","title":"Returns - ExESDB.StoreWorker.get_state/1","ref":"ExESDB.StoreWorker.html#get_state/1-returns"},{"type":"function","doc":"","title":"ExESDB.StoreWorker.start_link/1","ref":"ExESDB.StoreWorker.html#start_link/1"},{"type":"module","doc":"The ExESDB Streams SubSystem.","title":"ExESDB.Streams","ref":"ExESDB.Streams.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Streams.child_spec/1","ref":"ExESDB.Streams.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Streams.start_link/1","ref":"ExESDB.Streams.html#start_link/1"},{"type":"module","doc":"Provides helper functions for working with event store streams.","title":"ExESDB.StreamsHelper","ref":"ExESDB.StreamsHelper.html"},{"type":"function","doc":"","title":"ExESDB.StreamsHelper.calculate_versions/3","ref":"ExESDB.StreamsHelper.html#calculate_versions/3"},{"type":"function","doc":"Returns the version of the stream using 0-based indexing.","title":"ExESDB.StreamsHelper.get_version!/2","ref":"ExESDB.StreamsHelper.html#get_version!/2"},{"type":"function","doc":"- `store` is the name of the store.\n   - `stream_id` is the name of the stream.","title":"Parameters - ExESDB.StreamsHelper.get_version!/2","ref":"ExESDB.StreamsHelper.html#get_version!/2-parameters"},{"type":"function","doc":"- `version` (0-based) or `-1` if the stream does not exist.\n   This means:\n   - New stream (no events): -1\n   - Stream with 1 event: 0 (version of latest event)\n   - Stream with 2 events: 1 (version of latest event)\n   - etc.","title":"Returns - ExESDB.StreamsHelper.get_version!/2","ref":"ExESDB.StreamsHelper.html#get_version!/2-returns"},{"type":"function","doc":"","title":"ExESDB.StreamsHelper.pad_version/2","ref":"ExESDB.StreamsHelper.html#pad_version/2"},{"type":"function","doc":"","title":"ExESDB.StreamsHelper.stream_exists?/2","ref":"ExESDB.StreamsHelper.html#stream_exists?/2"},{"type":"function","doc":"","title":"ExESDB.StreamsHelper.to_event_record/5","ref":"ExESDB.StreamsHelper.html#to_event_record/5"},{"type":"function","doc":"","title":"ExESDB.StreamsHelper.version_to_integer/1","ref":"ExESDB.StreamsHelper.html#version_to_integer/1"},{"type":"module","doc":"This module is responsible for reading events from a stream.","title":"ExESDB.StreamsReader","ref":"ExESDB.StreamsReader.html"},{"type":"function","doc":"Returns a list of all streams in the store.","title":"ExESDB.StreamsReader.get_streams/1","ref":"ExESDB.StreamsReader.html#get_streams/1"},{"type":"function","doc":"- `store` is the name of the store.","title":"Parameters - ExESDB.StreamsReader.get_streams/1","ref":"ExESDB.StreamsReader.html#get_streams/1-parameters"},{"type":"function","doc":"- `{:ok, streams}`  if successful.","title":"Returns - ExESDB.StreamsReader.get_streams/1","ref":"ExESDB.StreamsReader.html#get_streams/1-returns"},{"type":"function","doc":"Streams events from `stream` in batches of `count` events, in a `direction`.","title":"ExESDB.StreamsReader.stream_events/5","ref":"ExESDB.StreamsReader.html#stream_events/5"},{"type":"function","doc":"","title":"ExESDB.StreamsReader.worker_id/2","ref":"ExESDB.StreamsReader.html#worker_id/2"},{"type":"module","doc":"As part of the ExESDB.System,","title":"ExESDB.StreamsReaderPool","ref":"ExESDB.StreamsReaderPool.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StreamsReaderPool.child_spec/1","ref":"ExESDB.StreamsReaderPool.html#child_spec/1"},{"type":"module","doc":"Provides functions for reading and streaming events from the event store_id.","title":"ExESDB.StreamsReaderWorker","ref":"ExESDB.StreamsReaderWorker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StreamsReaderWorker.child_spec/1","ref":"ExESDB.StreamsReaderWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.StreamsReaderWorker.start_link/1","ref":"ExESDB.StreamsReaderWorker.html#start_link/1"},{"type":"module","doc":"This module is responsible for writing events to a stream.\n  It is actually an API style wrapper around the StreamsWriterWorker.","title":"ExESDB.StreamsWriter","ref":"ExESDB.StreamsWriter.html"},{"type":"function","doc":"","title":"ExESDB.StreamsWriter.append_events/4","ref":"ExESDB.StreamsWriter.html#append_events/4"},{"type":"function","doc":"","title":"ExESDB.StreamsWriter.hr_worker_id_atom/2","ref":"ExESDB.StreamsWriter.html#hr_worker_id_atom/2"},{"type":"function","doc":"","title":"ExESDB.StreamsWriter.worker_id/2","ref":"ExESDB.StreamsWriter.html#worker_id/2"},{"type":"module","doc":"As part of the ExESDB.System,","title":"ExESDB.StreamsWriterPool","ref":"ExESDB.StreamsWriterPool.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.StreamsWriterPool.child_spec/1","ref":"ExESDB.StreamsWriterPool.html#child_spec/1"},{"type":"module","doc":"Provides functions for writing streams","title":"ExESDB.StreamsWriterWorker","ref":"ExESDB.StreamsWriterWorker.html"},{"type":"function","doc":"Returns a child spec for a streams writer worker.\n  Please note that the restart strategy is set to `:temporary`\n  to avoid restarting the worker when the idle timeout is reached.","title":"ExESDB.StreamsWriterWorker.child_spec/1","ref":"ExESDB.StreamsWriterWorker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.StreamsWriterWorker.start_link/1","ref":"ExESDB.StreamsWriterWorker.html#start_link/1"},{"type":"module","doc":"Monitors the health of subscriptions and their associated emitter pools.\n\nThis module provides comprehensive health monitoring for the subscription system:\n- Detects stale subscriptions (subscriber process is dead)\n- Identifies orphaned emitter pools (pools without corresponding subscriptions)\n- Finds missing emitter pools (subscriptions without pools on leader nodes)\n- Automatically triggers cleanup and recovery actions\n- Provides detailed health reports for observability\n\nThe monitor runs periodic health checks and can be triggered manually for\nimmediate diagnostics.","title":"ExESDB.SubscriptionHealthMonitor","ref":"ExESDB.SubscriptionHealthMonitor.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SubscriptionHealthMonitor.child_spec/1","ref":"ExESDB.SubscriptionHealthMonitor.html#child_spec/1"},{"type":"function","doc":"Force cleanup of a specific subscription or emitter pool.","title":"ExESDB.SubscriptionHealthMonitor.force_cleanup/2","ref":"ExESDB.SubscriptionHealthMonitor.html#force_cleanup/2"},{"type":"function","doc":"Perform an immediate health check and return the results.","title":"ExESDB.SubscriptionHealthMonitor.health_check/1","ref":"ExESDB.SubscriptionHealthMonitor.html#health_check/1"},{"type":"function","doc":"Get the last health report without performing a new check.","title":"ExESDB.SubscriptionHealthMonitor.last_health_report/1","ref":"ExESDB.SubscriptionHealthMonitor.html#last_health_report/1"},{"type":"function","doc":"Enable or disable automatic cleanup of detected issues.","title":"ExESDB.SubscriptionHealthMonitor.set_cleanup_enabled/2","ref":"ExESDB.SubscriptionHealthMonitor.html#set_cleanup_enabled/2"},{"type":"function","doc":"","title":"ExESDB.SubscriptionHealthMonitor.start_link/1","ref":"ExESDB.SubscriptionHealthMonitor.html#start_link/1"},{"type":"module","doc":"Centralized tracker for subscription health events received via :ex_esdb_system PubSub.\n\nThis module:\n- Subscribes to subscription health events from subscription proxies\n- Maintains current health status for all subscriptions in a store\n- Provides APIs to query subscription health\n- Publishes aggregated health summaries\n- Integrates with the broader ExESDB monitoring system","title":"ExESDB.SubscriptionHealthTracker","ref":"ExESDB.SubscriptionHealthTracker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SubscriptionHealthTracker.child_spec/1","ref":"ExESDB.SubscriptionHealthTracker.html#child_spec/1"},{"type":"function","doc":"Gets the current health status for all subscriptions in a store.","title":"ExESDB.SubscriptionHealthTracker.get_store_health_summary/1","ref":"ExESDB.SubscriptionHealthTracker.html#get_store_health_summary/1"},{"type":"function","doc":"Gets detailed health information for a specific subscription.","title":"ExESDB.SubscriptionHealthTracker.get_subscription_health/2","ref":"ExESDB.SubscriptionHealthTracker.html#get_subscription_health/2"},{"type":"type","doc":"","title":"ExESDB.SubscriptionHealthTracker.health_data/0","ref":"ExESDB.SubscriptionHealthTracker.html#t:health_data/0"},{"type":"type","doc":"","title":"ExESDB.SubscriptionHealthTracker.health_event/0","ref":"ExESDB.SubscriptionHealthTracker.html#t:health_event/0"},{"type":"type","doc":"","title":"ExESDB.SubscriptionHealthTracker.health_status/0","ref":"ExESDB.SubscriptionHealthTracker.html#t:health_status/0"},{"type":"function","doc":"Lists all subscriptions currently being tracked.","title":"ExESDB.SubscriptionHealthTracker.list_tracked_subscriptions/1","ref":"ExESDB.SubscriptionHealthTracker.html#list_tracked_subscriptions/1"},{"type":"function","doc":"","title":"ExESDB.SubscriptionHealthTracker.start_link/1","ref":"ExESDB.SubscriptionHealthTracker.html#start_link/1"},{"type":"module","doc":"Provides functions for working with event store subscriptions.","title":"ExESDB.Subscriptions","ref":"ExESDB.Subscriptions.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.Subscriptions.child_spec/1","ref":"ExESDB.Subscriptions.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.Subscriptions.start_link/1","ref":"ExESDB.Subscriptions.html#start_link/1"},{"type":"module","doc":"Provides functions for working with event store subscriptions.","title":"ExESDB.SubscriptionsReader","ref":"ExESDB.SubscriptionsReader.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SubscriptionsReader.child_spec/1","ref":"ExESDB.SubscriptionsReader.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsReader.get_subscriptions/1","ref":"ExESDB.SubscriptionsReader.html#get_subscriptions/1"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsReader.start_link/1","ref":"ExESDB.SubscriptionsReader.html#start_link/1"},{"type":"module","doc":"As part of the ExESDB.System, the SubscriptionsTracker is responsible for\n  observing the subscriptions that are maintained in the Store.\n\n  Since Khepri triggers are executed on the leader node, the SubscriptionsTracker\n  will be instructed to start the Emitters system on the leader node whenever a new subscription\n  is registered.\n\n  When a Subscription is deleted, the SubscriptionsTracker will instruct the Emitters system to stop \n  the associated EmitterPool.","title":"ExESDB.SubscriptionsTracker","ref":"ExESDB.SubscriptionsTracker.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SubscriptionsTracker.child_spec/1","ref":"ExESDB.SubscriptionsTracker.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsTracker.start_link/1","ref":"ExESDB.SubscriptionsTracker.html#start_link/1"},{"type":"module","doc":"Provides functions for working with event store subscriptions.","title":"ExESDB.SubscriptionsWriter","ref":"ExESDB.SubscriptionsWriter.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.SubscriptionsWriter.child_spec/1","ref":"ExESDB.SubscriptionsWriter.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsWriter.delete_subscription/4","ref":"ExESDB.SubscriptionsWriter.html#delete_subscription/4"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsWriter.put_subscription/6","ref":"ExESDB.SubscriptionsWriter.html#put_subscription/6"},{"type":"function","doc":"","title":"ExESDB.SubscriptionsWriter.start_link/1","ref":"ExESDB.SubscriptionsWriter.html#start_link/1"},{"type":"module","doc":"This module is the top level supervisor for the ExESDB system.\n  \n  It uses a layered supervision architecture for better fault tolerance:\n  \n  SINGLE NODE MODE:\n  1. PubSubSystem: Message distribution infrastructure\n  2. LoggingSystem: Structured logging and event processing\n  3. CoreSystem: Critical infrastructure (PersistenceSystem + NotificationSystem + StoreSystem)\n  4. GatewaySystem: External interface with pooled workers\n  \n  CLUSTER MODE:\n  1. PubSubSystem: Message distribution infrastructure\n  2. LoggingSystem: Structured logging and event processing\n  3. CoreSystem: Critical infrastructure (PersistenceSystem + NotificationSystem + StoreSystem)\n  4. LibCluster: Node discovery and connection (after core is ready)\n  5. ClusterSystem: Cluster coordination and membership\n  6. GatewaySystem: External interface (LAST - only after clustering is ready)\n  \n  NotificationSystem (part of CoreSystem) includes:\n  - LeaderSystem: Leadership responsibilities and subscription management\n  - EmitterSystem: Event emission and distribution\n  \n  IMPORTANT: Core functionality (Store, Persistence) must be fully operational \n  before any clustering/membership/registration components start. This ensures \n  the server is ready to handle requests before announcing itself to the cluster.\n  \n  In cluster mode, GatewaySystem starts LAST to prevent external connections\n  until the entire distributed system is properly initialized.\n  \n  Note: Store management is now handled by the distributed ex-esdb-gater API.","title":"ExESDB.System","ref":"ExESDB.System.html"},{"type":"function","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","title":"ExESDB.System.child_spec/1","ref":"ExESDB.System.html#child_spec/1"},{"type":"function","doc":"","title":"ExESDB.System.start/1","ref":"ExESDB.System.html#start/1"},{"type":"function","doc":"Start ExESDB.System with automatic configuration discovery.\n\nThis function supports several modes:\n1. With explicit opts: `ExESDB.System.start_link(opts)` - uses provided configuration\n2. With OTP app name: `ExESDB.System.start_link(:my_app)` - discovers config from specified app\n3. With empty opts or no args: `ExESDB.System.start_link([])` or `ExESDB.System.start_link()` - discovers config from calling application","title":"ExESDB.System.start_link/1","ref":"ExESDB.System.html#start_link/1"},{"type":"function","doc":"# Explicit configuration (current approach)\n    opts = ExESDB.Options.app_env(:my_app)\n    ExESDB.System.start_link(opts)\n\n    # Auto-discovery from specific app\n    ExESDB.System.start_link(:my_app)\n\n    # Auto-discovery from calling application\n    ExESDB.System.start_link()","title":"Examples - ExESDB.System.start_link/1","ref":"ExESDB.System.html#start_link/1-examples"},{"type":"function","doc":"","title":"ExESDB.System.stop/1","ref":"ExESDB.System.html#stop/1"},{"type":"function","doc":"Generate a store-specific name for this system supervisor.\n\nThis allows multiple ExESDB systems to run on the same node with different stores.","title":"ExESDB.System.system_name/1","ref":"ExESDB.System.html#system_name/1"},{"type":"function","doc":"iex> ExESDB.System.system_name(\"my_store\")\n    :\"exesdb_system_my_store\"\n    \n    iex> ExESDB.System.system_name(nil)\n    ExESDB.System","title":"Examples - ExESDB.System.system_name/1","ref":"ExESDB.System.html#system_name/1-examples"},{"type":"module","doc":"A module to calculate topic identifiers","title":"ExESDB.Topics","ref":"ExESDB.Topics.html"},{"type":"function","doc":"","title":"ExESDB.Topics.selector_hash/1","ref":"ExESDB.Topics.html#selector_hash/1"},{"type":"function","doc":"","title":"ExESDB.Topics.sub_topic/3","ref":"ExESDB.Topics.html#sub_topic/3"},{"type":"module","doc":"Helper module for starting ExESDB with umbrella configuration patterns.\n\nThis module provides functions to start ExESDB with app-specific configurations\nwhen used in umbrella applications.","title":"ExESDB.UmbrellaHelper","ref":"ExESDB.UmbrellaHelper.html"},{"type":"function","doc":"Start ExESDB with configuration from a specific OTP app.\n\nThis function configures ExESDB to read its configuration from the specified\nOTP app instead of the global :ex_esdb application.","title":"ExESDB.UmbrellaHelper.child_spec/1","ref":"ExESDB.UmbrellaHelper.html#child_spec/1"},{"type":"function","doc":"* `otp_app` - The OTP application name to read configuration from","title":"Parameters - ExESDB.UmbrellaHelper.child_spec/1","ref":"ExESDB.UmbrellaHelper.html#child_spec/1-parameters"},{"type":"function","doc":"# In your reckon app's application.ex:\n    def start(_type, _args) do\n      children = [\n        # Other children...\n        ExESDB.UmbrellaHelper.child_spec(:reckon_accounts)\n      ]\n      \n      Supervisor.start_link(children, strategy: :one_for_one)\n    end","title":"Examples - ExESDB.UmbrellaHelper.child_spec/1","ref":"ExESDB.UmbrellaHelper.html#child_spec/1-examples"},{"type":"function","doc":"Configure ExESDB to use the specified OTP app for configuration.\n\nThis function sets up ExESDB to use app-specific configuration by\nsetting the :otp_app configuration key.","title":"ExESDB.UmbrellaHelper.configure/1","ref":"ExESDB.UmbrellaHelper.html#configure/1"},{"type":"function","doc":"Start ExESDB with configuration from a specific OTP app.\n\nThis is a convenience function that directly starts the ExESDB system\nwith the specified OTP app configuration.","title":"ExESDB.UmbrellaHelper.start_link/1","ref":"ExESDB.UmbrellaHelper.html#start_link/1"},{"type":"extras","doc":"# Architecture Decision Records","title":"Architecture Decision Records","ref":"adr.html"},{"type":"extras","doc":"","title":"2025.05.24 - Architecture Decision Records","ref":"adr.html#2025-05-24"},{"type":"extras","doc":"Given that Khepri triggers are executed on the Ra leader, emitter pools must run on the Ra leader.\nIn order to achieve this, it is necessary that a separate process exists, that monitors the Cluster and subscribes to Ra's leadership changes, by starting relevant emitter pools on the new leader.","title":"Emitter pools must run on the Ra leader. - Architecture Decision Records","ref":"adr.html#emitter-pools-must-run-on-the-ra-leader"},{"type":"extras","doc":"","title":"2025.05.02 - Architecture Decision Records","ref":"adr.html#2025-05-02"},{"type":"extras","doc":"Experimentation with :khepri's trigger model has shown that it is advised to use as few dependencies as possible, when defining the trigger functions. This is because :khepri uses :horus to decompile such functions and build a separate module. For this reason, we limit the dependencies for these trigger functions to :pg (Process Groups), which is an :erlang native module that allows for inter-process communication.","title":"Triggers will obtain Emitter Pids from :pg (Process Groups) - Architecture Decision Records","ref":"adr.html#triggers-will-obtain-emitter-pids-from-pg-process-groups"},{"type":"extras","doc":"","title":"2025.04.16 - Architecture Decision Records","ref":"adr.html#2025-04-16"},{"type":"extras","doc":"`subscribe_to` and `unsubscribe` will interact with the :subscriptions branch of the store, but instead of storing the pid of the subscriber, the subscriber will be stored in a registry.","title":"Subscriptions will be managed via a registry. - Architecture Decision Records","ref":"adr.html#subscriptions-will-be-managed-via-a-registry"},{"type":"extras","doc":"","title":"2025.04.13 - Architecture Decision Records","ref":"adr.html#2025-04-13"},{"type":"extras","doc":"- `:streams` will be used to store the events that are being read from and written to the store.\n- `:subscriptions` will be used to store the subscription information of so-called `Persistent Subscriptions`.\n- `:projections` can best be thought of as stored procedures that are used to transform the events in the `Streams` into a different format or to enrich the streams with secondary or derived events, to name a few possible use cases.\n\nThus:\n\n```mono\n:khepri\n  |\n  +-:manage_orders\n      |\n      +-:streams\n      |\n      +-:subscriptions\n      |\n      +-:projections\n```","title":"Each Store will contain separate branches for Streams, Subscriptions, and Projections - Architecture Decision Records","ref":"adr.html#each-store-will-contain-separate-branches-for-streams-subscriptions-and-projections"},{"type":"extras","doc":"# Changelog","title":"Changelog","ref":"changelog.html"},{"type":"extras","doc":"","title":"version 0.7.0 (2025.07.27) - Changelog","ref":"changelog.html#version-0-7-0-2025-07-27"},{"type":"extras","doc":"#### Multi-Channel PubSub System\n\n- **Dedicated PubSub Instances**: Implemented three specialized Phoenix.PubSub instances for improved separation of concerns\n  - **`:ex_esdb_events`**: Primary channel for business event distribution and streaming\n  - **`:ex_esdb_system`**: Internal system operations, coordination messages, and metrics\n  - **`:ex_esdb_health`**: Dedicated health monitoring and service availability communications\n- **Architectural Foundation**: Established infrastructure for transitioning to fully Event-Driven Architecture (EDA)\n- **Channel Isolation**: Prevents cross-contamination of different message types and improves system reliability\n\n#### Color-Coded EmitterWorker Observability\n\n- **Visual Message Classification**: Implemented comprehensive color-coded logging system for immediate issue identification\n  - ** Success Messages (White on Green/Blue)**: Service activation, health subscriptions, successful operations\n  - ** Failure Messages (White on Red)**: Termination events, errors, unhealthy states\n  - ** Action Messages (White on Amber)**: Broadcasting, forwarding, dynamic worker creation, metrics\n  - ** Health Messages (White on Cyan)**: Health event processing, status changes\n- **Enhanced PID Backgrounds**: Color-coded process identifiers based on message type for rapid visual debugging\n- **BCUtils.ColorFuncs Integration**: Leveraged existing color functions for consistent theming across the system\n\n#### Comprehensive Health and Metrics Monitoring\n\n- **Health Event Logging**: Real-time visibility into subscription health status and service availability\n  - Health event subscription tracking: ` SUBSCRIBED to health events`\n  - Individual health events: ` HEALTH EVENT: subscription_name -> event_type`\n  - Health summaries: ` HEALTH SUMMARY: Store my_store - 5/7 healthy subscriptions`\n  - Health impact tracking: ` HEALTH IMPACT: subscription_name is HEALTHY`\n- **Metrics Event Logging**: Performance and operational metrics collection\n  - Metrics subscription tracking: ` SUBSCRIBED to metrics events`\n  - Individual metrics: ` METRICS EVENT: store -> events_per_second=1250`\n  - Metrics summaries: ` METRICS SUMMARY: 1250 eps, 50000 total, 12 active subs`\n- **Health-Aware Emission Control**: EmitterWorkers can pause/resume emission based on subscription health status\n\n#### Enhanced EmitterWorker Lifecycle Management\n\n- **Detailed Activation Messages**: Comprehensive worker startup information including subscriber details\n- **Enhanced Termination Messages**: Added subscriber information to termination logs for better debugging\n- **Lifecycle Visibility**: Complete tracking of worker lifecycle events with structured, color-coded output\n- **Subscriber Information**: Full visibility into which subscribers are affected by worker state changes\n\n#### SubscriptionHealthTracker Integration\n\n- **Dedicated Health PubSub**: Migrated from `:ex_esdb_system` to dedicated `:ex_esdb_health` PubSub instance\n- **Health Event Broadcasting**: Comprehensive health event distribution to interested subscribers\n- **Health Summary Distribution**: Periodic health summaries broadcast to monitoring systems\n- **Improved Separation**: Clean separation between health monitoring and system operations\n\n#### Technical Implementation\n\n- **Multi-PubSub Architecture**: Three specialized PubSub instances for different communication types\n- **Enhanced Themes System**: Extended `ExESDB.Themes` with color-coded message functions\n  - `emitter_worker_success_msg/2`, `emitter_worker_failure_msg/2`\n  - `emitter_worker_action_msg/2`, `emitter_worker_health_msg/2`\n  - Similar functions for `emitter_system` and `emitter_pool` components\n- **Health-Aware Processing**: EmitterWorkers subscribe to both health and metrics events\n- **Structured Logging**: Consistent, parseable log format with rich visual indicators\n\n#### Event-Driven Architecture Benefits\n\n- **Separation of Concerns**: Dedicated channels prevent message type cross-contamination\n- **Enhanced Observability**: Real-time visibility into system health, performance, and operations\n- **Improved Reliability**: Health-aware emission and circuit breaker integration\n- **Better Performance**: Asynchronous messaging and efficient broadcasting\n- **Operational Excellence**: Comprehensive monitoring and debugging capabilities\n\n#### Documentation and Guidelines\n\n- **New Architecture Guide**: Comprehensive `guides/pubsub_architecture.md` documentation\n- **Implementation Details**: Complete technical implementation and configuration guidance\n- **Migration Path**: Roadmap for evolving to fully Event-Driven Architecture\n- **Best Practices**: Topic naming conventions, message structure, and performance considerations\n- **Monitoring Guidelines**: Health dashboard, performance metrics, and debugging tools\n\n#### Migration Foundation\n\n- **Phase 1 Complete**: Internal events, health/metrics distribution, enhanced observability\n- **Phase 2 Prepared**: Foundation for business domain events and event sourcing patterns\n- **Phase 3 Ready**: Infrastructure for external system integration and event streaming\n\n#### Benefits\n\n- **Unprecedented Observability**: Color-coded logging and comprehensive event tracking\n- **Enhanced Reliability**: Health-aware systems with graceful degradation\n- **Improved Performance**: Efficient message routing and asynchronous processing\n- **Better Developer Experience**: Visual debugging aids and structured logging\n- **Operational Excellence**: Comprehensive monitoring and troubleshooting capabilities\n- **Future-Ready Architecture**: Solid foundation for full Event-Driven Architecture evolution","title":"Enhanced PubSub Architecture: Foundation for Event-Driven Architecture - Changelog","ref":"changelog.html#enhanced-pubsub-architecture-foundation-for-event-driven-architecture"},{"type":"extras","doc":"","title":"version 0.4.9 (2025.07.19) - Changelog","ref":"changelog.html#version-0-4-9-2025-07-19"},{"type":"extras","doc":"#### New Feature: ExESDB.Debugger\n\n- **REPL-friendly debugging tool**: Added comprehensive debugging and inspection module for ExESDB systems\n- **Real-time system monitoring**: Complete visibility into processes, performance, and system health\n- **Production-safe operations**: All debugging functions are read-only and safe for live systems\n- **Auto-discovery capabilities**: Automatically detects stores and configuration without manual setup\n\n#### Core Debugging Functions\n\n- **System Overview**: `ExESDB.Debugger.overview()` - Complete system status at a glance\n- **Process Management**: `ExESDB.Debugger.processes()` - List and inspect all ExESDB processes\n- **Supervision Tree**: `ExESDB.Debugger.supervision_tree()` - Visual process hierarchy inspection\n- **Configuration Analysis**: `ExESDB.Debugger.config()` - Shows current config with sources (env vs app config)\n- **Health Monitoring**: `ExESDB.Debugger.health()` - Comprehensive system health validation\n\n#### Data Investigation Tools\n\n- **Stream Inspection**: `ExESDB.Debugger.streams()` - List all streams in the system\n- **Event Browsing**: `ExESDB.Debugger.events(stream_id, opts)` - Browse events within specific streams\n- **Subscription Monitoring**: `ExESDB.Debugger.subscriptions()` - Track active subscriptions\n- **Emitter Pool Management**: `ExESDB.Debugger.emitters()` - Monitor emitter pools and workers\n\n#### Performance and Monitoring\n\n- **Performance Metrics**: `ExESDB.Debugger.performance()` - System memory, CPU, uptime statistics\n- **Resource Analysis**: `ExESDB.Debugger.top()` - Identify resource-heavy processes\n- **Observer Integration**: `ExESDB.Debugger.observer()` - Launch Erlang Observer GUI\n- **Memory Tracking**: Detailed memory usage analysis across all components\n\n#### Advanced Debugging Tools\n\n- **Function Tracing**: `ExESDB.Debugger.trace(module, function)` - Trace specific function calls\n- **Benchmarking**: `ExESDB.Debugger.benchmark(fun)` - Measure function performance\n- **Multi-Store Support**: All functions work seamlessly with multiple store configurations\n- **Built-in Help System**: `ExESDB.Debugger.help()` - Interactive command reference\n\n#### Technical Implementation\n\n- **Production-ready**: Comprehensive error handling with graceful degradation\n- **Rich output formatting**: Human-readable output with emojis and structured information\n- **Zero configuration**: Auto-discovery of stores and system configuration\n- **Integration libraries**: Added `:recon ~> 2.5` dependency for enhanced process inspection\n\n#### Documentation and Examples\n\n- **Comprehensive guide**: New `guides/debugging.md` with complete usage examples\n- **Interactive examples**: Built-in help system with real-world usage patterns\n- **Development workflow**: Step-by-step debugging procedures for common scenarios\n- **Production troubleshooting**: Safe investigation procedures for live systems\n\n#### Benefits\n\n- **Faster development**: Quickly understand system state and identify issues\n- **Enhanced operations**: Real-time monitoring and health checking capabilities\n- **Better debugging**: Trace function calls and analyze performance bottlenecks\n- **Learning tool**: Visual representation of ExESDB architecture and process relationships\n- **Multi-environment**: Works in development, testing, and production environments","title":"Comprehensive Debugging and Monitoring System - Changelog","ref":"changelog.html#comprehensive-debugging-and-monitoring-system"},{"type":"extras","doc":"","title":"version 0.3.5 (2025.07.18) - Changelog","ref":"changelog.html#version-0-3-5-2025-07-18"},{"type":"extras","doc":"#### Problem Resolution\n\n- **Eliminated remaining fence operations**: Completed modernization of all writer components to use asynchronous persistence\n- **Consistent performance**: All write operations now benefit from the same non-blocking persistence pattern\n- **Removed synchronous bottlenecks**: No more blocking fence operations throughout the entire system\n\n#### Updated Components\n\n- **ExESDB.SubscriptionsWriter**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n- **ExESDB.SnapshotsWriterWorker**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n- **subscriptions_store.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n- **subscriptions_procs.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n- **streams_procs.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n\n#### Technical Implementation\n\n- **Unified persistence pattern**: All writers now use the same asynchronous persistence mechanism\n- **Elixir-Erlang interoperability**: Erlang modules call Elixir PersistenceWorker using proper atom syntax\n- **Consistent behavior**: All write operations follow the same pattern: immediate memory write + async persistence request\n- **Complete fence removal**: No synchronous fence operations remain in the writer layer\n\n#### Benefits\n\n- **System-wide performance**: All write operations now complete in milliseconds instead of seconds\n- **Eliminated timeout risks**: No more 5-second timeout failures across any writer component\n- **Consistent user experience**: All write operations provide immediate feedback\n- **Maintained durability**: Data persistence still guaranteed through background worker\n- **Reduced complexity**: Single persistence mechanism across all writers","title":"Complete Writer Modernization - Changelog","ref":"changelog.html#complete-writer-modernization"},{"type":"extras","doc":"","title":"version 0.3.4 (2025.07.18) - Changelog","ref":"changelog.html#version-0-3-4-2025-07-18"},{"type":"extras","doc":"#### Problem Resolution\n\n- **Eliminated timeout failures**: Resolved 5-second timeout issues in `append_events` operations that were caused by synchronous fence operations\n- **Improved performance**: Event append operations now complete in ~10-50ms instead of 5+ seconds\n- **Enhanced user experience**: System no longer appears unresponsive during data-heavy operations\n\n#### New Components\n\n- **ExESDB.PersistenceWorker**: New GenServer that handles disk persistence operations asynchronously\n  - Configurable persistence intervals (default: 5 seconds)\n  - Batching of persistence requests for efficiency\n  - Non-blocking API for event operations\n  - Graceful shutdown with final persistence\n  - Comprehensive error handling and logging\n\n#### Updated Components\n\n- **ExESDB.PersistenceSystem**: Enhanced to include PersistenceWorker as a managed component\n- **ExESDB.StreamsWriterWorker**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n\n#### New APIs\n\n- **`ExESDB.PersistenceWorker.request_persistence(store_id)`**: Non-blocking call to request persistence\n- **`ExESDB.PersistenceWorker.force_persistence(store_id)`**: Blocking call for immediate persistence (useful for testing)\n\n#### Configuration Options\n\n- **Global configuration**: `config :ex_esdb, persistence_interval: 10_000`\n- **Per-store configuration**: `opts = [store_id: :my_store, persistence_interval: 5_000]`\n\n#### Technical Implementation\n\n- **Asynchronous persistence**: Events are written to memory immediately, persistence happens in background\n- **Batching optimization**: Multiple persistence requests are deduplicated and processed together\n- **Eventual consistency**: Data is eventually persisted (within persistence interval)\n- **Fault tolerance**: System continues operating even if persistence is delayed\n\n#### Benefits\n\n- **Immediate response**: Event append operations return instantly\n- **Better throughput**: Batched disk operations are more efficient\n- **Configurable intervals**: Persistence frequency can be tuned per environment\n- **Backward compatibility**: All existing APIs continue to work unchanged\n- **No breaking changes**: Optional configuration with sensible defaults\n\n#### Documentation\n\n- **New guide**: Added comprehensive `guides/persistence_architecture.md` guide\n- **Implementation details**: Complete architecture overview and migration notes\n- **Testing guidance**: Instructions for integration tests using `force_persistence`","title":"Asynchronous Persistence System - Changelog","ref":"changelog.html#asynchronous-persistence-system"},{"type":"extras","doc":"","title":"version 0.3.3 (2025.07.18) - Changelog","ref":"changelog.html#version-0-3-3-2025-07-18"},{"type":"extras","doc":"#### Khepri Fence Operations Implementation\n\n- **Enhanced data durability**: Added `fence` operations after all Khepri write operations to ensure data persistence to disk\n- **Guaranteed write consistency**: All write operations now block until data is committed and persisted through Ra's write-ahead logging\n- **Improved reliability**: Prevents data loss in case of system crashes or power failures\n\n#### Updated Components\n\n- **ExESDB.SubscriptionsWriter**: Added fence operations after direct `:khepri.delete!` calls and indirect subscription store operations\n- **ExESDB.SnapshotsWriterWorker**: Added fence operations after `:khepri.delete!` and `:khepri.put!` operations for snapshot management\n- **ExESDB.StreamsWriterWorker**: Added fence operations after `:khepri.put!` calls in event recording operations\n- **subscriptions_store.erl**: Added fence operations after all `khepri:put`, `khepri:update`, and `khepri:delete` operations\n- **streams_procs.erl**: Added fence operations after `khepri:put` operations for event procedure registration\n- **subscriptions_procs.erl**: Added fence operations after `khepri:put` operations for subscription procedure registration\n\n#### Technical Implementation\n\n- **Write-through persistence**: All write operations now call `:khepri.fence(store)` immediately after data modification\n- **Consistency guarantees**: Fence operations ensure that subsequent reads will see the results of all previous writes\n- **Durability assurance**: Data is guaranteed to be persisted to disk before write operations complete\n- **Performance consideration**: Fence operations provide strong consistency at the cost of slightly increased write latency\n\n#### Benefits\n\n- **Data integrity**: Eliminates risk of data loss during system failures\n- **Consistency guarantees**: Ensures all write operations are properly persisted before completion\n- **Improved reliability**: Provides stronger durability guarantees for critical event store operations\n- **Operational confidence**: Reduces risk of data corruption in production environments","title":"Data Persistence and Durability Enhancement - Changelog","ref":"changelog.html#data-persistence-and-durability-enhancement"},{"type":"extras","doc":"","title":"version 0.3.1 (2025.07.17) - Changelog","ref":"changelog.html#version-0-3-1-2025-07-17"},{"type":"extras","doc":"#### Problem Resolution\n\n- **Fixed LeaderWorker registration warnings**: Resolved warning messages \"LeaderWorker registration issue: expected #PID<...>, got nil\"\n- **Store-specific registration verification**: LeaderWorker now properly verifies its registration using the store-specific name instead of the hardcoded module name\n- **Improved logging**: Registration confirmation logs now show the actual store-specific process name\n\n#### Technical Implementation\n\n- **Updated init/1 function**: LeaderWorker now extracts store_id from config and uses `StoreNaming.genserver_name/2` to determine the expected registration name\n- **Correct registration check**: Uses `Process.whereis(expected_name)` instead of `Process.whereis(__MODULE__)` for verification\n- **Enhanced logging**: Log messages now show the actual store-specific name used for registration\n\n#### Benefits\n\n- **Cleaner logs**: Eliminates confusing registration warning messages during startup\n- **Better debugging**: Registration verification now works correctly with store-specific naming\n- **Improved reliability**: Proper registration verification helps identify actual process registration issues","title":"LeaderWorker Registration Fix - Changelog","ref":"changelog.html#leaderworker-registration-fix"},{"type":"extras","doc":"","title":"version 0.3.0 (2025.07.17) - Changelog","ref":"changelog.html#version-0-3-0-2025-07-17"},{"type":"extras","doc":"#### Problem Resolution\n\n- **Fixed naming conflicts**: Resolved `already started` errors when multiple umbrella applications attempted to start their own ExESDB systems\n- **Store isolation**: Each store now has its own isolated set of partition supervisors\n- **Multi-tenancy support**: Different applications can maintain completely separate event stores within the same Elixir node\n\n#### Updated Components\n\n- **ExESDB.Emitters**: Updated `start_emitter_pool/3` to use store-specific partition names\n- **ExESDB.EmitterSystem**: Updated supervisor child spec to use store-specific partition names\n- **ExESDB.GatewaySystem**: Updated supervisor child spec to use store-specific partition names\n- **ExESDB.LeaderWorker**: Updated process lookup to use store-specific partition names\n- **ExESDB.Snapshots**: Updated supervisor child specs to use store-specific partition names\n- **ExESDB.SnapshotsReader**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.SnapshotsWriter**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.Streams**: Updated supervisor child specs to use store-specific partition names\n- **ExESDB.StreamsReader**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.StreamsWriter**: Updated `start_child/2` to use store-specific partition names\n\n#### Technical Implementation\n\n- **Store-specific naming**: All modules now use `ExESDB.StoreNaming.partition_name/2` to generate unique process names\n- **Unique process names**: Each store gets its own partition supervisors (e.g., `:exesdb_streamswriters_store_one`, `:exesdb_streamswriters_store_two`)\n- **Backward compatibility**: Existing single-store deployments continue to work unchanged\n- **Consistent pattern**: All partition supervisors follow the same naming convention\n\n#### Affected Processes\n\n- **ExESDB.StreamsWriters**: Now store-specific to prevent conflicts\n- **ExESDB.StreamsReaders**: Now store-specific to prevent conflicts\n- **ExESDB.SnapshotsWriters**: Now store-specific to prevent conflicts\n- **ExESDB.SnapshotsReaders**: Now store-specific to prevent conflicts\n- **ExESDB.EmitterPools**: Now store-specific to prevent conflicts\n- **ExESDB.GatewayWorkers**: Now store-specific to prevent conflicts\n\n#### Benefits\n\n- **True multi-tenancy**: Multiple applications can run separate ExESDB systems without conflicts\n- **Better isolation**: Each store operates independently with its own resources\n- **Improved reliability**: Eliminates startup failures due to naming conflicts\n- **Enhanced scalability**: Supports complex umbrella application architectures\n- **Maintained compatibility**: Zero breaking changes for existing deployments","title":"Multiple Stores Naming Conflicts Fix - Changelog","ref":"changelog.html#multiple-stores-naming-conflicts-fix"},{"type":"extras","doc":"","title":"version 0.1.7 (2025.07.16) - Changelog","ref":"changelog.html#version-0-1-7-2025-07-16"},{"type":"extras","doc":"#### Legacy Configuration Removal\n\n- **Removed khepri configuration**: Eliminated legacy `config :ex_esdb, :khepri` configuration format\n- **Modernized Options module**: Updated `ExESDB.Options` to use umbrella-aware configuration patterns\n- **Consistent configuration format**: All configurations now use `config :your_app_name, :ex_esdb, [options]` format\n- **Improved isolation**: Better configuration isolation between applications in umbrella projects\n\n#### Configuration Documentation\n\n- **New configuration guide**: Added comprehensive `guides/configuring_exesdb_apps.md` guide\n- **Standalone application examples**: Complete configuration examples for single-app deployments\n- **Umbrella application examples**: Detailed examples for multi-app umbrella projects\n- **Environment variable documentation**: Complete reference for all environment variable overrides\n- **Migration guide**: Instructions for migrating from legacy khepri configuration\n\n#### Configuration Features\n\n- **Context management**: Enhanced context switching for umbrella applications\n- **Explicit context support**: Added `app_env/3` functions for explicit context usage\n- **Context wrapper support**: Added `with_context/2` for scoped configuration access\n- **Libcluster integration**: Emphasized libcluster usage over deprecated seed_nodes mechanism\n\n#### Benefits\n\n- **Better umbrella support**: Proper configuration isolation for umbrella applications\n- **Clearer configuration patterns**: Consistent `config :app_name, :ex_esdb` format\n- **Improved developer experience**: Comprehensive documentation and examples\n- **Enhanced maintainability**: Removed legacy code paths and simplified configuration logic","title":"Configuration System Modernization - Changelog","ref":"changelog.html#configuration-system-modernization"},{"type":"extras","doc":"","title":"version 0.1.2 (2025.07.14) - Changelog","ref":"changelog.html#version-0-1-2-2025-07-14"},{"type":"extras","doc":"#### New Environment Variables\n\n- **`EX_ESDB_STORE_DESCRIPTION`**: Human-readable description of the store for documentation and operational purposes\n- **`EX_ESDB_STORE_TAGS`**: Comma-separated tags for store categorization and filtering (e.g., \"production,cluster,core\")\n\n#### Rich Store Configuration\n\n- **Operational Metadata**: Added `created_at`, `version`, and `status` fields to store configuration\n- **Resource Management**: Added `priority` and `auto_start` fields for resource allocation control\n- **Administrative Info**: Enhanced store registry with `tags`, `environment`, and `description` fields\n- **Enhanced Querying**: Store registry now supports filtering by tags, environment, priority, and other metadata\n\n#### Configuration Integration\n\n- **Runtime Configuration**: New environment variables integrated into `config/runtime.exs`\n- **Options System**: Added parsers in `ExESDB.Options` with comma-separated tag parsing\n- **Store Registry**: Enhanced `build_store_config/1` to include all new metadata fields\n- **Development Environment**: Updated `dev-env/*.yaml` files with new environment variables","title":"Store Configuration Enhancement - Changelog","ref":"changelog.html#store-configuration-enhancement"},{"type":"extras","doc":"#### NotificationSystem Introduction\n\n- **New Core Component**: Created `ExESDB.NotificationSystem` as a core supervisor for event notification\n- **Leadership Integration**: Moved `LeaderSystem` from clustering layer to core system\n- **Core System Enhancement**: Updated `CoreSystem` to include `NotificationSystem` alongside `PersistenceSystem` and `StoreSystem`\n- **Supervision Order**: `PersistenceSystem`  `NotificationSystem`  `StoreSystem` for proper dependency management\n\n#### LeaderWorker Availability Fix\n\n- **Core System Integration**: LeaderWorker now starts as part of core system, not clustering components\n- **Startup Order**: LeaderWorker is available before clustering components attempt to use it\n- **Resolved `:noproc` Error**: Fixed LeaderWorker activation failures by ensuring it's always running when needed\n- **Single and Cluster Mode**: LeaderWorker now available in both single-node and cluster modes\n\n#### System Architecture Cleanup\n\n- **Removed LeadershipSystem**: Consolidated functionality into `NotificationSystem`\n- **Cleaner Separation**: Core functionality (leadership, events) vs clustering (coordination, membership)\n- **Improved Documentation**: Updated supervision tree documentation to reflect new architecture\n- **Simplified Dependencies**: Reduced coupling between core and clustering components","title":"Architectural Refactoring - Changelog","ref":"changelog.html#architectural-refactoring"},{"type":"extras","doc":"#### Graceful Shutdown Implementation\n\n- **Universal Coverage**: All 18 GenServer processes now implement graceful shutdown\n- **Terminate Callbacks**: Added `terminate/2` callbacks to all GenServers for proper cleanup\n- **Exit Trapping**: Enabled `Process.flag(:trap_exit, true)` on all GenServers\n- **Resource Cleanup**: Proper cleanup of Swarm registrations, PubSub subscriptions, and Khepri stores\n\n#### Enhanced Process Lifecycle\n\n- **Swarm Registration Cleanup**: Worker processes properly unregister from Swarm on shutdown\n- **PubSub Subscription Cleanup**: EventProjector properly unsubscribes from topics\n- **Khepri Store Shutdown**: Store processes gracefully stop Khepri instances\n- **Network Monitoring**: NodeMonitor properly disables network monitoring on shutdown","title":"Process Management Enhancement - Changelog","ref":"changelog.html#process-management-enhancement"},{"type":"extras","doc":"#### Configuration Updates\n\n- **proc-sup Configuration**: Added description \"Process Supervisor Event Store\" and tags \"development,cluster,proc-sup,core\"\n- **reg-gh Configuration**: Added description \"Registration System Event Store\" and tags \"development,cluster,reg-gh,registration\"\n- **Environment-Specific Tags**: Different tags for development vs production environments\n- **Consistent Formatting**: Standardized environment variable layout across all configuration files","title":"Development Environment - Changelog","ref":"changelog.html#development-environment"},{"type":"extras","doc":"#### Operational Improvements\n\n- **Enhanced Monitoring**: Rich store metadata enables better operational visibility\n- **Improved Debugging**: Store descriptions and tags help identify issues faster\n- **Better Resource Management**: Priority and auto-start fields enable fine-grained control\n- **Cleaner Shutdown**: All processes terminate gracefully without resource leaks\n\n#### Development Experience\n\n- **Clearer Architecture**: Separation of core vs clustering concerns\n- **Consistent Configuration**: Standardized environment variable management\n- **Better Testability**: Core components can be tested independently of clustering\n- **Simplified Debugging**: LeaderWorker availability issues resolved\n\n#### System Reliability\n\n- **Reduced Race Conditions**: Proper startup order prevents timing-related failures\n- **Resource Leak Prevention**: Graceful shutdown prevents resource accumulation\n- **Improved Fault Tolerance**: Better separation of concerns reduces cascade failures\n- **Enhanced Observability**: Rich metadata supports better monitoring and alerting","title":"Benefits - Changelog","ref":"changelog.html#benefits"},{"type":"extras","doc":"","title":"version 0.1.1 (2025.07.13) - Changelog","ref":"changelog.html#version-0-1-1-2025-07-13"},{"type":"extras","doc":"#### Enhanced Architecture\n\n- **Store Registration Centralization**: Moved store registration functionality from `StoreCluster` to dedicated `StoreRegistry` module\n- **Self-Registration**: `StoreRegistry` now automatically registers its own store during initialization when `store_id` is provided\n- **Simplified StoreCluster**: `StoreCluster` now focuses purely on cluster coordination without store registration concerns\n\n#### API Integration\n\n- **ExESDBGater.API Integration**: `list_stores()` function now directly calls `ExESDB.StoreRegistry.list_stores()` instead of maintaining local state\n- **Single Source of Truth**: Store information is now centralized in `StoreRegistry` across the entire system\n- **Improved Error Handling**: Added proper error handling for StoreRegistry calls in GaterAPI\n\n#### System Integration\n\n- **StoreSystem Supervision**: Added `StoreRegistry` to the `StoreSystem` supervisor with proper startup order\n- **Component Isolation**: Each component now has a single, well-defined responsibility\n- **Cleaner State Management**: Removed redundant store state from multiple components\n\n#### Benefits\n\n- **Separation of Concerns**: Clear boundaries between clustering and registration responsibilities\n- **Maintainability**: Easier to maintain and reason about store registration logic\n- **Testability**: Store registration can now be tested in isolation\n- **Reduced Coupling**: Components are less tightly coupled and more modular","title":"StoreRegistry Refactoring - Changelog","ref":"changelog.html#storeregistry-refactoring"},{"type":"extras","doc":"","title":"version 0.0.17 (2025.07.01) - Changelog","ref":"changelog.html#version-0-0-17-2025-07-01"},{"type":"extras","doc":"- `ExESDB` nodes now automatically join the cluster\n- \"Split-Brain\" scenarios are now mitigated","title":"Auto-Clustering - Changelog","ref":"changelog.html#auto-clustering"},{"type":"extras","doc":"- All functionality related to styling is now transferred to the `:bc_utils` package.\n- Added a Banner after startup.\n- Logger filtering for Swarm and LibCluster noise reduction (via BCUtils.LoggerFilters)","title":"BCUtils - Changelog","ref":"changelog.html#bcutils"},{"type":"extras","doc":"#### Features\n\nThe `ExESDB.LoggerFilters` module provides additional log noise reduction specifically for ExESDB's distributed systems components:\n\n- **Ra Consensus Filtering**: Reduces Ra heartbeat, append_entries, pre_vote, request_vote, and routine state transition messages while preserving all errors/warnings\n- **Khepri Database Filtering**: Filters internal Khepri operations (cluster state, store operations) at info/debug levels while maintaining error/warning visibility\n- **Enhanced Swarm Filtering**: Complements BCUtils filtering with additional ExESDB-specific Swarm noise reduction\n- **Enhanced LibCluster Filtering**: Complements BCUtils filtering with additional ExESDB-specific cluster formation noise reduction\n\n#### Benefits\n\n- Dramatically improves log readability in development and production environments\n- Intelligent filtering preserves all error and warning messages\n- Focused on ExESDB-specific distributed systems infrastructure (Ra, Khepri)\n- Works in conjunction with BCUtils.LoggerFilters for comprehensive noise reduction","title":"ExESDB Logger Filtering - Changelog","ref":"changelog.html#exesdb-logger-filtering"},{"type":"extras","doc":"- The `ExESDB.GatewayAPI` is moved to the `:ex_esdb_gater` package.\n\n#### Features\n\nSnapshots Subsystem provides cluster wide support for reading and writing snapshots, using a key derived from the `source_uuid`, `stream_uuid` and `version` of the snapshot.","title":"ExESDBGater - Changelog","ref":"changelog.html#exesdbgater"},{"type":"extras","doc":"","title":"version 0.0.16 (2025.06.26) - Changelog","ref":"changelog.html#version-0-0-16-2025-06-26"},{"type":"extras","doc":"#### Features\n\nSnapshots Subsystem provides cluster wide support for reading and writing snapshots, using a key derived from the `source_uuid`, `stream_uuid` and `version` of the snapshot.\n\n- `record_snapshot/5` function\n- `delete_snapshot/4` function\n- `read_snapshot/4` function\n- `list_snapshots/3` function\n\n#### Supported by Gateway API\n\nIt is advised to use `ExESDB.GatewayAPI` to access the Snapshots Subsystem.","title":"Snapshots - Changelog","ref":"changelog.html#snapshots"},{"type":"extras","doc":"","title":"version 0.0.15 (2025.06.15) - Changelog","ref":"changelog.html#version-0-0-15-2025-06-15"},{"type":"extras","doc":"#### Transient subscriptions\n\n- `:by_stream`, `:by_event_type`, `:by_event_pattern`, `:by_event_payload`\n- Events are forwarded to `Phoenix.PubSub` for now\n\n#### Persistent subscriptions\n\n- `:by_stream`, with support for replaying from a given version\n- Events are forwarded to a specific subscriber process\n- `ack_event/3` function is provided\n\n#### \"Follow-the-Leader\"\n\nEmitter processes are automatically started on the leader node,\nwhen a new leader is elected.\n\n#### Gateway API\n\n- A cluster-wide gateway API is provided\n- is an entry point for all the other modules\n- provides basic High-Availability and Load-Balancing","title":"Subscriptions - Changelog","ref":"changelog.html#subscriptions"},{"type":"extras","doc":"","title":"version 0.0.9-alpha (2025.05.04) - Changelog","ref":"changelog.html#version-0-0-9-alpha-2025-05-04"},{"type":"extras","doc":"- `ExESDB.Subscriptions` module\n- `func_registrations.exs` file\n- emitter trigger in `khepri` now only uses the `erlang`-native :pg library (process groups)\n\n#### Skeleton support for Commanded\n\n- `ExESDB.Commanded.Adapter` module\n- `ExESDB.Commanded.Mapper` module","title":"Subscriptions - Changelog","ref":"changelog.html#subscriptions-1"},{"type":"extras","doc":"","title":"version 0.0.8-alpha - Changelog","ref":"changelog.html#version-0-0-8-alpha"},{"type":"extras","doc":"- Added `ExESDB.EventStore.stream_forward/4` function\n- Added `BeamCampus.ColorFuncs` module\n- Added `ExESDB.Commanded.Adapter` module\n- Refactored `ExESDB.EventStreamReader` and `ExESDB.EventStreamWriter` modules:\n- Streams are now read and written using the `ExESDB.Streams` module\n- Removed `ExESDB.EventStreamReader` module\n- Removed `ExESDB.EventStreamWriter` module","title":"2025.04.13 - Changelog","ref":"changelog.html#2025-04-13"},{"type":"extras","doc":"","title":"version 0.0.7-alpha - Changelog","ref":"changelog.html#version-0-0-7-alpha"},{"type":"extras","doc":"","title":"version 0.0.1-alpha - Changelog","ref":"changelog.html#version-0-0-1-alpha"},{"type":"extras","doc":"- Initial release","title":"2025.03.25 - Changelog","ref":"changelog.html#2025-03-25"},{"type":"extras","doc":"# Getting Started with ExESDB","title":"Getting Started","ref":"getting-started.html"},{"type":"extras","doc":"Event Sourcing with CQRS is a technique for building applications that are based on an immutable log of events, which makes it ideal for building concurrent, distributed systems.\n\nThough it is gaining popularity, the number of options for storing these events is limited and require specialized services like Kurrent (aka Greg's EventStore) or AxonIQ.\n\nOne of the strong-points of the BEAM is, that it comes 'batteries included': there are BEAM-native libraries for many common tasks, like: storage, pub/sub, caching, logging, telemetry, etc.\n\n`ExESDB` is an attempt to create a BEAM-native Event Store written in Erlang/Elixir, building further upon the [Khepri](https://github.com/rabbitmq/khepri) library, which in turn builds upon the [Ra](https://github.com/rabbitmq/ra) library.","title":"Introduction - Getting Started","ref":"getting-started.html#introduction"},{"type":"extras","doc":"**This is a work in progress**\n\nThe project is in an early stage of development, and is not ready for production use.\n\nSource code is available on [GitHub](https://github.com/beam-campus/ex-esdb).","title":"Status - Getting Started","ref":"getting-started.html#status"},{"type":"extras","doc":"In your `mix.exs` file:\n\n```elixir\ndef deps do\n  [\n    {:ex_esdb, \"~> 0.0.16\"}\n  ]\nend\n```","title":"Installation - Getting Started","ref":"getting-started.html#installation"},{"type":"extras","doc":"> **Note**: For detailed configuration examples including umbrella applications, see the [Configuring ExESDB Applications](configuring-exesdb-apps.md) guide.\n\n1. in your `config/config.exs` file:\n\n```elixir\n# Example standalone configuration\n# If your app is named :my_event_store\nconfig :my_event_store, :ex_esdb,\n  data_dir: \"/var/lib/ex_esdb\",\n  store_id: :my_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :my_pubsub,\n  reader_idle_ms: 15000,\n  writer_idle_ms: 12000,\n  store_description: \"My Event Store\",\n  store_tags: [\"production\", \"events\"]\n\n# Configure libcluster (recommended)\nconfig :libcluster,\n  topologies: [\n    example: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"my_secret\"\n      ]\n    ]\n  ]\n```\n\n2. from the ENVIRONMENT:\n\n```bash\n\nEX_ESDB_DATA_DIR=\"/var/lib/ex_esdb\"\nEX_ESDB_STORE_ID=my_store\nEX_ESDB_DB_TYPE=cluster\nEX_ESDB_TIMEOUT=5000\nEX_ESDB_PUB_SUB=my_pubsub\n\n```","title":"Configuration - Getting Started","ref":"getting-started.html#configuration"},{"type":"extras","doc":"```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  @impl true\n  def start(_type, _args) do\nopts = ExESDB.Options.app_env(:my_event_store)\n    children = [\n      {ExESDB.System, opts},\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\n\nend\n```","title":"Usage - Getting Started","ref":"getting-started.html#usage"},{"type":"extras","doc":"# ExESDB Architecture Analysis","title":"Architecture","ref":"architecture.html"},{"type":"extras","doc":"ExESDB is a BEAM-native Event Store built on top of the Khepri library, which in turn is built on the Ra library. It's designed as a distributed, fault-tolerant event sourcing system that leverages the strengths of the BEAM ecosystem for handling concurrent, distributed workloads.","title":"Overview - Architecture","ref":"architecture.html#overview"},{"type":"extras","doc":"```mermaid\ngraph TD\n    A[ExESDB.App] --> B[ExESDB.System]\n    B --> C[PubSub Layer]\n    B --> D[Store Management]\n    B --> E[Cluster Management]\n    B --> F[Event Processing]\n    B --> G[Gateway Layer]\n    \n    D --> D1[StoreManager]\n    D --> D2[Store Workers]\n    D --> D3[Khepri Backend]\n    \n    E --> E1[LibCluster]\n    E --> E2[ClusterSystem]\n    E --> E3[KhepriCluster]\n    E --> E4[LeaderSystem]\n    \n    F --> F1[Streams]\n    F --> F2[Snapshots]\n    F --> F3[Subscriptions]\n    \n    G --> G1[GatewaySupervisor]\n    G --> G2[GatewayWorker]\n```","title":"High-Level Architecture - Architecture","ref":"architecture.html#high-level-architecture"},{"type":"extras","doc":"","title":"Core Components - Architecture","ref":"architecture.html#core-components"},{"type":"extras","doc":"#### ExESDB.App\n- **Purpose**: Main application entry point\n- **Responsibilities**:\n  - Application lifecycle management\n  - Initial configuration loading\n  - Starting the main supervisor tree\n  - Graceful shutdown handling\n\n#### ExESDB.System\n- **Purpose**: Top-level supervisor for the entire system\n- **Responsibilities**:\n  - Supervises all major subsystems\n  - Manages system startup sequence\n  - Handles OS signal processing\n  - Dynamically configures components based on deployment mode (single vs cluster)","title":"1. Application Layer - Architecture","ref":"architecture.html#1-application-layer"},{"type":"extras","doc":"#### ExESDB.StoreManager\n- **Purpose**: Multi-store management and coordination\n- **Responsibilities**:\n  - Dynamic store creation and removal\n  - Store lifecycle management\n  - Configuration management per store\n  - Store status tracking\n\n#### ExESDB.Store\n- **Purpose**: Individual event store wrapper around Khepri\n- **Responsibilities**:\n  - Khepri store initialization\n  - Store state management\n  - Direct interaction with Khepri API\n\n```mermaid\ngraph LR\n    A[StoreManager] --> B[Store1]\n    A --> C[Store2]\n    A --> D[StoreN]\n    \n    B --> E[Khepri Instance 1]\n    C --> F[Khepri Instance 2]\n    D --> G[Khepri Instance N]\n    \n    E --> H[Data Directory 1]\n    F --> I[Data Directory 2]\n    G --> J[Data Directory N]\n```","title":"2. Storage Layer - Architecture","ref":"architecture.html#2-storage-layer"},{"type":"extras","doc":"The clustering layer provides distributed coordination and fault tolerance:\n\n#### ExESDB.KhepriCluster\n- **Purpose**: Khepri-specific cluster coordination\n- **Responsibilities**:\n  - Cluster join/leave operations\n  - Leadership detection and tracking\n  - Membership monitoring\n  - Node health monitoring\n\n#### ExESDB.ClusterSystem\n- **Purpose**: High-level cluster coordination\n- **Responsibilities**:\n  - Supervises cluster coordination components\n  - Manages cluster-specific services\n  - Handles split-brain prevention\n\n#### ExESDB.LeaderSystem\n- **Purpose**: Leadership management\n- **Responsibilities**:\n  - Leader election coordination\n  - Leader-specific functionality activation\n  - Leader state tracking\n\n```mermaid\ngraph TD\n    A[LibCluster] --> B[Node Discovery]\n    B --> C[KhepriCluster]\n    C --> D[Cluster Join/Leave]\n    C --> E[Leadership Detection]\n    C --> F[Membership Monitoring]\n    \n    G[ClusterSystem] --> H[ClusterCoordinator]\n    G --> I[NodeMonitor]\n    \n    J[LeaderSystem] --> K[LeaderWorker]\n    J --> L[LeaderTracker]\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style J fill:#e8f5e8\n```","title":"3. Clustering Layer - Architecture","ref":"architecture.html#3-clustering-layer"},{"type":"extras","doc":"#### ExESDB.Streams\n- **Purpose**: Event stream management\n- **Responsibilities**:\n  - Stream read/write operations\n  - Stream partitioning via PartitionSupervisor\n  - Worker pool management for stream operations\n\n#### ExESDB.Snapshots\n- **Purpose**: Snapshot management for event sourcing\n- **Responsibilities**:\n  - Snapshot creation and retrieval\n  - Snapshot versioning\n  - Snapshot storage path management\n\n#### ExESDB.Subscriptions\n- **Purpose**: Event subscription management\n- **Responsibilities**:\n  - Subscription lifecycle management\n  - Event delivery to subscribers\n  - Subscription persistence\n\n```mermaid\ngraph TD\n    A[Streams] --> B[StreamsWriters Pool]\n    A --> C[StreamsReaders Pool]\n    \n    D[Snapshots] --> E[SnapshotsWriters Pool]\n    D --> F[SnapshotsReaders Pool]\n    \n    G[Subscriptions] --> H[SubscriptionsReader]\n    G --> I[SubscriptionsWriter]\n    \n    B --> J[DynamicSupervisor]\n    C --> K[DynamicSupervisor]\n    E --> L[DynamicSupervisor]\n    F --> M[DynamicSupervisor]\n    \n    J --> N[StreamWriterWorker1]\n    J --> O[StreamWriterWorker2]\n    K --> P[StreamReaderWorker1]\n    K --> Q[StreamReaderWorker2]\n```","title":"4. Event Processing Layer - Architecture","ref":"architecture.html#4-event-processing-layer"},{"type":"extras","doc":"#### PubSub Integration\n- **Purpose**: Inter-process and inter-node communication\n- **Responsibilities**:\n  - Event broadcasting\n  - Subscription management\n  - Message routing\n\n#### ExESDB.GatewaySupervisor & GatewayWorker\n- **Purpose**: External API gateway\n- **Responsibilities**:\n  - External client request handling\n  - API endpoint management\n  - Request routing to appropriate subsystems","title":"5. Communication Layer - Architecture","ref":"architecture.html#5-communication-layer"},{"type":"extras","doc":"","title":"Architecture Patterns - Architecture","ref":"architecture.html#architecture-patterns"},{"type":"extras","doc":"```mermaid\ngraph TD\n    A[ExESDB.App] --> B[ExESDB.System]\n    B --> C[PubSub]\n    B --> D[StoreManager]\n    B --> E[Streams]\n    B --> F[Snapshots]\n    B --> G[Subscriptions]\n    B --> H[LeaderSystem]\n    B --> I[KhepriCluster]\n    B --> J[GatewaySupervisor]\n    B --> K[ClusterSystem]\n    B --> L[EmitterPools]\n    \n    E --> M[PartitionSupervisor - Writers]\n    E --> N[PartitionSupervisor - Readers]\n    \n    F --> O[PartitionSupervisor - Writers]\n    F --> P[PartitionSupervisor - Readers]\n    \n    H --> Q[LeaderWorker]\n    H --> R[LeaderTracker]\n    \n    K --> S[ClusterCoordinator]\n    K --> T[NodeMonitor]\n    \n    J --> U[GatewayWorker]\n```","title":"1. Supervision Tree Pattern - Architecture","ref":"architecture.html#1-supervision-tree-pattern"},{"type":"extras","doc":"ExESDB extensively uses worker pools for different types of operations:\n\n```mermaid\ngraph LR\n    A[Client Request] --> B[PartitionSupervisor]\n    B --> C[DynamicSupervisor]\n    C --> D[Worker1]\n    C --> E[Worker2]\n    C --> F[WorkerN]\n    \n    G[Hash Ring] --> B\n    H[Load Balancing] --> B\n```","title":"2. Worker Pool Pattern - Architecture","ref":"architecture.html#2-worker-pool-pattern"},{"type":"extras","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant G as Gateway\n    participant L as Leader\n    participant F as Follower\n    participant K as Khepri\n    \n    C->>G: Write Request\n    G->>L: Route to Leader\n    L->>K: Write to Khepri\n    K->>F: Replicate to Followers\n    F->>K: Acknowledge\n    K->>L: Confirm Write\n    L->>G: Success Response\n    G->>C: Return Result\n```","title":"3. Distributed State Management - Architecture","ref":"architecture.html#3-distributed-state-management"},{"type":"extras","doc":"","title":"Deployment Modes - Architecture","ref":"architecture.html#deployment-modes"},{"type":"extras","doc":"- **Configuration**: `db_type: :single`\n- **Characteristics**:\n  - No clustering components\n  - Local-only operations\n  - Simplified architecture\n  - Development/testing focused","title":"Single Node Mode - Architecture","ref":"architecture.html#single-node-mode"},{"type":"extras","doc":"- **Configuration**: `db_type: :cluster`\n- **Characteristics**:\n  - Full clustering capabilities\n  - Distributed consensus via Ra\n  - Fault tolerance\n  - Production-ready\n\n```mermaid\ngraph TD\n    A[Configuration] --> B{db_type}\n    B -->|:single| C[Single Node Components]\n    B -->|:cluster| D[Cluster Components]\n    \n    C --> E[StoreManager]\n    C --> F[Local Streams]\n    C --> G[Local Snapshots]\n    \n    D --> H[LibCluster]\n    D --> I[ClusterSystem]\n    D --> J[KhepriCluster]\n    D --> K[Distributed Components]\n```","title":"Cluster Mode - Architecture","ref":"architecture.html#cluster-mode"},{"type":"extras","doc":"","title":"Data Flow - Architecture","ref":"architecture.html#data-flow"},{"type":"extras","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant GW as Gateway\n    participant SM as StoreManager\n    participant S as Store\n    participant K as Khepri\n    participant PS as PubSub\n    \n    C->>GW: Write Event\n    GW->>SM: Route to Store\n    SM->>S: Write Request\n    S->>K: Store Event\n    K->>S: Confirm Write\n    S->>PS: Publish Event\n    PS->>C: Event Notification\n    S->>GW: Success Response\n    GW->>C: Return Result\n```","title":"Event Write Flow - Architecture","ref":"architecture.html#event-write-flow"},{"type":"extras","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant GW as Gateway\n    participant SR as StreamReader\n    participant S as Store\n    participant K as Khepri\n    \n    C->>GW: Read Stream\n    GW->>SR: Route to Reader\n    SR->>S: Read Request\n    S->>K: Query Events\n    K->>S: Return Events\n    S->>SR: Event Data\n    SR->>GW: Stream Response\n    GW->>C: Return Events\n```","title":"Event Read Flow - Architecture","ref":"architecture.html#event-read-flow"},{"type":"extras","doc":"","title":"Key Design Decisions - Architecture","ref":"architecture.html#key-design-decisions"},{"type":"extras","doc":"- **Rationale**: BEAM-native, Ra-based distributed database\n- **Benefits**: \n  - Native Erlang integration\n  - Built-in clustering\n  - Strong consistency guarantees\n  - Fault tolerance","title":"1. Khepri as Backend - Architecture","ref":"architecture.html#1-khepri-as-backend"},{"type":"extras","doc":"- **Rationale**: Leverages OTP supervision principles\n- **Benefits**:\n  - Fault isolation\n  - Automatic restart strategies\n  - System resilience\n  - Clear responsibility boundaries","title":"2. Supervisor Tree Architecture - Architecture","ref":"architecture.html#2-supervisor-tree-architecture"},{"type":"extras","doc":"- **Rationale**: Efficient concurrent processing\n- **Benefits**:\n  - Load distribution\n  - Resource management\n  - Scalability\n  - Fault tolerance","title":"3. Worker Pool Pattern - Architecture","ref":"architecture.html#3-worker-pool-pattern"},{"type":"extras","doc":"- **Rationale**: Support for multiple event stores in single cluster\n- **Benefits**:\n  - Tenant isolation\n  - Resource optimization\n  - Flexible deployment\n  - Gradual migration support","title":"4. Multi-Store Architecture - Architecture","ref":"architecture.html#4-multi-store-architecture"},{"type":"extras","doc":"","title":"Performance Considerations - Architecture","ref":"architecture.html#performance-considerations"},{"type":"extras","doc":"- Uses PartitionSupervisor for distributing workload\n- Hash-based routing for even distribution\n- Separate pools for read/write operations","title":"Partitioning Strategy - Architecture","ref":"architecture.html#partitioning-strategy"},{"type":"extras","doc":"- Configurable probe intervals for node monitoring\n- Failure thresholds to prevent cascade failures\n- Efficient membership change detection","title":"Clustering Optimization - Architecture","ref":"architecture.html#clustering-optimization"},{"type":"extras","doc":"- Dynamic worker creation/destruction\n- Configurable timeout values\n- Memory-efficient event storage via Khepri","title":"Resource Management - Architecture","ref":"architecture.html#resource-management"},{"type":"extras","doc":"","title":"Security Considerations - Architecture","ref":"architecture.html#security-considerations"},{"type":"extras","doc":"- Node-to-node communication via Erlang distribution\n- Cluster authentication via shared secrets\n- Network partitioning detection and handling","title":"Network Security - Architecture","ref":"architecture.html#network-security"},{"type":"extras","doc":"- Gateway-based request filtering\n- Store-level access control\n- Subscription-based permissions","title":"Access Control - Architecture","ref":"architecture.html#access-control"},{"type":"extras","doc":"","title":"Monitoring and Observability - Architecture","ref":"architecture.html#monitoring-and-observability"},{"type":"extras","doc":"- Built-in metrics module\n- Performance monitoring\n- Cluster health tracking","title":"Metrics Collection - Architecture","ref":"architecture.html#metrics-collection"},{"type":"extras","doc":"- Structured logging throughout\n- Configurable log levels\n- Cluster-aware log correlation","title":"Logging Strategy - Architecture","ref":"architecture.html#logging-strategy"},{"type":"extras","doc":"- Node health monitoring\n- Store availability checks\n- Leadership status tracking","title":"Health Checks - Architecture","ref":"architecture.html#health-checks"},{"type":"extras","doc":"","title":"Scalability Patterns - Architecture","ref":"architecture.html#scalability-patterns"},{"type":"extras","doc":"- Add nodes to existing cluster\n- Automatic workload redistribution\n- Leader election for coordination","title":"Horizontal Scaling - Architecture","ref":"architecture.html#horizontal-scaling"},{"type":"extras","doc":"- Worker pool sizing\n- Memory allocation tuning\n- Timeout configuration\n\nThis architecture provides a solid foundation for building distributed, fault-tolerant event sourcing systems while leveraging the unique strengths of the BEAM ecosystem.","title":"Vertical Scaling - Architecture","ref":"architecture.html#vertical-scaling"},{"type":"extras","doc":"","title":"Testing","ref":"testing.html"},{"type":"extras","doc":"# Failure Handling in ExESDB\n\nThis guide provides a comprehensive overview of all failure handling strategies implemented in ExESDB, from individual process failures to cluster-wide outages. ExESDB is built on the BEAM's \"let it crash\" philosophy while providing robust recovery mechanisms at every level.","title":"Failure Handling","ref":"failure-handling.html"},{"type":"extras","doc":"1. [Failure Categories](#failure-categories)\n2. [Supervision Strategies](#supervision-strategies)\n3. [Node-Level Failures](#node-level-failures)\n4. [Network Partitions](#network-partitions)\n5. [Data Consistency](#data-consistency)\n6. [Worker Process Failures](#worker-process-failures)\n7. [Storage Failures](#storage-failures)\n8. [Configuration and Monitoring](#configuration-and-monitoring)\n9. [Recovery Procedures](#recovery-procedures)\n10. [Testing Failure Scenarios](#testing-failure-scenarios)","title":"Table of Contents - Failure Handling","ref":"failure-handling.html#table-of-contents"},{"type":"extras","doc":"ExESDB handles failures across multiple dimensions:","title":"Failure Categories - Failure Handling","ref":"failure-handling.html#failure-categories"},{"type":"extras","doc":"- **Worker crashes**: Individual GenServer processes failing\n- **Supervisor crashes**: Supervisor trees failing\n- **Application crashes**: Entire OTP applications going down","title":"1. Process-Level Failures - Failure Handling","ref":"failure-handling.html#1-process-level-failures"},{"type":"extras","doc":"- **Hard crashes**: Sudden node termination (power loss, kill -9)\n- **Soft crashes**: Graceful shutdowns and restarts\n- **Network isolation**: Node becomes unreachable but continues running","title":"2. Node-Level Failures - Failure Handling","ref":"failure-handling.html#2-node-level-failures"},{"type":"extras","doc":"- **Split-brain scenarios**: Network partitions causing multiple leaders\n- **Quorum loss**: Insufficient nodes for consensus\n- **Data corruption**: Storage-level integrity issues","title":"3. Cluster-Level Failures - Failure Handling","ref":"failure-handling.html#3-cluster-level-failures"},{"type":"extras","doc":"- **Disk failures**: Storage becoming unavailable\n- **Corruption**: Data integrity violations\n- **Performance degradation**: Slow storage affecting operations","title":"4. Storage-Level Failures - Failure Handling","ref":"failure-handling.html#4-storage-level-failures"},{"type":"extras","doc":"ExESDB uses a hierarchical supervision tree with different restart strategies for different components:","title":"Supervision Strategies - Failure Handling","ref":"failure-handling.html#supervision-strategies"},{"type":"extras","doc":"```\nExESDB.App (one_for_one)\n ExESDB.System (one_for_one)\n     Phoenix.PubSub\n     Cluster.Supervisor (LibCluster)\n     PartitionSupervisor (EmitterPools)\n     ExESDB.Store\n     ExESDB.ClusterSystem (one_for_one)\n     ExESDB.Streams (one_for_one)\n     ExESDB.Snapshots\n     ExESDB.Subscriptions (one_for_one)\n     ExESDB.GatewaySupervisor (one_for_one)\n     ExESDB.LeaderSystem (one_for_one)\n```","title":"Root Supervision Tree - Failure Handling","ref":"failure-handling.html#root-supervision-tree"},{"type":"extras","doc":"| Component | Strategy | Restart | Reason |\n|-----------|----------|---------|--------|\n| **System** | `:one_for_one` | `:permanent` | Core system components |\n| **ClusterSystem** | `:one_for_one` | `:permanent` | Independent cluster services |\n| **Streams** | `:one_for_one` | `:permanent` | Stream readers/writers are independent |\n| **GatewaySupervisor** | `:one_for_one` | `:permanent` | Gateway API and workers |\n| **StreamsWriterWorker** | `:temporary` | `:temporary` | TTL-based lifecycle |\n| **StreamsReaderWorker** | `:temporary` | `:temporary` | On-demand workers |\n| **GatewayWorker** | `:permanent` | `:permanent` | Core gateway functionality |","title":"Restart Strategies by Component - Failure Handling","ref":"failure-handling.html#restart-strategies-by-component"},{"type":"extras","doc":"**Stream Workers**:\n- Use `:temporary` restart to prevent infinite restart loops\n- Implement TTL-based shutdown for resource management\n- Automatically clean up Swarm registrations on exit\n\n**Gateway Workers**:\n- Use `:permanent` restart for high availability\n- Register with Swarm for distributed load balancing\n- Handle graceful shutdown with cleanup","title":"Worker Lifecycle Management - Failure Handling","ref":"failure-handling.html#worker-lifecycle-management"},{"type":"extras","doc":"","title":"Node-Level Failures - Failure Handling","ref":"failure-handling.html#node-level-failures"},{"type":"extras","doc":"When a node crashes unexpectedly, multiple systems work together to detect and recover:\n\n#### 1. NodeMonitor Service (Fast Detection)\n\n**Problem**: Traditional Raft consensus timeouts can take 10-30 seconds\n**Solution**: Proactive health monitoring with 6-second detection\n\nThis solution implements a comprehensive approach to quickly detect and handle node failures:","title":"Hard Crash Detection and Recovery - Failure Handling","ref":"failure-handling.html#hard-crash-detection-and-recovery"},{"type":"extras","doc":"**Location**: `lib/ex_esdb/node_monitor.ex`\n\n**Features**:\n- **Active Health Probing**: Probes cluster nodes every 2 seconds\n- **Multi-Layer Health Checks**: Verifies node connectivity + application health\n- **Fast Failure Detection**: Marks nodes as failed after 3 consecutive probe failures (6 seconds total)\n- **Automatic Cleanup**: Removes stale Swarm registrations from failed nodes\n- **Event-Driven Updates**: Responds to `:nodeup`/`:nodedown` events\n\n**Configuration**:\n```elixir\n# Default settings (configurable)\nprobe_interval: 2_000,     # 2 seconds between probes\nfailure_threshold: 3,      # 3 failures = node considered down\nprobe_timeout: 1_000       # 1 second timeout per probe\n```","title":"1. NodeMonitor Service - Failure Handling","ref":"failure-handling.html#1-nodemonitor-service"},{"type":"extras","doc":"The NodeMonitor integrates seamlessly with your current LibCluster setup:\n\n**Modified Files**:\n- `lib/ex_esdb/cluster_system.ex` - Added NodeMonitor to supervision tree\n- Uses existing `ClusterCoordinator` for split-brain prevention\n- Leverages current `Swarm` registrations for worker cleanup","title":"2. Integration with Existing Architecture - Failure Handling","ref":"failure-handling.html#2-integration-with-existing-architecture"},{"type":"extras","doc":"#### Health Probe Cycle (Every 2 seconds):\n1. **Discover Nodes**: Get cluster members from Khepri\n2. **Probe Health**: Test each node with RPC calls\n3. **Track Failures**: Increment failure count for unresponsive nodes\n4. **Trigger Cleanup**: Handle nodes that exceed failure threshold\n5. **Update State**: Maintain monitoring state for next cycle\n\n#### Failure Detection:\n```elixir\n# Multi-layer health check\n1. Node connectivity: :rpc.call(node, :erlang, :node, [])\n2. Application health: Check if :ex_esdb is running\n3. Failure tracking: 3 consecutive failures = node down\n```\n\n#### Automatic Cleanup:\n```elixir\n# When a node is detected as failed:\n1. Remove Swarm worker registrations from failed node\n2. Update cluster state (notify other components)\n3. Clean up subscriptions tied to failed node\n4. Log failure for monitoring/alerting\n```","title":"3. How It Works - Failure Handling","ref":"failure-handling.html#3-how-it-works"},{"type":"extras","doc":"**Fast Detection**: \n- Traditional Raft consensus timeout: 10-30 seconds\n- This solution: 6 seconds (3 failures  2 second intervals)\n\n**Proactive Cleanup**:\n- Prevents requests to unavailable workers\n- Maintains cluster integrity\n- Enables faster recovery\n\n**Graceful Degradation**:\n- System continues operating with remaining nodes\n- Workers redistribute automatically via Swarm\n- No single point of failure","title":"4. Benefits - Failure Handling","ref":"failure-handling.html#4-benefits"},{"type":"extras","doc":"#### Check Cluster Health:\n```elixir\n# Get current health status\nExESDB.NodeMonitor.health_status()\n# Returns: %{\n#   monitored_nodes: [:node1@host, :node2@host],\n#   node_failures: %{},\n#   last_seen: %{:node1@host => 1625567890123},\n#   threshold: 3\n# }\n```\n\n#### Manual Node Probe:\n```elixir\n# Force probe a specific node\nExESDB.NodeMonitor.probe_node(:node1@host)\n# Returns: :healthy or :unhealthy\n```","title":"5. Usage - Failure Handling","ref":"failure-handling.html#5-usage"},{"type":"extras","doc":"**Environment Variables** (can be added to your config):\n```elixir\nconfig :ex_esdb, :node_monitor,\n  probe_interval: 2_000,      # How often to probe (ms)\n  failure_threshold: 3,       # Failures before marking as down\n  probe_timeout: 1_000,       # Timeout per probe (ms)\n  cleanup_stale_workers: true # Auto-cleanup Swarm registrations\n```","title":"6. Configuration Options - Failure Handling","ref":"failure-handling.html#6-configuration-options"},{"type":"extras","doc":"**Log Messages**:\n- `NodeMonitor started with 2000ms intervals`\n- `Health probe failed for node1@host (2/3)`\n- `Node node1@host detected as failed, initiating cleanup`\n- `Cleaning up Swarm registration: {:gateway_worker, node1@host, 1234}`\n\n**Integration Points**:\n- Logs can be forwarded to your monitoring system\n- Health status can be exposed via HTTP endpoints\n- Alerts can be triggered on node failures","title":"7. Monitoring and Observability - Failure Handling","ref":"failure-handling.html#7-monitoring-and-observability"},{"type":"extras","doc":"The solution is designed to be extensible:\n\n**Planned Enhancements**:\n- **Forced Khepri Node Removal**: Actively remove failed nodes from consensus\n- **Worker Redistribution**: Trigger immediate rebalancing of Swarm workers\n- **Leader Election**: Handle leader failures more aggressively\n- **Custom Health Checks**: Application-specific health validation","title":"8. Advanced Features (Future Extensions) - Failure Handling","ref":"failure-handling.html#8-advanced-features-future-extensions"},{"type":"extras","doc":"**No Breaking Changes**: \n- Fully backward compatible with existing cluster\n- Can be deployed incrementally (node by node)\n- Falls back gracefully if monitoring fails\n\n**Resource Usage**:\n- Minimal CPU overhead (RPC calls every 2 seconds)\n- Low memory footprint (tracks failure state only)\n- Network traffic: ~1KB per node per probe","title":"9. Deployment - Failure Handling","ref":"failure-handling.html#9-deployment"},{"type":"extras","doc":"**Chaos Engineering**:\n```bash\n# Simulate hard crash\ndocker kill ex-esdb-node1\n\n# Monitor logs for detection\ndocker logs ex-esdb-node2 | grep NodeMonitor\n\n# Verify cleanup\n# Should see Swarm registrations removed within 6 seconds\n```\n\n**Expected Timeline**:\n- T+0: Node crashes\n- T+2s: First probe failure detected\n- T+4s: Second probe failure  \n- T+6s: Third probe failure, node marked as failed\n- T+6s: Cleanup initiated (Swarm registrations removed)\n- T+8s: Next probe cycle (failed node no longer monitored)\n\n#### 2. LibCluster Integration\n\n**Built-in Node Detection**:\n- Uses `:net_kernel.monitor_nodes(true, [:nodedown_reason])` for immediate notification\n- Gossip-based discovery helps detect network issues\n- Automatic cluster formation and healing\n\n#### 3. ClusterCoordinator\n\n**Split-Brain Prevention**:\n- Deterministic leader election (lowest node name)\n- Prevents multiple clusters from forming\n- Coordinates safe cluster joining\n\n**Features**:\n```elixir\n# Prevent split-brain during network partitions\ndef should_be_cluster_coordinator(connected_nodes) do\n  all_nodes = [node() | connected_nodes] |> Enum.sort()\n  node() == List.first(all_nodes)\nend\n```","title":"10. Testing the Solution - Failure Handling","ref":"failure-handling.html#10-testing-the-solution"},{"type":"extras","doc":"ExESDB handles graceful shutdowns through multiple mechanisms:\n\n**Signal Handling**:\n```elixir\n# SIGTERM and SIGQUIT handling\n:os.set_signal(:sigterm, :handle)\n:os.set_signal(:sigquit, :handle)\n```\n\n**Process Cleanup**:\n- Workers unregister from Swarm before termination\n- Subscription state is persisted\n- Transactions are completed or rolled back","title":"Graceful Shutdown Handling - Failure Handling","ref":"failure-handling.html#graceful-shutdown-handling"},{"type":"extras","doc":"","title":"Network Partitions - Failure Handling","ref":"failure-handling.html#network-partitions"},{"type":"extras","doc":"ExESDB implements multiple layers to prevent split-brain scenarios:\n\n#### 1. ClusterCoordinator Logic\n- **Deterministic Election**: Uses sorted node names for consistent leader selection\n- **Existing Cluster Detection**: Searches for active clusters before forming new ones\n- **Coordinated Joining**: Prevents multiple simultaneous cluster formations\n\n#### 2. Raft Consensus (Ra/Khepri)\n- **Majority Quorum**: Requires majority of nodes for write operations\n- **Leader Election**: Automatic failover when leader becomes unavailable\n- **Log Replication**: Ensures consistency across cluster members\n\n#### 3. Partition Tolerance\n**Minority Partition Behavior**:\n- Nodes in minority partition become read-only\n- No new events can be written without quorum\n- Automatic healing when partition resolves\n\n**Majority Partition Behavior**:\n- Continues normal operations\n- Elects new leader if needed\n- Accepts new writes and maintains consistency","title":"Split-Brain Prevention - Failure Handling","ref":"failure-handling.html#split-brain-prevention"},{"type":"extras","doc":"**Automatic Healing Process**:\n1. **Detection**: Nodes detect network connectivity restoration\n2. **State Synchronization**: Minority nodes sync with majority\n3. **Conflict Resolution**: Raft log reconciliation\n4. **Service Restoration**: Workers redistribute across all nodes","title":"Network Partition Recovery - Failure Handling","ref":"failure-handling.html#network-partition-recovery"},{"type":"extras","doc":"","title":"Data Consistency - Failure Handling","ref":"failure-handling.html#data-consistency"},{"type":"extras","doc":"ExESDB provides ACID guarantees through Khepri transactions:\n\n**Optimistic Concurrency Control**:\n```elixir\ndef try_append_events(store, stream_id, expected_version, events) do\n  current_version = get_current_version(store, stream_id)\n  \n  if current_version == expected_version do\n    # Proceed with append\n    append_events_atomically(store, stream_id, events, current_version)\n  else\n    {:error, :wrong_expected_version}\n  end\nend\n```\n\n**Transaction Isolation**:\n- Uses Khepri's MVCC for concurrent access\n- Transactions are atomic and isolated\n- Automatic rollback on failures","title":"Transaction Handling - Failure Handling","ref":"failure-handling.html#transaction-handling"},{"type":"extras","doc":"**Version Conflicts**:\n- Expected version mismatches return `:wrong_expected_version`\n- Clients must retry with updated expected version\n- No automatic conflict resolution (explicit client handling)\n\n**Concurrent Writes**:\n- Only one writer per stream at a time\n- Serialized access through stream-specific workers\n- Queue management for high-throughput scenarios","title":"Conflict Resolution - Failure Handling","ref":"failure-handling.html#conflict-resolution"},{"type":"extras","doc":"","title":"Worker Process Failures - Failure Handling","ref":"failure-handling.html#worker-process-failures"},{"type":"extras","doc":"#### Writers (StreamsWriterWorker)\n**Lifecycle Management**:\n- `:temporary` restart strategy prevents restart loops\n- TTL-based shutdown for resource management\n- Automatic Swarm cleanup on termination\n\n**Failure Scenarios**:\n```elixir\n# TTL-based shutdown\ndef handle_info(:check_idle, %{idle_since: idle_since} = state) do\n  writer_ttl = Options.writer_idle_ms()\n  \n  if idle_since + writer_ttl < epoch_time_ms() do\n    Process.exit(self(), :ttl_reached)\n  end\n  \n  {:noreply, state}\nend\n\n# Graceful cleanup on exit\ndef handle_info({:EXIT, _pid, reason}, %{worker_name: name} = state) do\n  Swarm.unregister_name(name)\n  {:noreply, state}\nend\n```\n\n#### Readers (StreamsReaderWorker)\n- Similar TTL-based lifecycle\n- On-demand creation for read operations\n- Automatic cleanup when no longer needed","title":"Stream Worker Failure Handling - Failure Handling","ref":"failure-handling.html#stream-worker-failure-handling"},{"type":"extras","doc":"**High Availability Design**:\n- `:permanent` restart strategy for critical gateway functions\n- Load balancing through random worker selection\n- Graceful failover to other gateway workers\n\n**Worker Distribution**:\n```elixir\n# Random load balancing with fallback\ndefp random_gateway_worker do\n  case Swarm.members(:gateway_workers) do\n    [] -> \n      # Fallback to local gateway if no distributed workers\n      ExESDB.GatewayWorker\n    workers -> \n      workers |> Enum.random() |> elem(1)\n  end\nend\n```","title":"Gateway Worker Failures - Failure Handling","ref":"failure-handling.html#gateway-worker-failures"},{"type":"extras","doc":"**\"Follow-the-Leader\" Pattern**:\n- Subscription workers automatically migrate to leader node\n- Persistent subscription state survives worker failures\n- Automatic restart and state recovery","title":"Subscription Worker Failures - Failure Handling","ref":"failure-handling.html#subscription-worker-failures"},{"type":"extras","doc":"","title":"Storage Failures - Failure Handling","ref":"failure-handling.html#storage-failures"},{"type":"extras","doc":"**Data Directory Management**:\n```elixir\n# Configurable data directory\nconfig :ex_esdb, :khepri,\n  data_dir: \"/data\",\n  store_id: :reg_gh,\n  timeout: 2_000\n```\n\n**Failure Scenarios**:\n\n#### Disk Space Exhaustion\n- **Detection**: Monitor disk usage in production\n- **Mitigation**: Implement log compaction and cleanup\n- **Recovery**: Restore from snapshots if available\n\n#### Corruption Detection\n- **Checksums**: Ra maintains data integrity checks\n- **Verification**: Periodic consistency checks\n- **Recovery**: Restore from cluster peers or backups\n\n#### Performance Degradation\n- **Monitoring**: Track operation latencies\n- **Alerting**: Set thresholds for response times\n- **Mitigation**: Scale storage or redistribute load","title":"Khepri/Ra Storage Resilience - Failure Handling","ref":"failure-handling.html#khepri-ra-storage-resilience"},{"type":"extras","doc":"**Snapshot Management**:\n- Regular snapshots of aggregate state\n- Version-based snapshot storage\n- Distributed snapshot replication\n\n**Disaster Recovery**:\n1. **Data Loss Prevention**: Multi-node replication\n2. **Point-in-Time Recovery**: Event replay from snapshots\n3. **Cross-Region Backup**: External backup strategies","title":"Backup and Recovery - Failure Handling","ref":"failure-handling.html#backup-and-recovery"},{"type":"extras","doc":"","title":"Configuration and Monitoring - Failure Handling","ref":"failure-handling.html#configuration-and-monitoring"},{"type":"extras","doc":"**Cluster Health**:\n```elixir\n# Check overall cluster status\nExESDB.NodeMonitor.health_status()\n\n# Check specific node\nExESDB.NodeMonitor.probe_node(:node1@host)\n\n# Get cluster members\nExESDB.Cluster.members(store_id)\n```\n\n**Performance Metrics**:\n- Operation latencies\n- Throughput measurements\n- Resource utilization\n- Error rates","title":"Health Check Endpoints - Failure Handling","ref":"failure-handling.html#health-check-endpoints"},{"type":"extras","doc":"**Structured Logging**:\n```elixir\nconfig :logger, :console,\n  format: \"$time $metadata[$level] $message\\n\",\n  metadata: [:mfa]\n```\n\n**Alert Categories**:\n- **Critical**: Node failures, data corruption\n- **Warning**: Performance degradation, high error rates\n- **Info**: Normal operations, state changes","title":"Logging and Alerting - Failure Handling","ref":"failure-handling.html#logging-and-alerting"},{"type":"extras","doc":"**Production Settings**:\n```elixir\n# Timeouts\nconfig :ex_esdb, :khepri,\n  timeout: 5_000  # Increase for production\n\n# Node monitoring\nconfig :ex_esdb, :node_monitor,\n  probe_interval: 2_000,\n  failure_threshold: 3,\n  probe_timeout: 1_000\n\n# Worker TTL\nconfig :ex_esdb, :worker_idle_ms, 300_000  # 5 minutes\n```\n\n**Development Settings**:\n```elixir\n# Faster timeouts for development\nconfig :ex_esdb, :khepri,\n  timeout: 10_000\n\n# Shorter TTL for resource management\nconfig :ex_esdb, :worker_idle_ms, 60_000  # 1 minute\n```","title":"Configuration Best Practices - Failure Handling","ref":"failure-handling.html#configuration-best-practices"},{"type":"extras","doc":"","title":"Recovery Procedures - Failure Handling","ref":"failure-handling.html#recovery-procedures"},{"type":"extras","doc":"#### Single Node Recovery\n1. **Identify Issue**: Check logs and monitoring\n2. **Isolate Node**: Remove from load balancer if needed\n3. **Restart Service**: Use graceful restart procedures\n4. **Verify Health**: Confirm cluster membership\n5. **Restore Traffic**: Gradually return to service\n\n#### Cluster Recovery\n1. **Assess Damage**: Determine scope of failure\n2. **Quorum Check**: Ensure majority of nodes available\n3. **Leader Election**: Verify or trigger new leader election\n4. **Data Integrity**: Check for any corruption\n5. **Service Validation**: Test critical operations","title":"Manual Recovery Steps - Failure Handling","ref":"failure-handling.html#manual-recovery-steps"},{"type":"extras","doc":"**Self-Healing Mechanisms**:\n- Automatic process restarts via supervision\n- Worker redistribution through Swarm\n- Cluster reformation after partitions\n- Leader election on failures\n\n**Monitoring Integration**:\n- Health check failures trigger alerts\n- Automatic scaling based on load\n- Proactive maintenance scheduling","title":"Automated Recovery - Failure Handling","ref":"failure-handling.html#automated-recovery"},{"type":"extras","doc":"","title":"Testing Failure Scenarios - Failure Handling","ref":"failure-handling.html#testing-failure-scenarios"},{"type":"extras","doc":"**Node Failures**:\n```bash\n# Hard crash simulation\ndocker kill ex-esdb-node1\n\n# Network partition simulation\niptables -A INPUT -s   -j DROP\niptables -A OUTPUT -d   -j DROP\n\n# Resource exhaustion\nstress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 60s\n```\n\n**Service Failures**:\n```bash\n# Stop specific services\nsystemctl stop ex_esdb\n\n# Simulate disk failures\nfill-disk.sh /data 95%\n\n# Network latency injection\ntc qdisc add dev eth0 root netem delay 500ms\n```","title":"Chaos Engineering - Failure Handling","ref":"failure-handling.html#chaos-engineering"},{"type":"extras","doc":"#### Scenario 1: Single Node Failure\n**Steps**:\n1. Start 3-node cluster\n2. Kill one node abruptly\n3. Verify cluster continues operating\n4. Check client request success rates\n5. Restart failed node\n6. Verify automatic rejoin\n\n**Expected Results**:\n- Cluster maintains quorum (2/3 nodes)\n- No data loss\n- Client operations continue\n- Failed node rejoins automatically\n\n#### Scenario 2: Network Partition\n**Steps**:\n1. Start 3-node cluster\n2. Create network partition (2 vs 1 node)\n3. Verify majority partition continues\n4. Check minority partition becomes read-only\n5. Heal partition\n6. Verify automatic reconciliation\n\n**Expected Results**:\n- Majority partition elects new leader\n- Minority partition rejects writes\n- Healing triggers state synchronization\n- No data inconsistencies\n\n#### Scenario 3: Leader Failure\n**Steps**:\n1. Identify current leader\n2. Kill leader node\n3. Verify new leader election\n4. Check subscription migration\n5. Validate continued operations\n\n**Expected Results**:\n- New leader elected within timeout\n- Subscriptions migrate to new leader\n- Client operations resume\n- Worker redistribution occurs","title":"Test Scenarios - Failure Handling","ref":"failure-handling.html#test-scenarios"},{"type":"extras","doc":"**Key Metrics**:\n- Response times\n- Error rates\n- Memory usage\n- CPU utilization\n- Network connectivity\n- Disk I/O\n\n**Log Analysis**:\n```bash\n# Monitor cluster events\ndocker logs -f ex-esdb-node1 | grep -E \"(NodeMonitor|Cluster|Leader)\"\n\n# Check for errors\ndocker logs ex-esdb-node1 | grep -i error\n\n# Monitor worker redistribution\ndocker logs ex-esdb-node1 | grep -i swarm\n```","title":"Monitoring During Tests - Failure Handling","ref":"failure-handling.html#monitoring-during-tests"},{"type":"extras","doc":"ExESDB provides comprehensive failure handling across all system levels, from individual process failures to cluster-wide outages. The system is designed with the BEAM's \"let it crash\" philosophy while ensuring data consistency and high availability through:\n\n**Key Strengths**:\n- **Fast Failure Detection**: 6-second node failure detection vs 10-30 second consensus timeouts\n- **Automatic Recovery**: Self-healing mechanisms at every level\n- **Data Consistency**: ACID guarantees through Raft consensus\n- **High Availability**: No single points of failure\n- **Graceful Degradation**: System continues operating with reduced capacity\n\n**Operational Benefits**:\n- **Reduced Downtime**: Automatic failover and recovery\n- **Operational Simplicity**: Minimal manual intervention required\n- **Predictable Behavior**: Well-defined failure modes and recovery procedures\n- **Monitoring Integration**: Comprehensive observability and alerting\n\n**Production Readiness**:\n- Battle-tested BEAM supervision principles\n- Proven Raft consensus implementation (Ra)\n- Comprehensive testing scenarios\n- Clear operational procedures\n\nThis failure handling strategy ensures that ExESDB can operate reliably in production environments while maintaining the flexibility and resilience that makes BEAM-based systems ideal for distributed, fault-tolerant applications.","title":"Conclusion - Failure Handling","ref":"failure-handling.html#conclusion"},{"type":"extras","doc":"# Logger Filtering in ExESDB Event Store\n\nExESDB is built on top of several distributed systems components that can generate significant amounts of log noise during normal operation. To provide a better developer experience and cleaner production logs, ExESDB implements a comprehensive logger filtering system.","title":"Logger Filtering","ref":"logger-filtering.html"},{"type":"extras","doc":"ExESDB uses a two-tier approach to logger filtering:\n\n1. **BCUtils.LoggerFilters** - Provides general-purpose filtering for common BEAM distributed systems\n2. **ExESDB.LoggerFilters** - Provides specialized filtering for ExESDB's core infrastructure components\n\nThis layered approach ensures comprehensive noise reduction while maintaining full visibility into errors and warnings.","title":"Overview - Logger Filtering","ref":"logger-filtering.html#overview"},{"type":"extras","doc":"BCUtils provides foundational logger filtering for commonly used distributed systems libraries:","title":"BCUtils Logger Filtering - Logger Filtering","ref":"logger-filtering.html#bcutils-logger-filtering"},{"type":"extras","doc":"- Filters routine process registration/unregistration messages\n- Reduces noise from node up/down events\n- Maintains visibility into registry errors and conflicts","title":"Swarm Process Registry - Logger Filtering","ref":"logger-filtering.html#swarm-process-registry"},{"type":"extras","doc":"- Filters cluster formation and gossip protocol messages\n- Reduces connection/disconnection event noise\n- Preserves cluster formation failures and split-brain warnings","title":"LibCluster Auto-Clustering - Logger Filtering","ref":"logger-filtering.html#libcluster-auto-clustering"},{"type":"extras","doc":"ExESDB extends the filtering capabilities with specialized filters for its core infrastructure:","title":"ExESDB Logger Filtering - Logger Filtering","ref":"logger-filtering.html#exesdb-logger-filtering"},{"type":"extras","doc":"The Ra consensus library is fundamental to ExESDB's distributed operation but generates substantial log noise during normal consensus operations.\n\n**Filtered Messages:**\n- Heartbeat messages between Ra nodes\n- `append_entries` consensus protocol messages\n- `pre_vote` and `request_vote` election messages\n- Routine state transitions (follower  candidate  leader)\n- Internal Ra module operations at info/debug levels\n\n**Preserved Messages:**\n- All error and warning level messages\n- Election failures and split-brain scenarios\n- Consensus failures and recovery operations","title":"Ra Consensus Library Filtering - Logger Filtering","ref":"logger-filtering.html#ra-consensus-library-filtering"},{"type":"extras","doc":"Khepri serves as ExESDB's distributed database backend and can be verbose about internal operations.\n\n**Filtered Messages:**\n- Cluster state synchronization messages\n- Store operation confirmations\n- Routine cluster member coordination\n- Internal Khepri module operations at info/debug levels\n\n**Preserved Messages:**\n- Database errors and transaction failures\n- Cluster coordination problems\n- Data consistency warnings","title":"Khepri Database Filtering - Logger Filtering","ref":"logger-filtering.html#khepri-database-filtering"},{"type":"extras","doc":"ExESDB provides additional filtering beyond BCUtils for ExESDB-specific use cases:\n\n**Enhanced Swarm Filtering:**\n- ExESDB-specific process registry patterns\n- Stream coordinator registration/unregistration\n- Event emitter lifecycle messages\n\n**Enhanced LibCluster Filtering:**\n- ExESDB cluster topology changes\n- Node role transitions (leader/follower)\n- Gateway API cluster coordination","title":"Enhanced Swarm & LibCluster Filtering - Logger Filtering","ref":"logger-filtering.html#enhanced-swarm-libcluster-filtering"},{"type":"extras","doc":"","title":"Configuration - Logger Filtering","ref":"logger-filtering.html#configuration"},{"type":"extras","doc":"In development, you may want to see more detailed logs. Configure your logger in `config/dev.exs`:\n\n```elixir\n# Minimal filtering - see more activity\nconfig :logger, :console,\n  level: :debug,\n  format: \"[$level] $message\\n\"\n\n# Apply only critical noise reduction\nconfig :logger, \n  backends: [:console],\n  compile_time_purge_matching: [\n    [level_lower_than: :info]\n  ]\n```","title":"Development Environment - Logger Filtering","ref":"logger-filtering.html#development-environment"},{"type":"extras","doc":"For production, apply comprehensive filtering in `config/prod.exs`:\n\n```elixir\n# Apply all ExESDB logger filters\nconfig :logger,\n  backends: [:console],\n  compile_time_purge_matching: [\n    [level_lower_than: :info]\n  ]\n\n# Add custom filters\nconfig :logger, :console,\n  level: :info,\n  format: \"[$level] $message\\n\",\n  metadata_filter: [\n    {ExESDB.LoggerFilters, :filter_ra},\n    {ExESDB.LoggerFilters, :filter_khepri},\n    {ExESDB.LoggerFilters, :filter_swarm},\n    {ExESDB.LoggerFilters, :filter_libcluster}\n  ]\n```","title":"Production Environment - Logger Filtering","ref":"logger-filtering.html#production-environment"},{"type":"extras","doc":"You can extend the filtering system for your specific needs:\n\n```elixir\ndefmodule MyApp.CustomLoggerFilters do\n  def filter_my_component(log_event) do\n    case log_event do\n      {level, _gl, {Logger, msg, _ts, metadata}} ->\n        if should_filter_my_component?(level, msg, metadata) do\n          :stop\n        else\n          :ignore\n        end\n      _ ->\n        :ignore\n    end\n  end\n\n  defp should_filter_my_component?(level, msg, metadata) do\n    # Your custom filtering logic\n    level in [:info, :debug] and routine_operation?(msg)\n  end\nend\n```","title":"Custom Filtering - Logger Filtering","ref":"logger-filtering.html#custom-filtering"},{"type":"extras","doc":"","title":"Best Practices - Logger Filtering","ref":"logger-filtering.html#best-practices"},{"type":"extras","doc":"Always ensure that error and warning messages are never filtered. The ExESDB filters follow this principle religiously.","title":"1. Preserve Error Visibility - Logger Filtering","ref":"logger-filtering.html#1-preserve-error-visibility"},{"type":"extras","doc":"Use both message content and logger metadata to make intelligent filtering decisions:\n\n```elixir\ndefp should_filter?(level, msg, metadata) do\n  cond do\n    level in [:error, :warning] -> false\n    routine_message?(msg) and routine_module?(metadata) -> true\n    true -> false\n  end\nend\n```","title":"2. Filter by Message Content and Metadata - Logger Filtering","ref":"logger-filtering.html#2-filter-by-message-content-and-metadata"},{"type":"extras","doc":"Apply different filtering levels based on your environment:\n- **Development**: Minimal filtering for debugging\n- **Testing**: Aggressive filtering for clean test output\n- **Production**: Balanced filtering for operational visibility","title":"3. Environment-Specific Configuration - Logger Filtering","ref":"logger-filtering.html#3-environment-specific-configuration"},{"type":"extras","doc":"Regularly review your logs to ensure:\n- Important messages aren't being filtered\n- Noise levels remain manageable\n- New components don't introduce new noise patterns","title":"4. Monitor Filter Effectiveness - Logger Filtering","ref":"logger-filtering.html#4-monitor-filter-effectiveness"},{"type":"extras","doc":"Logger filters are applied during log message processing and should be efficient:\n\n- Use pattern matching for quick message classification\n- Cache expensive computations where possible\n- Prefer string contains checks over regex for performance\n- Exit early from filter functions when possible","title":"Filter Performance - Logger Filtering","ref":"logger-filtering.html#filter-performance"},{"type":"extras","doc":"","title":"Troubleshooting - Logger Filtering","ref":"logger-filtering.html#troubleshooting"},{"type":"extras","doc":"If you're still seeing too much noise:\n1. Check that filters are properly configured\n2. Identify new noise sources and extend filters\n3. Consider adjusting log levels for specific modules","title":"Too Much Noise - Logger Filtering","ref":"logger-filtering.html#too-much-noise"},{"type":"extras","doc":"If important messages are being filtered:\n1. Review filter logic for overly broad patterns\n2. Add explicit exceptions for critical message types\n3. Test filters in development before production deployment","title":"Missing Important Messages - Logger Filtering","ref":"logger-filtering.html#missing-important-messages"},{"type":"extras","doc":"If logging performance is impacted:\n1. Profile filter functions for bottlenecks\n2. Optimize message matching patterns\n3. Consider compile-time filtering for high-volume noise","title":"Performance Issues - Logger Filtering","ref":"logger-filtering.html#performance-issues"},{"type":"extras","doc":"ExESDB's logger filtering integrates well with Phoenix telemetry and observability tools:\n\n```elixir\n# Telemetry events are not affected by logger filtering\n:telemetry.execute([:exesdb, :stream, :read], %{duration: duration}, %{\n  stream_id: stream_id,\n  event_count: length(events)\n})\n```\n\nThis ensures that your observability and monitoring systems continue to receive all necessary operational data while keeping logs clean and readable.","title":"Integration with Telemetry - Logger Filtering","ref":"logger-filtering.html#integration-with-telemetry"},{"type":"extras","doc":"ExESDB's comprehensive logger filtering system provides a clean, production-ready logging experience while maintaining full visibility into system health and errors. By layering BCUtils general-purpose filtering with ExESDB-specific filters, developers get the best of both worlds: quiet logs during normal operation and detailed diagnostics when things go wrong.\n\nThe filtering system is designed to be:\n- **Intelligent**: Preserves all errors and warnings\n- **Comprehensive**: Covers all major noise sources\n- **Configurable**: Adaptable to different environments and needs\n- **Extensible**: Easy to add custom filters for specific requirements\n- **Performant**: Minimal impact on logging performance","title":"Conclusion - Logger Filtering","ref":"logger-filtering.html#conclusion"},{"type":"extras","doc":"# How ExESDB Handles Multiple Stores\n\nThis document describes the dynamic store creation and management capabilities added to ExESDB, allowing users to create multiple event stores on-demand within a single cluster.","title":"Multiple Stores","ref":"multiple-stores.html"},{"type":"extras","doc":"Previously, ExESDB was configured with a single store ID at startup. Now, with the `ExESDB.StoreManager`, users can:\n\n- Create new stores dynamically at runtime\n- Remove stores when no longer needed\n- List and query store status and configuration\n- Use multiple stores simultaneously in the same cluster","title":"Overview - Multiple Stores","ref":"multiple-stores.html#overview"},{"type":"extras","doc":"","title":"Architecture - Multiple Stores","ref":"multiple-stores.html#architecture"},{"type":"extras","doc":"1. **ExESDB.StoreManager**: The core GenServer that manages multiple Khepri stores\n2. **ExESDBGater.API**: Updated API with store management functions\n3. **ExESDB.GatewayWorker**: Updated to handle store management operations","title":"Components - Multiple Stores","ref":"multiple-stores.html#components"},{"type":"extras","doc":"1. The `StoreManager` replaces the single `Store` process in the supervision tree\n2. Each store gets its own unique data directory under the base data directory\n3. Stores are managed independently but share the same cluster infrastructure\n4. All existing stream, subscription, and snapshot operations work with any managed store","title":"How It Works - Multiple Stores","ref":"multiple-stores.html#how-it-works"},{"type":"extras","doc":"","title":"API Reference - Multiple Stores","ref":"multiple-stores.html#api-reference"},{"type":"extras","doc":"```elixir\n# Fire-and-forget operation\n:ok = ExESDBGater.API.create_store(:my_new_store, [timeout: 15_000])\n```","title":"Creating a Store - Multiple Stores","ref":"multiple-stores.html#creating-a-store"},{"type":"extras","doc":"```elixir\n# Fire-and-forget operation\n:ok = ExESDBGater.API.remove_store(:my_store)\n```","title":"Removing a Store - Multiple Stores","ref":"multiple-stores.html#removing-a-store"},{"type":"extras","doc":"```elixir\n{:ok, stores} = ExESDBGater.API.list_stores()\n# Returns: %{store_id => %{status: :running, config: [...]}}\n```","title":"Listing Stores - Multiple Stores","ref":"multiple-stores.html#listing-stores"},{"type":"extras","doc":"```elixir\n{:ok, :running} = ExESDBGater.API.get_store_status(:my_store)\n```","title":"Getting Store Status - Multiple Stores","ref":"multiple-stores.html#getting-store-status"},{"type":"extras","doc":"```elixir\n{:ok, config} = ExESDBGater.API.get_store_config(:my_store)\n```","title":"Getting Store Configuration - Multiple Stores","ref":"multiple-stores.html#getting-store-configuration"},{"type":"extras","doc":"Once a store is created, you can use it with all existing operations:\n\n```elixir\n# Append events to a specific store\n{:ok, version} = ExESDBGater.API.append_events(:my_store, \"stream-1\", events)\n\n# Read events from a specific store\n{:ok, events} = ExESDBGater.API.get_events(:my_store, \"stream-1\", 0, 10)\n\n# List streams in a specific store\n{:ok, streams} = ExESDBGater.API.get_streams(:my_store)\n\n# Create subscriptions for a specific store\n:ok = ExESDBGater.API.save_subscription(:my_store, :by_stream, \"$all\", \"my_sub\")\n```","title":"Store Operations - Multiple Stores","ref":"multiple-stores.html#store-operations"},{"type":"extras","doc":"","title":"Configuration - Multiple Stores","ref":"multiple-stores.html#configuration"},{"type":"extras","doc":"The system still creates a default store on startup using the existing configuration:\n\n```elixir\n# In runtime.exs\nconfig :ex_esdb, :khepri,\n  data_dir: data_dir(),\n  store_id: store_id(),  # This becomes the default store\n  timeout: timeout(),\n  db_type: db_type(),\n  pub_sub: pub_sub()\n```","title":"Default Store - Multiple Stores","ref":"multiple-stores.html#default-store"},{"type":"extras","doc":"New stores inherit the default configuration but can override specific settings:\n\n```elixir\nExESDBGater.API.create_store(:custom_store, [\n  timeout: 20_000,        # Custom timeout\n  # data_dir is automatically set to base_dir/custom_store\n])\n```","title":"Dynamic Store Configuration - Multiple Stores","ref":"multiple-stores.html#dynamic-store-configuration"},{"type":"extras","doc":"Each store gets its own data directory:\n\n```\n/data/\n ex_esdb_store/          # Default store\n user_data_store/        # Custom store 1\n analytics_store/        # Custom store 2\n audit_logs_store/       # Custom store 3\n```","title":"Data Storage - Multiple Stores","ref":"multiple-stores.html#data-storage"},{"type":"extras","doc":"","title":"Use Cases - Multiple Stores","ref":"multiple-stores.html#use-cases"},{"type":"extras","doc":"Create separate stores for each tenant:\n\n```elixir\n# Create tenant-specific stores\nExESDBGater.API.create_store(:tenant_123_store)\nExESDBGater.API.create_store(:tenant_456_store)\n\n# Use tenant-specific store for operations\nExESDBGater.API.append_events(:tenant_123_store, \"orders\", events)\n```","title":"Multi-Tenant Applications - Multiple Stores","ref":"multiple-stores.html#multi-tenant-applications"},{"type":"extras","doc":"Create stores for different business domains:\n\n```elixir\n# Separate stores by domain\nExESDBGater.API.create_store(:user_management_store)\nExESDBGater.API.create_store(:order_processing_store)\nExESDBGater.API.create_store(:analytics_store)\n```","title":"Domain Separation - Multiple Stores","ref":"multiple-stores.html#domain-separation"},{"type":"extras","doc":"Create stores for different purposes:\n\n```elixir\n# Development/testing stores\nExESDBGater.API.create_store(:test_store)\nExESDBGater.API.create_store(:staging_store)\n```","title":"Environment-Specific Stores - Multiple Stores","ref":"multiple-stores.html#environment-specific-stores"},{"type":"extras","doc":"- Stores are created on the node that receives the request\n- Khepri handles replication across the cluster automatically\n- Each store maintains its own Raft consensus group\n- Store operations are distributed across cluster nodes via Swarm","title":"Cluster Behavior - Multiple Stores","ref":"multiple-stores.html#cluster-behavior"},{"type":"extras","doc":"The changes are fully backward compatible:\n\n- Existing single-store configurations continue to work\n- All existing APIs work with the default store\n- No migration is required for existing deployments","title":"Backward Compatibility - Multiple Stores","ref":"multiple-stores.html#backward-compatibility"},{"type":"extras","doc":"","title":"Best Practices - Multiple Stores","ref":"multiple-stores.html#best-practices"},{"type":"extras","doc":"Use descriptive, unique atom names:\n\n```elixir\n# Good\n:user_events_store\n:order_processing_store\n:analytics_events_store\n\n# Avoid\n:store1\n:store\n:temp\n```","title":"Store Naming - Multiple Stores","ref":"multiple-stores.html#store-naming"},{"type":"extras","doc":"- Monitor store count to avoid resource exhaustion\n- Remove unused stores to free up resources\n- Consider store lifecycle in your application design","title":"Resource Management - Multiple Stores","ref":"multiple-stores.html#resource-management"},{"type":"extras","doc":"- Use consistent timeout values for related stores\n- Plan data directory structure for backup/restore operations\n- Consider store-specific configuration needs","title":"Configuration - Multiple Stores","ref":"multiple-stores.html#configuration-1"},{"type":"extras","doc":"To monitor store health:\n\n```elixir\n# Get all stores and their status\n{:ok, stores} = ExESDBGater.API.list_stores()\n\nfor {store_id, info} <- stores do\n  IO.puts(\"Store #{store_id}: #{info.status}\")\nend\n```","title":"Monitoring - Multiple Stores","ref":"multiple-stores.html#monitoring"},{"type":"extras","doc":"Common error scenarios:\n\n```elixir\n# Store already exists\n{:error, :already_exists} = ExESDBGater.API.create_store(:existing_store)\n\n# Store not found\n{:error, :not_found} = ExESDBGater.API.get_store_status(:nonexistent_store)\n{:error, :not_found} = ExESDBGater.API.remove_store(:nonexistent_store)\n```","title":"Error Handling - Multiple Stores","ref":"multiple-stores.html#error-handling"},{"type":"extras","doc":"If you're currently using a single store and want to adopt multiple stores:\n\n1. **No immediate action required** - your existing setup continues to work\n2. **Gradual migration** - start creating new stores for new features\n3. **Optional consolidation** - consider reorganizing existing data into domain-specific stores","title":"Migration Guide - Multiple Stores","ref":"multiple-stores.html#migration-guide"},{"type":"extras","doc":"- Each store has its own Khepri cluster member\n- Memory usage scales with the number of stores\n- Network traffic increases with store count due to more Raft groups\n- Consider store count limits based on cluster capacity","title":"Performance Considerations - Multiple Stores","ref":"multiple-stores.html#performance-considerations"},{"type":"extras","doc":"- Store creation/removal should be restricted to authorized operations\n- Consider implementing store-level access controls in your application\n- Monitor store creation for unauthorized usage","title":"Security Considerations - Multiple Stores","ref":"multiple-stores.html#security-considerations"},{"type":"extras","doc":"- Store IDs must be valid Elixir atoms\n- Each store requires cluster resources (memory, network)\n- Maximum practical store count depends on cluster capacity\n- Store removal is immediate and irreversible","title":"Limitations - Multiple Stores","ref":"multiple-stores.html#limitations"},{"type":"extras","doc":"Potential future improvements:\n\n- Store templates for consistent configuration\n- Store migration utilities\n- Store-level metrics and monitoring\n- Automatic store cleanup policies\n- Store backup/restore functionality","title":"Future Enhancements - Multiple Stores","ref":"multiple-stores.html#future-enhancements"},{"type":"extras","doc":"# Configuring ExESDB Applications\n\nThis guide explains how to configure ExESDB in both standalone and umbrella applications.","title":"Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html"},{"type":"extras","doc":"ExESDB supports two deployment patterns:\n\n1. **Standalone Applications**: A single OTP application that uses ExESDB\n2. **Umbrella Applications**: Multiple child applications within an umbrella project, each with their own ExESDB configuration\n\nThe configuration format is always: `config :your_app_name, :ex_esdb, [options]` where:\n- `:your_app_name` is the name of your OTP application\n- `:ex_esdb` is the configuration namespace that ExESDB looks for\n- `[options]` are the ExESDB configuration options","title":"Configuration Overview - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#configuration-overview"},{"type":"extras","doc":"For a standalone application, configure ExESDB under your application's name:\n\n```elixir\n# config/config.exs\n# If your app is named :my_event_store\nconfig :my_event_store, :ex_esdb,\n  data_dir: \"/var/lib/ex_esdb\",\n  store_id: :my_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :my_pubsub,\n  reader_idle_ms: 15000,\n  writer_idle_ms: 12000,\n  store_description: \"My Event Store\",\n  store_tags: [\"production\", \"events\"]\n\n# Libcluster configuration (recommended over seed_nodes)\nconfig :libcluster,\n  topologies: [\n    example: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"my_secret\"\n      ]\n    ]\n  ]\n```","title":"Standalone Application Configuration - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#standalone-application-configuration"},{"type":"extras","doc":"```elixir\n# Uses the default context (your app name)\nExESDB.Options.data_dir()          # \"/var/lib/ex_esdb\"\nExESDB.Options.store_id()          # :my_store\nExESDB.Options.db_type()           # :cluster\n```","title":"Usage in Standalone Apps - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#usage-in-standalone-apps"},{"type":"extras","doc":"For umbrella applications, each child application can have its own ExESDB configuration:\n\n```elixir\n# config/config.exs (umbrella root)\n\n# Child app 1: orders service\nconfig :orders_service, :ex_esdb,\n  data_dir: \"/var/lib/orders_events\",\n  store_id: :orders_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :orders_pubsub,\n  reader_idle_ms: 10000,\n  writer_idle_ms: 8000,\n  store_description: \"Orders Event Store\",\n  store_tags: [\"orders\", \"production\"]\n\n# Child app 2: inventory service\nconfig :inventory_service, :ex_esdb,\n  data_dir: \"/var/lib/inventory_events\",\n  store_id: :inventory_store,\n  timeout: 3000,\n  db_type: :single,\n  pub_sub: :inventory_pubsub,\n  reader_idle_ms: 12000,\n  writer_idle_ms: 10000,\n  store_description: \"Inventory Event Store\",\n  store_tags: [\"inventory\", \"production\"]\n\n# Child app 3: user service\nconfig :user_service, :ex_esdb,\n  data_dir: \"/var/lib/user_events\",\n  store_id: :user_store,\n  timeout: 4000,\n  db_type: :cluster,\n  pub_sub: :user_pubsub,\n  reader_idle_ms: 8000,\n  writer_idle_ms: 6000,\n  store_description: \"User Event Store\",\n  store_tags: [\"users\", \"auth\", \"production\"]\n\n# Shared libcluster configuration\nconfig :libcluster,\n  topologies: [\n    umbrella_cluster: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"umbrella_secret\"\n      ]\n    ]\n  ]\n```","title":"Umbrella Application Configuration - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#umbrella-application-configuration"},{"type":"extras","doc":"ExESDB provides several ways to work with different contexts in umbrella applications:\n\n#### Setting Context\n\n```elixir\n# Set context for orders service\nExESDB.Options.set_context(:orders_service)\nExESDB.Options.data_dir()          # \"/var/lib/orders_events\"\nExESDB.Options.store_id()          # :orders_store\n```\n\n#### Explicit Context\n\n```elixir\n# Use with explicit context\nExESDB.Options.data_dir(:inventory_service)     # \"/var/lib/inventory_events\"\nExESDB.Options.store_id(:user_service)          # :user_store\n```\n\n#### Context Wrapper\n\n```elixir\n# Use with context wrapper\nExESDB.Options.with_context(:orders_service, fn ->\n  ExESDB.Options.db_type()         # :cluster\n  ExESDB.Options.timeout()         # 5000\nend)\n```","title":"Usage in Umbrella Apps - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#usage-in-umbrella-apps"},{"type":"extras","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `data_dir` | String | `\"/data\"` | Directory for storing event data |\n| `store_id` | Atom | `:undefined` | Unique identifier for the store |\n| `timeout` | Integer | `10_000` | Timeout in milliseconds |\n| `db_type` | Atom | `:single` | Database type (`:single` or `:cluster`) |\n| `pub_sub` | Atom | `:ex_esdb_pubsub` | PubSub module name |\n| `reader_idle_ms` | Integer | `10_000` | Reader idle timeout in milliseconds |\n| `writer_idle_ms` | Integer | `10_000` | Writer idle timeout in milliseconds |\n| `store_description` | String | `\"undefined!\"` | Human-readable store description |\n| `store_tags` | List | `[]` | List of tags for the store |\n| `persistence_interval` | Integer | `5_000` | Persistence interval in milliseconds |\n| `persistence_enabled` | Boolean | `true` | Whether persistence is enabled |","title":"Configuration Options - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#configuration-options"},{"type":"extras","doc":"All configuration options can be overridden using environment variables, which take precedence over application configuration:\n\n```bash\n# These will override any app configuration\nexport EX_ESDB_DATA_DIR=\"/tmp/events\"\nexport EX_ESDB_STORE_ID=\"temp_store\"\nexport EX_ESDB_DB_TYPE=\"single\"\nexport EX_ESDB_TIMEOUT=\"2000\"\nexport EX_ESDB_PUB_SUB=\"temp_pubsub\"\nexport EX_ESDB_READER_IDLE_MS=\"5000\"\nexport EX_ESDB_WRITER_IDLE_MS=\"4000\"\nexport EX_ESDB_STORE_DESCRIPTION=\"Temporary Store\"\nexport EX_ESDB_STORE_TAGS=\"temp,testing\"\nexport EX_ESDB_PERSISTENCE_INTERVAL=\"10000\"\nexport EX_ESDB_PERSISTENCE_ENABLED=\"true\"\n```\n\nEnvironment variables are especially useful for:\n- Development vs production configurations\n- Container deployments\n- Testing scenarios\n- Runtime configuration changes","title":"Environment Variable Overrides - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#environment-variable-overrides"},{"type":"extras","doc":"ExESDB is designed to work with `libcluster` for node discovery and clustering. The old `seed_nodes` mechanism has been deprecated in favor of `libcluster`'s more robust topology strategies.","title":"Clustering Configuration - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#clustering-configuration"},{"type":"extras","doc":"Common strategies include:\n\n- **Gossip**: For local network discovery\n- **Kubernetes**: For Kubernetes deployments\n- **ECS**: For AWS ECS deployments\n- **EpMD**: For Erlang Port Mapper Daemon\n- **DNS**: For DNS-based discovery\n\nRefer to the [libcluster documentation](https://hexdocs.pm/libcluster/) for detailed configuration options.","title":"Libcluster Strategies - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#libcluster-strategies"},{"type":"extras","doc":"1. **Use libcluster**: Always prefer `libcluster` over manual seed nodes configuration\n2. **Isolate configurations**: In umbrella apps, keep each service's configuration separate\n3. **Environment-specific configs**: Use environment variables for deployment-specific settings\n4. **Meaningful names**: Use descriptive `store_id` and `store_description` values\n5. **Tagging**: Use `store_tags` for operational visibility and monitoring\n6. **Resource sizing**: Adjust timeout and idle settings based on your workload characteristics","title":"Best Practices - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#best-practices"},{"type":"extras","doc":"If you're migrating from the legacy `khepri` configuration format, update your configuration from:\n\n```elixir\n# Old format (deprecated)\nconfig :ex_esdb, :khepri, [options]\n```\n\nTo:\n\n```elixir\n# New format\nconfig :your_app_name, :ex_esdb, [options]\n```\n\nThe new format provides better isolation and supports umbrella applications more effectively.","title":"Migration from Legacy Configuration - Configuring ExESDB Applications","ref":"configuring-exesdb-apps.html#migration-from-legacy-configuration"},{"type":"extras","doc":"# ExESDB Asynchronous Persistence Architecture","title":"Persistence Architecture","ref":"persistence-architecture.html"},{"type":"extras","doc":"The ExESDB persistence system has been redesigned to handle disk persistence operations asynchronously, eliminating timeout issues that were occurring during event append operations.","title":"Overview - Persistence Architecture","ref":"persistence-architecture.html#overview"},{"type":"extras","doc":"In version 0.3.3, synchronous fence operations were introduced to ensure data persistence to disk. However, these operations caused significant performance issues:\n\n1. **Blocking Operations**: Synchronous `khepri.fence()` calls blocked event append operations\n2. **Timeout Failures**: Operations would timeout after 5 seconds, causing command failures\n3. **Poor User Experience**: System appeared unresponsive during data-heavy operations","title":"Problem Statement - Persistence Architecture","ref":"persistence-architecture.html#problem-statement"},{"type":"extras","doc":"","title":"Solution: Asynchronous Persistence System - Persistence Architecture","ref":"persistence-architecture.html#solution-asynchronous-persistence-system"},{"type":"extras","doc":"#### 1. PersistenceWorker (`ex_esdb/persistence_worker.ex`)\n\nA dedicated GenServer that handles all disk persistence operations asynchronously:\n\n- **Batching**: Collects multiple persistence requests and processes them together\n- **Periodic Execution**: Runs every 5 seconds (configurable) to persist pending data\n- **Non-blocking**: Event append operations return immediately without waiting for disk writes\n- **Graceful Shutdown**: Ensures all pending data is persisted before termination\n\n#### 2. Modified StreamsWriterWorker (`ex_esdb/streams_writer_worker.ex`)\n\nThe event append process now:\n- Stores events in memory immediately\n- Queues a persistence request to the PersistenceWorker\n- Returns success without waiting for disk persistence\n\n#### 3. Enhanced PersistenceSystem (`ex_esdb/persistence_system.ex`)\n\nThe supervisor now includes the PersistenceWorker as a managed component.","title":"Architecture Components - Persistence Architecture","ref":"persistence-architecture.html#architecture-components"},{"type":"extras","doc":"1. **Immediate Response**: Event append operations return instantly\n2. **Better Throughput**: Batched disk operations are more efficient\n3. **Configurable Intervals**: Persistence frequency can be tuned per environment\n4. **Fault Tolerance**: Failed persistence operations are retried automatically\n5. **Graceful Degradation**: System continues operating even if persistence is delayed","title":"Benefits - Persistence Architecture","ref":"persistence-architecture.html#benefits"},{"type":"extras","doc":"The persistence system can be configured in several ways:\n\n#### Application Configuration\n```elixir\n# Global configuration\nconfig :ex_esdb, \n  persistence_interval: 10_000,  # 10 seconds\n  persistence_enabled: true       # Enable/disable persistence\n\n# Per-application configuration (umbrella apps)\nconfig :my_app, :ex_esdb,\n  persistence_interval: 5_000,    # 5 seconds\n  persistence_enabled: true\n```\n\n#### Environment Variables\n```bash\n# Persistence interval in milliseconds\nexport EX_ESDB_PERSISTENCE_INTERVAL=10000\n\n# Enable or disable persistence\nexport EX_ESDB_PERSISTENCE_ENABLED=true\n```\n\n#### Runtime Configuration\n```elixir\n# Per-store configuration at startup\nopts = [\n  store_id: :my_store, \n  persistence_interval: 5_000,\n  otp_app: :my_app\n]\n```\n\n#### Configuration Options\n\n- **`persistence_interval`**: Time in milliseconds between persistence cycles (default: 5000)\n- **`persistence_enabled`**: Whether persistence is enabled (default: true)\n- **`otp_app`**: OTP application name for umbrella app configurations","title":"Configuration - Persistence Architecture","ref":"persistence-architecture.html#configuration"},{"type":"extras","doc":"#### Request Asynchronous Persistence\n```elixir\n# Non-blocking call to request persistence\nExESDB.PersistenceWorker.request_persistence(:my_store)\n```\n\n#### Force Immediate Persistence\n```elixir\n# Blocking call for immediate persistence (useful for testing/shutdown)\nExESDB.PersistenceWorker.force_persistence(:my_store)\n```","title":"API Usage - Persistence Architecture","ref":"persistence-architecture.html#api-usage"},{"type":"extras","doc":"#### Event Flow\n1. Client calls `append_events`\n2. Event is written to Khepri in-memory store\n3. Persistence request is queued with PersistenceWorker\n4. Success is returned immediately to client\n5. PersistenceWorker processes persistence in background\n\n#### Persistence Batching\n- Multiple persistence requests for the same store are deduplicated\n- Periodic timer processes all pending stores together\n- Failed persistence operations are logged but don't affect event storage\n\n#### Error Handling\n- Persistence failures are logged but don't interrupt event processing\n- System continues to operate with in-memory data\n- Persistence is retried on next interval","title":"Implementation Details - Persistence Architecture","ref":"persistence-architecture.html#implementation-details"},{"type":"extras","doc":"The system includes comprehensive testing support:\n\n```elixir\n# For integration tests, force persistence before assertions\nExESDB.PersistenceWorker.force_persistence(:test_store)\n\n# Verify events are persisted\nassert {:ok, events} = ExESDB.get_events(:test_store, stream_id)\n```","title":"Testing - Persistence Architecture","ref":"persistence-architecture.html#testing"},{"type":"extras","doc":"#### From Synchronous to Asynchronous Persistence\n\n- **Immediate Effect**: Event append operations will be significantly faster\n- **Eventual Consistency**: Data is eventually persisted (within persistence interval)\n- **Testing Impact**: Tests may need to call `force_persistence` before assertions\n- **Configuration**: Default 5-second interval can be adjusted per requirements\n\n#### Backward Compatibility\n\nThe changes are fully backward compatible:\n- Existing APIs continue to work unchanged\n- No changes required to client code\n- Configuration is optional (sensible defaults provided)","title":"Migration Notes - Persistence Architecture","ref":"persistence-architecture.html#migration-notes"},{"type":"extras","doc":"#### Before (Synchronous)\n- Event append time: ~5+ seconds (with fence operation)\n- Frequent timeouts under load\n- Poor user experience\n\n#### After (Asynchronous)\n- Event append time: ~10-50ms (memory write only)\n- No timeouts\n- Smooth user experience\n- Configurable persistence latency","title":"Performance Characteristics - Persistence Architecture","ref":"persistence-architecture.html#performance-characteristics"},{"type":"extras","doc":"The system provides comprehensive logging:\n\n```\n[info] PersistenceWorker[my_store] is UP\n[debug] PersistenceWorker[my_store] persisting 3 stores\n[debug] Successfully persisted store my_store\n```","title":"Monitoring - Persistence Architecture","ref":"persistence-architecture.html#monitoring"},{"type":"extras","doc":"- Data is stored in memory immediately, so it's not lost on process restart\n- Khepri handles in-memory to disk persistence reliably\n- Graceful shutdown ensures no data loss during system shutdown","title":"Security Considerations - Persistence Architecture","ref":"persistence-architecture.html#security-considerations"},{"type":"extras","doc":"The asynchronous persistence architecture eliminates timeout issues while maintaining data durability. The system is now responsive, scalable, and provides a better user experience while ensuring data integrity through reliable background persistence.","title":"Conclusion - Persistence Architecture","ref":"persistence-architecture.html#conclusion"},{"type":"extras","doc":"# General Implementation Guidelines for Reckon_* Applications","title":"Implementation Guidelines","ref":"implementation-guidelines.html"},{"type":"extras","doc":"Traditional layered architectures organize code by **technical concerns**controllers, services, repositories, models. This approach creates several problems:","title":"Introduction: Why Vertical Slicing and Screaming Architecture? - Implementation Guidelines","ref":"implementation-guidelines.html#introduction-why-vertical-slicing-and-screaming-architecture"},{"type":"extras","doc":"1. **Scattered Business Logic**: A single business operation spans multiple layers and directories\n2. **Cognitive Overhead**: Developers must navigate between layers to understand one feature\n3. **Tight Coupling**: Changes often require modifications across multiple layers\n4. **Testing Complexity**: Integration tests become necessary to verify simple business operations\n5. **Team Conflicts**: Multiple developers working on the same layers create merge conflicts\n6. **Hidden Business Intent**: The codebase doesn't reveal what the system actually does","title":"Problems with Layered Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#problems-with-layered-architecture"},{"type":"extras","doc":"**Vertical slicing** organizes code by **business capabilities** instead of technical layers. Each business operation becomes a self-contained \"slice\" with everything it needs:\n\n- **Single Source of Truth**: All code for one business operation lives together\n- **Reduced Cognitive Load**: Developers see the complete feature in one place\n- **Independent Evolution**: Features can change without affecting others\n- **Simplified Testing**: Each slice can be tested in isolation\n- **Team Autonomy**: Different teams can own different slices\n- **Business Alignment**: Code structure mirrors how the business thinks","title":"Why We Chose Vertical Slicing - Implementation Guidelines","ref":"implementation-guidelines.html#why-we-chose-vertical-slicing"},{"type":"extras","doc":"**Screaming Architecture** (Uncle Bob's term) means your codebase \"screams\" its business intent. When someone looks at the folder structure, they immediately understand:\n\n- **What the system does** (not how it's implemented)\n- **What business operations exist** (initialize_account, cast_vote)\n- **What the domain is about** (voting, accounts, profiles)\n\nInstead of seeing generic technical folders like `controllers/`, `services/`, `models/`, you see business-focused folders like `initialize_poll/`, `cast_vote/`, `activate_account/`.","title":"Why We Chose Screaming Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#why-we-chose-screaming-architecture"},{"type":"extras","doc":"This architecture has proven benefits in production systems:\n\n- **Faster Onboarding**: New developers understand the system quickly\n- **Reduced Bugs**: Business logic stays cohesive and isolated\n- **Better Documentation**: The code structure IS the documentation\n- **Easier Refactoring**: Changes are localized to specific slices\n- **Business Conversations**: Non-technical stakeholders can navigate the codebase","title":"Real-World Benefits - Implementation Guidelines","ref":"implementation-guidelines.html#real-world-benefits"},{"type":"extras","doc":"","title":"Architecture Philosophy - Implementation Guidelines","ref":"implementation-guidelines.html#architecture-philosophy"},{"type":"extras","doc":"The reckon_* applications follow a **vertical slicing architecture** where each business operation (use case) is implemented as a self-contained slice that includes all necessary components:\n\n- **Command**: Input structure defining the operation\n- **Event**: Output structure representing what happened\n- **Handler**: Business logic processing the command and emitting events\n- **Projections**: Read models updated by events (when needed)\n\nThis approach ensures that each business capability is:\n- **Isolated**: Changes to one slice don't affect others\n- **Cohesive**: All related code is co-located\n- **Testable**: Each slice can be tested independently\n- **Discoverable**: Business operations are obvious from the folder structure","title":"Vertical Slicing Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#vertical-slicing-architecture"},{"type":"extras","doc":"The codebase \"screams\" its business intent through:\n\n1. **Directory Structure**: Business operations are immediately visible as top-level folders\n2. **Module Names**: Clearly express business concepts, not technical layers\n3. **File Organization**: Business logic is organized by use case, not by technical pattern","title":"Screaming Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#screaming-architecture"},{"type":"extras","doc":"**RULE**: Each new project MUST start with an initial Event Storming session, of which the result will be written in the apps `./design_docs/event-storming.md` document.\n\nThis document is a timestamped log of event storming sessions and will contain the following sections:\n\n1. **Executive Summary** (a description of the process under design)\n2. **ASCII diagram** that depicts the commands, the policies they are called from, the events they emit\n3. **ASCII code structure diagram**\n4. **Description of each slice**\n\nThe event storming document serves as the foundation for implementing the vertical slicing architecture and ensures all stakeholders understand the business processes before development begins.","title":" MANDATORY Event Storming Requirement - Implementation Guidelines","ref":"implementation-guidelines.html#mandatory-event-storming-requirement"},{"type":"extras","doc":"**These rules are MANDATORY and MUST be followed without exception:**","title":" STRICT VERSIONING RULES - Implementation Guidelines","ref":"implementation-guidelines.html#strict-versioning-rules"},{"type":"extras","doc":"- **Commands**: `InitializePollV1`, `CastVoteV1`\n- **Events**: `PollInitializedV1`, `VoteCastedV1`\n- **Event Type Strings**: `\"poll_initialized:v1\"`, `\"vote_casted:v1\"`","title":"1. All Commands and Events MUST Have Versions - Implementation Guidelines","ref":"implementation-guidelines.html#1-all-commands-and-events-must-have-versions"},{"type":"extras","doc":"- **Format**: `[command_name]/` (no version in directory name)\n- **Examples**: `initialize_poll/`, `cast_vote/`, `activate_account/`\n- **Rationale**: Prevents unwieldy module names like `InitializePollV1.CommandV1`","title":"2. Directory Names Are Clean (No Version Suffix) - Implementation Guidelines","ref":"implementation-guidelines.html#2-directory-names-are-clean-no-version-suffix"},{"type":"extras","doc":"- **Files**: `command_v1.ex`, `event_v1.ex`, `maybe_initialize_poll_v1.ex`\n- **Event Handlers**: `initialized_to_state_v1.ex`, `casted_to_summary_v1.ex`\n- **Projections**: `initialized_to_summary_v1.ex`\n- **Evolution**: V2 files coexist in same directory: `command_v2.ex`, `event_v2.ex`","title":"3. File Names MUST Include Version - Implementation Guidelines","ref":"implementation-guidelines.html#3-file-names-must-include-version"},{"type":"extras","doc":"**CRUD events have NO business meaning and are STRICTLY FORBIDDEN:**\n\n** FORBIDDEN CRUD Events:**\n- `PollCreated`, `AccountUpdated`, `UserDeleted`\n- `ProfileCreated`, `MembershipUpdated`\n- Any event ending in `-Created`, `-Updated`, `-Deleted`\n\n** REQUIRED: Business-Meaningful Events:**\n- `PollInitialized`, `AccountActivated`, `UserSuspended`\n- `ProfileEstablished`, `MembershipGranted`, `MembershipExpired`\n- Events that express **business intent and meaning**","title":"4.  FORBIDDEN: CRUD Events - Implementation Guidelines","ref":"implementation-guidelines.html#4-forbidden-crud-events"},{"type":"extras","doc":"```\nreckon_[domain]/\n lib/\n    reckon_[domain]/\n       application.ex           # OTP application bootstrap\n       repo.ex                  # Ecto repository for read models\n       router.ex                # Commanded router for command dispatch\n       shared/\n          [aggregate].ex       # Domain aggregate root\n      \n       domain/                  #  LITERAL \"domain\" directory (invariant)\n          [command_name]/      #  VERTICAL SLICE (named after COMMAND)\n             command.ex       # Command structure (input)\n             event.ex         # Event structure (output)\n             handler.ex       # Command handler (business logic)\n             [event]_to_state.ex                    # Event handler (aggregate updates)\n             [event]_to_[readmodel].ex              # Projection (optional)\n             when_[event]_then_[command].ex         # Policy (optional)\n         \n          [another_command]/   # Another vertical slice\n              command.ex\n              event.ex\n              handler.ex\n              [event]_to_state.ex\n              ...\n      \n       projections/             # Cross-cutting read models (if needed)\n           [projection_name].ex\n   \n    reckon_[domain].ex           # Public API (business facade)\n    mix/\n        tasks/\n            reckon_[domain].setup.ex\n            reckon_[domain].reset.ex\n config/\n    config.exs                   # Main config with ExESDB setup\n    dev.exs\n    test.exs\n    prod.exs\n test/\n priv/\n mix.exs\n```","title":"Project Structure - Implementation Guidelines","ref":"implementation-guidelines.html#project-structure"},{"type":"extras","doc":"","title":"Core Principles - Implementation Guidelines","ref":"implementation-guidelines.html#core-principles"},{"type":"extras","doc":"**FUNDAMENTAL RULE**: We follow a strict 1-capability-per-module strategy across all components:\n\n- **1 Command per Module**: Each command gets its own dedicated module\n- **1 Event per Module**: Each event gets its own dedicated module  \n- **1 Handler per Module**: Each handler processes one command type\n- **1 Projection per Module**: Each projection handles one event type to one read model\n- **1 Policy per Module**: Each policy (process manager) handles one event-to-command translation\n\nThis ensures maximum cohesion, testability, and discoverability.","title":"1. One Capability Per Module Strategy - Implementation Guidelines","ref":"implementation-guidelines.html#1-one-capability-per-module-strategy"},{"type":"extras","doc":"** WRONG - Grouping multiple commands/events in one module:**\n```elixir\ndefmodule ReckonProfiles.Commands do\n  defmodule CreateProfile do\n    # ...\n  end\n  \n  defmodule UpdateProfile do\n    # ...\n  end\nend\n```\n\n** CORRECT - Each command/event in its own dedicated module:**\n```elixir\n# lib/reckon_profiles/create_profile/command.ex\ndefmodule ReckonProfiles.CreateProfile.Command do\n  # ...\nend\n\n# lib/reckon_profiles/update_profile/command.ex\ndefmodule ReckonProfiles.UpdateProfile.Command do\n  # ...\nend\n```","title":"2. Each Command/Event Must Reside in Its Own Module - Implementation Guidelines","ref":"implementation-guidelines.html#2-each-command-event-must-reside-in-its-own-module"},{"type":"extras","doc":"Each business operation follows this exact pattern:\n\n```\ndomain/\n [command_name]/                      #  SLICE named after COMMAND (no version)\n     command_v1.ex                    # Input structure - what the user wants to do\n     event_v1.ex                      # Output structure - what actually happened\n     maybe_[command]_v1.ex            # Command handler - business logic\n     [event]_to_state_v1.ex           # Event handler - aggregate state updates\n     [event]_to_[readmodel]_v1.ex     # Projection (optional)\n     when_[event]_then_[command]_v1.ex # Policy (optional)\n    \n    # V2 Evolution (coexists in same directory)\n     command_v2.ex                    # Updated command structure\n     event_v2.ex                      # Updated event structure\n     maybe_[command]_v2.ex            # Updated command handler\n```\n\n**Key Points:**\n- All slices go under the literal `domain/` directory\n- Slices are named after the **command** they contain (not the business operation)\n- `maybe_[command].ex` = **Command Handler** (processes commands, emits events)\n- `[event]_to_state.ex` = **Event Handler** (applies events to aggregate state)\n- Policies go in the slice where the **command** they trigger is located","title":"3. Vertical Slice Structure - Implementation Guidelines","ref":"implementation-guidelines.html#3-vertical-slice-structure"},{"type":"extras","doc":"**MANDATORY FORMAT**: Projections MUST be named using the exact format ` _to_ .ex`\n\n**Rules:**\n- Projections belong in the same slice as the event they process\n- File name format: `[lowercase_event_name]_to_[readmodel_name].ex`\n- Module name format: `Domain.BusinessOperation.EventToReadmodel`\n- Each projection handles exactly ONE event type to ONE read model\n\n**Examples:**\n```\n# File: lib/reckon_accounts/initialize_account/initialized_to_summary.ex\n# Module: ReckonAccounts.InitializeAccount.InitializedToSummary\n# Processes: AccountInitialized  AccountSummary\n\n# File: lib/reckon_profiles/establish_profile/established_to_directory.ex\n# Module: ReckonProfiles.EstablishProfile.EstablishedToDirectory\n# Processes: ProfileEstablished  ProfileDirectory\n```","title":"4. Strict Projection Naming Convention - Implementation Guidelines","ref":"implementation-guidelines.html#4-strict-projection-naming-convention"},{"type":"extras","doc":"**Purpose**: Policies translate events into commands and dispatch them through CommandedApp.\n\n**Rules:**\n- Policies belong in the slice where the **command** they trigger is located\n- File name format: `when_ _then_ .ex`\n- Module name format: `Domain.CommandName.When Then `\n- Each policy handles ONE event type and dispatches ONE command type\n- **MANDATORY**: All command dispatching MUST go through `CommandedApp.dispatch/1`\n\n**Policy Structure:**\n```elixir\n# File: lib/reckon_profiles/domain/establish_profile/when_account_activated_then_establish_profile.ex\ndefmodule ReckonProfiles.Domain.EstablishProfile.WhenAccountActivatedThenEstablishProfile do\n  @moduledoc \"\"\"\n  Policy that triggers profile establishment when an account is activated.\n  \n  This policy listens to AccountActivated events and automatically\n  dispatches an EstablishProfile command to the profiles domain.\n  \"\"\"\n  \n  use Commanded.Event.Handler,\n    application: ReckonProfiles.CommandedApp,\n    name: \"when_account_activated_then_establish_profile\"\n  \n  alias ReckonAccounts.Domain.ActivateAccount.Event, as: AccountActivated\n  alias ReckonProfiles.Domain.EstablishProfile.Command, as: EstablishProfileCommand\n  alias ReckonProfiles.CommandedApp\n  \n  def handle(%AccountActivated{} = event, _metadata) do\n    command = %EstablishProfileCommand{\n      account_id: event.account_id,\n      email: event.email,\n      requested_at: DateTime.utc_now()\n    }\n    \n    # MANDATORY: Dispatch through CommandedApp\n    CommandedApp.dispatch(command)\n  end\nend\n```\n\n**Key Policy Placement Rule:**\n- The policy goes in the slice of the **command** it triggers, not the event it listens to\n- This makes sense because the policy is about **causing** that command to be executed","title":"5. Policy (Process Manager) Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#5-policy-process-manager-pattern"},{"type":"extras","doc":"To make the new structure crystal clear, here are concrete examples:","title":"Concrete Examples - Implementation Guidelines","ref":"implementation-guidelines.html#concrete-examples"},{"type":"extras","doc":"```\nreckon_profiles/\n lib/\n    reckon_profiles/\n       application.ex\n       repo.ex\n       router.ex\n       shared/\n          profile.ex                        # Profile aggregate\n      \n       domain/                              #  LITERAL \"domain\" directory\n          establish_profile/              # Command slice\n             command.ex                   # EstablishProfile command\n             event.ex                     # ProfileEstablished event\n             handler.ex                   # Command handler\n             established_to_state.ex     # Event handler (ProfileEstablished  Profile aggregate)\n             established_to_directory.ex # Projection (ProfileEstablished  ProfileDirectory)\n             when_account_activated_then_establish_profile.ex  # Policy\n         \n          update_profile/                 # Another command slice\n             command.ex                   # UpdateProfile command\n             event.ex                     # ProfileUpdated event\n             handler.ex                   # Command handler\n             updated_to_state.ex         # Event handler\n             updated_to_directory.ex     # Projection\n         \n          deactivate_profile/\n              command.ex\n              event.ex\n              handler.ex\n              deactivated_to_state.ex\n      \n       projections/                        # Cross-cutting projections (if needed)\n           profile_search_projection.ex\n   \n    reckon_profiles.ex                      # Public API\n    ...\n```","title":"Example: ReckonProfiles Domain Structure - Implementation Guidelines","ref":"implementation-guidelines.html#example-reckonprofiles-domain-structure"},{"type":"extras","doc":"**Command:**\n- File: `lib/reckon_profiles/domain/establish_profile/command.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Command`\n\n**Event:**\n- File: `lib/reckon_profiles/domain/establish_profile/event.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Event`\n\n**Command Handler:**\n- File: `lib/reckon_profiles/domain/establish_profile/handler.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Handler`\n\n**Event Handler (to State):**\n- File: `lib/reckon_profiles/domain/establish_profile/established_to_state.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.EstablishedToState`\n\n**Projection:**\n- File: `lib/reckon_profiles/domain/establish_profile/established_to_directory.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.EstablishedToDirectory`\n\n**Policy:**\n- File: `lib/reckon_profiles/domain/establish_profile/when_account_activated_then_establish_profile.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.WhenAccountActivatedThenEstablishProfile`","title":"Example: File Names and Module Names - Implementation Guidelines","ref":"implementation-guidelines.html#example-file-names-and-module-names"},{"type":"extras","doc":"1. **Directory Structure**: `reckon_[domain]/lib/reckon_[domain]/domain/[command_name]/`\n2. **Slice Names**: Named after the **command** (not the business operation)\n3. **Handler Types**:\n   - `maybe_[command].ex` = Command Handler\n   - `[event]_to_state.ex` = Event Handler (aggregate updates)\n4. **Projections**: `[event]_to_[readmodel].ex` (in slice with event)\n5. **Policies**: `when_[event]_then_[command].ex` (in slice with command they trigger)\n6. **Module Naming**: `ReckonDomain.Domain.CommandName.FileType`","title":"Key Naming Rules Summary - Implementation Guidelines","ref":"implementation-guidelines.html#key-naming-rules-summary"},{"type":"extras","doc":"Folder names should express **business operations**, not technical concepts:\n\n** GOOD - Business-focused names:**\n- `initialize_account/`\n- `verify_email/`\n- `create_profile/`\n- `update_profile_picture/`\n- `close_account/`\n\n** BAD - Technical-focused names:**\n- `commands/`\n- `events/`\n- `handlers/`\n- `controllers/`\n- `services/`","title":"6. Screaming Business Intent - Implementation Guidelines","ref":"implementation-guidelines.html#6-screaming-business-intent"},{"type":"extras","doc":"","title":"Module Naming Conventions - Implementation Guidelines","ref":"implementation-guidelines.html#module-naming-conventions"},{"type":"extras","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Command do\n  @moduledoc \"\"\"\n  Command to create a new user profile.\n  \n  This command is triggered when a user completes their initial\n  profile setup after account verification.\n  \"\"\"\n  \n  defstruct [\n    :account_id,\n    :display_name,\n    :bio,\n    :requested_at\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    display_name: String.t(),\n    bio: String.t() | nil,\n    requested_at: DateTime.t()\n  }\nend\n```","title":"Command Modules - Implementation Guidelines","ref":"implementation-guidelines.html#command-modules"},{"type":"extras","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Event do\n  @moduledoc \"\"\"\n  Event emitted when a profile is successfully created.\n  \n  This event triggers read model updates and may trigger\n  other domain reactions.\n  \"\"\"\n  \n  @derive Jason.Encoder\n  defstruct [\n    :account_id,\n    :display_name,\n    :bio,\n    :created_at,\n    :version\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    display_name: String.t(),\n    bio: String.t() | nil,\n    created_at: DateTime.t(),\n    version: integer()\n  }\nend\n```","title":"Event Modules - Implementation Guidelines","ref":"implementation-guidelines.html#event-modules"},{"type":"extras","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Handler do\n  @moduledoc \"\"\"\n  Command handler for CreateProfile command.\n  \n  Business rules:\n  - Profile can only be created once per account\n  - Display name must be unique\n  - Bio is optional but limited to 500 characters\n  \"\"\"\n  \n  alias ReckonProfiles.Shared.Profile\n  alias ReckonProfiles.CreateProfile.{Command, Event}\n  \n  def execute(%Profile{account_id: nil}, %Command{} = command) do\n    # Business logic here\n    %Event{\n      account_id: command.account_id,\n      display_name: command.display_name,\n      bio: command.bio,\n      created_at: command.requested_at,\n      version: 1\n    }\n  end\n  \n  def execute(%Profile{}, %Command{}) do\n    {:error, :profile_already_exists}\n  end\n  \n  def apply(%Profile{} = profile, %Event{} = event) do\n    # State updates here\n    %Profile{profile |\n      account_id: event.account_id,\n      display_name: event.display_name,\n      bio: event.bio,\n      created_at: event.created_at,\n      updated_at: event.created_at\n    }\n  end\nend\n```","title":"Handler Modules - Implementation Guidelines","ref":"implementation-guidelines.html#handler-modules"},{"type":"extras","doc":"","title":"Technical Configuration - Implementation Guidelines","ref":"implementation-guidelines.html#technical-configuration"},{"type":"extras","doc":"```elixir\ndefp deps do\n  [\n    {:dns_cluster, \"~> 0.1.1\"},\n    {:phoenix_pubsub, \"~> 2.1\"},\n    {:ecto_sql, \"~> 3.10\"},\n    {:ecto_sqlite3, \">= 0.0.0\"},\n    {:jason, \"~> 1.2\"},\n    {:ex_esdb, \"~> 0.1.4\"},\n    {:ex_esdb_commanded, \"0.1.3\"}\n  ]\nend\n```","title":"Dependencies (mix.exs) - Implementation Guidelines","ref":"implementation-guidelines.html#dependencies-mix-exs"},{"type":"extras","doc":"```elixir\n# Configure ExESDB for ReckonProfiles\nconfig :ex_esdb, :khepri,\n  data_dir: \"tmp/reckon_profiles\",\n  store_id: :reckon_profiles,  #  UNIQUE per domain\n  timeout: 10_000,\n  db_type: :single,\n  pub_sub: :ex_esdb_pubsub,\n  store_description: \"Reckon Profiles Event Store\",\n  store_tags: [\"reckon\", \"profiles\", \"event-sourcing\", \"development\"]\n\n# Configure the Commanded application\nconfig :reckon_profiles, ReckonProfiles.CommandedApp,\n  event_store: [\n    adapter: ExESDB.Commanded.Adapter,\n    store_id: :reckon_profiles,\n    stream_prefix: \"reckon_profiles_\",  #  UNIQUE per domain\n    serializer: Jason,\n    event_type_mapper: ReckonProfiles.EventTypeMapper\n  ]\n```","title":"ExESDB Configuration (config/config.exs) - Implementation Guidelines","ref":"implementation-guidelines.html#exesdb-configuration-config-config-exs"},{"type":"extras","doc":"```elixir\n# Configure libcluster (preferred over seed_nodes)\nconfig :libcluster,\n  topologies: [\n    reckon_profiles: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45_894,  #  UNIQUE per domain\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"255.255.255.255\",\n        broadcast_only: true,\n        secret: System.get_env(\"RECKON_PROFILES_CLUSTER_SECRET\") || \"reckon_profiles_cluster_secret\"\n      ]\n    ]\n  ]\n```","title":"LibCluster Configuration (User Preference) - Implementation Guidelines","ref":"implementation-guidelines.html#libcluster-configuration-user-preference"},{"type":"extras","doc":"The main module provides a business-focused API:\n\n```elixir\ndefmodule ReckonProfiles do\n  @moduledoc \"\"\"\n  ReckonProfiles domain - User profile management and personalization.\n  \n  This module provides the public API for profile operations including:\n  - Profile creation and updates\n  - Profile picture management\n  - Privacy settings\n  \n  Uses vertical slicing architecture where each command has its own slice\n  containing command, events, and handlers.\n  \"\"\"\n  \n  alias ReckonProfiles.CommandedApp\n  \n  @doc \"\"\"\n  Creates a new user profile.\n  \n  This is typically called after account verification is complete.\n  \"\"\"\n  def create_profile(account_id, display_name, bio \\\\ nil) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    CommandedApp.dispatch(command)\n  end\n  \n  # ... other business operations\nend\n```","title":"Public API Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#public-api-pattern"},{"type":"extras","doc":"1. **Discoverability**: New developers can immediately understand what the system does by looking at folder names\n2. **Maintainability**: Changes to one business operation don't affect others\n3. **Testability**: Each slice can be unit tested independently\n4. **Scalability**: Teams can work on different slices without conflicts\n5. **Business Alignment**: Code structure mirrors business processes","title":"Key Benefits of This Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#key-benefits-of-this-architecture"},{"type":"extras","doc":"","title":"Projection Naming and Organization - Implementation Guidelines","ref":"implementation-guidelines.html#projection-naming-and-organization"},{"type":"extras","doc":"Projections should use the `event_to_projection_type` naming pattern to immediately communicate their intent and relationship, following screaming architecture principles:\n\n```\nreckon_[domain]/\n lib/\n    reckon_[domain]/\n       initialize_account/\n          command.ex\n          event.ex\n          handler.ex\n          initialized_to_summary.ex    # Projection: AccountInitialized  AccountSummary\n       activate_account/\n          command.ex\n          event.ex\n          handler.ex\n          activated_to_summary.ex      # Projection: AccountActivated  AccountSummary\n       close_account/\n           command.ex\n           event.ex\n           handler.ex\n           closed_to_summary.ex         # Projection: AccountClosed  AccountSummary\n```","title":"Event-to-Projection Naming Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#event-to-projection-naming-pattern"},{"type":"extras","doc":"Projection modules should follow the same naming pattern:\n\n```elixir\n# lib/reckon_accounts/initialize_account/initialized_to_summary.ex\ndefmodule ReckonAccounts.InitializeAccount.InitializedToSummary do\n  @moduledoc \"\"\"\n  Projection that handles AccountInitialized events and updates the AccountSummary read model.\n  \n  This projection creates new entries in the account_summaries table when accounts are initialized.\n  \"\"\"\n  \n  use Commanded.Projections.Ecto,\n    application: ReckonAccounts.CommandedApp,\n    repo: ReckonAccounts.Repo,\n    name: \"ReckonAccounts.InitializeAccount.InitializedToSummary\"\n  \n  alias ReckonAccounts.InitializeAccount.Event, as: AccountInitialized\n  alias ReckonAccounts.Schemas.AccountSummary\n  \n  project(%AccountInitialized{} = event, _metadata, fn multi ->\n    # Projection logic here\n  end)\nend\n```","title":"Projection Module Naming - Implementation Guidelines","ref":"implementation-guidelines.html#projection-module-naming"},{"type":"extras","doc":"","title":"Testing Guidelines - Implementation Guidelines","ref":"implementation-guidelines.html#testing-guidelines"},{"type":"extras","doc":"Tests should mirror the vertical slicing architecture and include projections within their related slices:\n\n```\nreckon_[domain]/\n test/\n    reckon_[domain]/\n       initialize_account/              # Tests for initialize_account slice\n          command_test.exs\n          handler_test.exs\n          event_test.exs\n          initialized_to_summary_test.exs  # Projection test within slice\n       activate_account/                # Tests for activate_account slice\n          command_test.exs\n          handler_test.exs\n          event_test.exs\n          activated_to_summary_test.exs    # Projection test within slice\n       close_account/\n           command_test.exs\n           handler_test.exs\n           event_test.exs\n           closed_to_summary_test.exs       # Projection test within slice\n    support/\n        test_helper.ex\n```","title":"Vertical Slicing in Tests - Implementation Guidelines","ref":"implementation-guidelines.html#vertical-slicing-in-tests"},{"type":"extras","doc":"1. **Domain Tests Should Be Isolated**: Each domain's tests should run independently without requiring other domains to be running.\n\n2. **Use Ecto.Adapters.SQL.Sandbox**: For database tests, use sandbox mode to ensure test isolation:\n\n```elixir\n# test/test_helper.exs\nEcto.Adapters.SQL.Sandbox.mode(ReckonAccounts.Repo, :manual)\n\n# In test modules\nsetup do\n  :ok = Ecto.Adapters.SQL.Sandbox.checkout(ReckonAccounts.Repo)\nend\n```\n\n3. **Test Each Slice Independently**: Write unit tests for commands, handlers, events, and projections separately within each slice.\n\n4. **Integration Tests for Cross-Slice Interactions**: Use integration tests sparingly and only when testing interactions between slices within the same domain.","title":"Test Isolation Principles - Implementation Guidelines","ref":"implementation-guidelines.html#test-isolation-principles"},{"type":"extras","doc":"1. **Projections Stay With Their Events**: Projections should be located in the same slice as their related events, using the `event_to_projection_type` naming pattern.\n\n2. **Test Projections Separately**: Write dedicated tests for projections that verify:\n   - Event handling and state updates\n   - Database persistence\n   - Error handling and recovery\n\n3. **Use Test Fixtures**: Create test fixtures for events to ensure consistent testing:\n\n```elixir\n# test/support/fixtures.ex\ndefmodule ReckonAccounts.Fixtures do\n  def account_initialized_event(attrs \\\\ %{}) do\n    %ReckonAccounts.InitializeAccount.Event{\n      account_id: attrs[:account_id] || \"test-account-123\",\n      email: attrs[:email] || \"test@example.com\",\n      initialized_at: attrs[:initialized_at] || DateTime.utc_now() |> DateTime.truncate(:second)\n    }\n  end\nend\n```","title":"Projection Testing Guidelines - Implementation Guidelines","ref":"implementation-guidelines.html#projection-testing-guidelines"},{"type":"extras","doc":"","title":"Integration Between Domains - Implementation Guidelines","ref":"implementation-guidelines.html#integration-between-domains"},{"type":"extras","doc":"Integrations between domain services (`reckon_*` apps) and the website (`landing_site_web`) must go through the umbrella's web service proxy (`landing_site`). This ensures:\n\n1. **Loose Coupling**: Domains don't directly depend on web concerns\n2. **Consistent API**: All web interactions go through a single, well-defined interface\n3. **Testability**: Web and domain logic can be tested independently\n4. **Scalability**: Domains can be extracted to separate services later\n\n```elixir\n#  Correct: Web interactions through proxy\nLandingSite.Accounts.initialize_account(email, password)\n\n#  Incorrect: Direct domain access from web\nReckonAccounts.initialize_account(email, password)\n```","title":"Web Service Proxy Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#web-service-proxy-pattern"},{"type":"extras","doc":"","title":"Integration Events (Facts) Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#integration-events-facts-pattern"},{"type":"extras","doc":"**Domain Events** are internal to a domain and handle business logic within that domain.\n\n**Facts** are integration events that communicate between domains. They represent immutable facts about what happened in one domain that other domains might care about.","title":"Facts vs Domain Events - Implementation Guidelines","ref":"implementation-guidelines.html#facts-vs-domain-events"},{"type":"extras","doc":"Facts are defined in the `reckon_shared` application, organized by originating domain:\n\n```\nreckon_shared/\n lib/\n    reckon_shared/\n       accounts/                    # Facts from reckon_accounts domain\n          account_initialized_fact.ex\n          account_activated_fact.ex\n          account_closed_fact.ex\n       profiles/                    # Facts from reckon_profiles domain\n          profile_created_fact.ex\n          profile_updated_fact.ex\n       memberships/                 # Facts from reckon_memberships domain\n           membership_created_fact.ex\n           membership_expired_fact.ex\n    reckon_shared.ex\n```","title":"Facts Structure - Implementation Guidelines","ref":"implementation-guidelines.html#facts-structure"},{"type":"extras","doc":"```elixir\n# lib/reckon_shared/accounts/account_activated_fact.ex\ndefmodule ReckonShared.Accounts.AccountActivatedFact do\n  @moduledoc \"\"\"\n  Integration event (Fact) emitted when an account is activated.\n  \n  This fact is consumed by other domains that need to react to\n  account activation, such as:\n  - ReckonProfiles (to enable profile creation)\n  - ReckonMemberships (to activate trial memberships)\n  \"\"\"\n  \n  @derive Jason.Encoder\n  defstruct [\n    :account_id,\n    :email,\n    :activated_at,\n    :fact_id,\n    :fact_version\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    email: String.t(),\n    activated_at: DateTime.t(),\n    fact_id: String.t(),\n    fact_version: integer()\n  }\n  \n  @doc \"\"\"\n  Creates a new account activated fact.\n  \"\"\"\n  def new(account_id, email, activated_at \\\\ DateTime.utc_now()) do\n    %__MODULE__{\n      account_id: account_id,\n      email: email,\n      activated_at: activated_at,\n      fact_id: UUID.uuid4(),\n      fact_version: 1\n    }\n  end\nend\n```","title":"Fact Module Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#fact-module-pattern"},{"type":"extras","doc":"Each domain has projections that convert domain events into facts and publish them:\n```elixir\n# lib/reckon_accounts/projections/account_facts_projection.ex\ndefmodule ReckonAccounts.Projections.AccountFactsProjection do\n  @moduledoc \"\"\"\n  Projection that converts domain events into integration facts.\n  \n  Publishes facts to PubSub topics for consumption by other domains.\n  \"\"\"\n  \n  use Commanded.Projections.Ecto,\n    application: ReckonAccounts.CommandedApp,\n    repo: ReckonAccounts.Repo,\n    name: \"account_facts_projection\"\n  \n  alias ReckonAccounts.VerifyEmail.Event, as: EmailVerifiedEvent\n  alias ReckonShared.Accounts.AccountActivatedFact\n  \n  project(%EmailVerifiedEvent{} = event, _metadata) do\n    # Convert domain event to integration fact\n    fact = AccountActivatedFact.new(\n      event.account_id,\n      event.email,\n      event.verified_at\n    )\n    \n    # Publish to event-specific topic\n    Phoenix.PubSub.publish(\n      :ex_esdb_pubsub,\n      \"accounts:facts:account_activated\",\n      fact\n    )\n    \n    :ok\n  end\nend\n```","title":"Fact Projection Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#fact-projection-pattern"},{"type":"extras","doc":"**Design Rule**: We communicate via event-specific topics, not generic domain topics.\n\n** GOOD - Event-specific topics:**\n- `\"accounts:facts:account_activated\"`\n- `\"accounts:facts:account_closed\"`\n- `\"profiles:facts:profile_created\"`\n- `\"profiles:facts:profile_updated\"`\n- `\"memberships:facts:membership_created\"`\n\n** BAD - Generic domain topics:**\n- `\"accounts:events\"`\n- `\"profiles:all\"`\n- `\"domain:updates\"`","title":"Event-Specific Topics Communication - Implementation Guidelines","ref":"implementation-guidelines.html#event-specific-topics-communication"},{"type":"extras","doc":"Other domains subscribe to specific fact topics:\n```elixir\n# lib/reckon_profiles/event_handlers/account_event_handler.ex\ndefmodule ReckonProfiles.EventHandlers.AccountEventHandler do\n  @moduledoc \"\"\"\n  Handles account-related facts from other domains.\n  \"\"\"\n  \n  use GenServer\n  \n  alias ReckonShared.Accounts.AccountActivatedFact\n  alias ReckonProfiles.CreateProfile.Command\n  \n  def start_link(opts) do\n    GenServer.start_link(__MODULE__, opts, name: __MODULE__)\n  end\n  \n  def init(_opts) do\n    # Subscribe to specific account facts\n    Phoenix.PubSub.subscribe(:ex_esdb_pubsub, \"accounts:facts:account_activated\")\n    Phoenix.PubSub.subscribe(:ex_esdb_pubsub, \"accounts:facts:account_closed\")\n    \n    {:ok, %{}}\n  end\n  \n  def handle_info(%AccountActivatedFact{} = fact, state) do\n    # React to account activation by enabling profile creation\n    # (Business logic here)\n    {:noreply, state}\n  end\nend\n```","title":"Cross-Domain Event Handling - Implementation Guidelines","ref":"implementation-guidelines.html#cross-domain-event-handling"},{"type":"extras","doc":"```\n[source_domain]:facts:[event_name]\n```\n\n**Examples:**\n- `accounts:facts:account_initialized`\n- `accounts:facts:account_activated`\n- `accounts:facts:account_closed`\n- `profiles:facts:profile_created`\n- `profiles:facts:profile_picture_updated`\n- `memberships:facts:membership_created`\n- `memberships:facts:membership_expired`","title":"Topic Naming Convention - Implementation Guidelines","ref":"implementation-guidelines.html#topic-naming-convention"},{"type":"extras","doc":"1. **Decoupling**: Domains don't know about each other, only about facts\n2. **Versioning**: Facts can be versioned independently\n3. **Selective Consumption**: Domains subscribe only to events they care about\n4. **Auditability**: All integration events are explicitly defined as facts\n5. **Testability**: Fact publishing and consumption can be tested independently","title":"Benefits of This Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#benefits-of-this-pattern"},{"type":"extras","doc":"","title":"Web UI Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#web-ui-architecture"},{"type":"extras","doc":"**Design Rule**: Use Phoenix LiveView instead of classic MVC architecture for interactive web interfaces.\n\n** PREFERRED - LiveView architecture:**\n- Real-time interactivity without JavaScript\n- Stateful user interfaces\n- Server-side rendering with client-side updates\n- Built-in handling of form validation and user feedback\n- Simplified state management\n\n** AVOID - Classic MVC where LiveView is suitable:**\n- Controllers for simple form handling\n- Multiple request/response cycles for interactive features\n- Client-side JavaScript for basic interactivity\n- Complex form validation handling","title":"Phoenix LiveView Preference - Implementation Guidelines","ref":"implementation-guidelines.html#phoenix-liveview-preference"},{"type":"extras","doc":"```elixir\n# lib/landing_site_web/live/auth_live.ex\ndefmodule LandingSiteWeb.AuthLive do\n  use LandingSiteWeb, :live_view\n  \n  # LiveView callbacks\n  def mount(_params, _session, socket) do\n    # Initialize socket state\n  end\n  \n  def handle_event(\"register\", %{\"user\" => user_params}, socket) do\n    # Handle user registration\n  end\n  \n  def handle_event(\"login\", %{\"user\" => user_params}, socket) do\n    # Handle user authentication\n  end\n  \n  def render(assigns) do\n    # Render the LiveView template\n  end\nend\n```","title":"LiveView Module Structure - Implementation Guidelines","ref":"implementation-guidelines.html#liveview-module-structure"},{"type":"extras","doc":"**Acceptable use cases for Controllers:**\n- API endpoints (JSON responses)\n- Simple redirects or downloads\n- Authentication callbacks (OAuth, etc.)\n- Webhook handlers\n- Static page rendering without interactivity","title":"When to Use Classic MVC - Implementation Guidelines","ref":"implementation-guidelines.html#when-to-use-classic-mvc"},{"type":"extras","doc":"Refactoring from classic MVC to LiveView is generally straightforward:\n\n1. **Controller actions**  **LiveView event handlers**\n2. **Form submissions**  **LiveView events**\n3. **Flash messages**  **LiveView assigns**\n4. **Redirects**  **LiveView navigation**\n5. **Template rendering**  **LiveView render function**\n\n**Example migration:**\n```elixir\n# Before: Classic MVC\ndef create_account(conn, %{\"user\" => user_params}) do\n  case UserContext.register_user(user_params) do\n    {:ok, user} ->\n      conn\n      |> put_flash(:info, \"Account created successfully\")\n      |> redirect(to: ~p\"/auth/login\")\n    {:error, changeset} ->\n      render(conn, :register, changeset: changeset)\n  end\nend\n\n# After: LiveView\ndef handle_event(\"register\", %{\"user\" => user_params}, socket) do\n  case UserContext.register_user(user_params) do\n    {:ok, user} ->\n      socket\n      |> put_flash(:info, \"Account created successfully\")\n      |> push_navigate(to: ~p\"/auth/login\")\n      |> noreply()\n    {:error, changeset} ->\n      socket\n      |> assign(:changeset, changeset)\n      |> noreply()\n  end\nend\n```","title":"Migration from MVC to LiveView - Implementation Guidelines","ref":"implementation-guidelines.html#migration-from-mvc-to-liveview"},{"type":"extras","doc":"","title":"Reckon_App Integration Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#reckon_app-integration-pattern"},{"type":"extras","doc":"**Design Rule**: Each reckon_app owns a DomainAPI GenServer that provides the interface for external communication.\n\n**Architecture Components:**\n\n1. **DomainAPI GenServer**: Each reckon_app implements its own DomainAPI that:\n   - Registers itself in Swarm using `Swarm.register_name(api_name(), self())`\n   - Offers a user-friendly set of API functions for sending commands to the Domain\n   - Handles `GenServer.cast` and `GenServer.call` callbacks\n   - Uses pattern matching like `GenServer.cast(api_pid(), message)` to route messages\n\n2. **Service Registration**: \n   ```elixir\n   # In each reckon_app's DomainAPI\n   defmodule ReckonProfiles.DomainAPI do\n     use GenServer\n\n     \n     @domain_name :my_domain\n     \n     def api_name(), do: {:domain_api, @domain_name}\n     def api_pid(), do: Swarm.whereis_name(api_name())\n     \n     def start_link(opts) do\n       GenServer.start_link(__MODULE__, opts, name: __MODULE__)\n     end\n     \n     def init(opts) do\n       # Register this API with Swarm for discovery\n       Swarm.register_name({:domain_api, :reckon_profiles}, __MODULE__, [])\n       {:ok, opts}\n     end\n     \n     # User-friendly API functions\n     def establish_profile(account_id, display_name, bio \\\\ nil) do\n       command = %ReckonProfiles.EstablishProfile.Command{\n         account_id: account_id,\n         display_name: display_name,\n         bio: bio,\n         requested_at: DateTime.utc_now()\n       }\n       \n       GenServer.call(api_pid(), {:dispatch_command, command})\n     end\n     \n     def handle_call({:dispatch_command, command}, _from, state) do\n       result = ReckonProfiles.CommandedApp.dispatch(command)\n       {:reply, result, state}\n     end\n   end\n   ```\n\n3. **Landing Site Integration**: \n   ```elixir\n   # In landing_site mix.exs\n   defp deps do\n     [\n       # Add each reckon_app as dependency without starting the application\n       {:reckon_accounts, path: \"../reckon_apps/reckon_accounts/\", application: false},\n       {:reckon_profiles, path: \"../reckon_apps/reckon_profiles/\", application: false},\n       {:reckon_memberships, path: \"../reckon_apps/reckon_memberships/\", application: false}\n     ]\n   end\n   ```\n\n4. **LibCluster Configuration**: All services use the same cluster configuration to enable service discovery:\n   ```elixir\n   # In each reckon_app's config/config.exs\n   config :libcluster,\n     topologies: [\n       reckon_cluster: [\n         strategy: Cluster.Strategy.Gossip,\n         config: [\n           port: 45_890,  # Shared port for all reckon services\n           if_addr: \"0.0.0.0\",\n           multicast_addr: \"230.1.1.251\",\n           multicast_ttl: 1,\n           secret: System.get_env(\"RECKON_CLUSTER_SECRET\") || \"reckon_cluster_dev_secret\"\n         ]\n       ]\n     ]\n   ```","title":"DomainAPI Architecture - Implementation Guidelines","ref":"implementation-guidelines.html#domainapi-architecture"},{"type":"extras","doc":"1. **Domain Ownership**: Each reckon_app owns its API boundary and communication protocol\n2. **Service Discovery**: Swarm provides automatic service discovery across the cluster\n3. **Fault Tolerance**: Services can be started/stopped independently\n4. **Clean Dependencies**: Landing site depends on reckon_apps for compilation but not runtime\n5. **Scalability**: Multiple instances of each reckon_app can be deployed\n6. **LibCluster Integration**: Follows the preferred clustering approach over seed_nodes","title":"Benefits of This Pattern - Implementation Guidelines","ref":"implementation-guidelines.html#benefits-of-this-pattern-1"},{"type":"extras","doc":"**For each reckon_app:**\n- [ ] Implement DomainAPI GenServer with Swarm registration\n- [ ] Add user-friendly API functions that wrap CommandedApp.dispatch calls\n- [ ] Configure libcluster with shared cluster settings\n- [ ] Add libcluster and swarm dependencies\n- [ ] Update Application supervision tree to start DomainAPI\n\n**For landing_site:**\n- [ ] Add reckon_apps as `application: false` dependencies\n- [ ] Implement service discovery to find available DomainAPIs\n- [ ] Configure libcluster to join the same cluster\n- [ ] Create communication layer that uses Swarm.whereis_name to find services","title":"Implementation Checklist - Implementation Guidelines","ref":"implementation-guidelines.html#implementation-checklist"},{"type":"extras","doc":"1. ** Grouping by technical layer** (controllers/, services/, models/)\n2. ** Shared command/event modules** (mixing multiple operations)\n3. ** Generic naming** (using technical terms instead of business terms)\n4. ** Horizontal slicing** (splitting one business operation across multiple technical layers)\n5. ** Anemic domain models** (putting business logic in \"service\" classes)\n6. ** Generic PubSub topics** (using broad topics instead of event-specific ones)\n7. ** Direct domain coupling** (one domain importing modules from another)\n8. ** Mixing domain events with integration facts** (using same struct for both)\n9. ** CRUD-based event naming** (using generic -created, -updated, -deleted instead of meaningful business events)\n10. ** Direct Router usage** (bypassing CommandedApp for command dispatch)\n11. ** Multiple capabilities per module** (violating 1-capability-per-module rule)\n12. ** Incorrect projection naming** (not following ` _to_ .ex` format)\n13. ** Policies bypassing CommandedApp** (direct command creation without proper dispatch)","title":"Anti-Patterns to Avoid - Implementation Guidelines","ref":"implementation-guidelines.html#anti-patterns-to-avoid"},{"type":"extras","doc":"**This anti-pattern is FORBIDDEN in all reckon_* applications.**\n\n** BAD - Direct Router usage:**\n```elixir\ndefmodule ReckonProfiles do\n  alias ReckonProfiles.Router  #  FORBIDDEN\n  \n  def create_profile(account_id, display_name, bio) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    Router.dispatch(command)  #  FORBIDDEN - bypasses CommandedApp\n  end\nend\n```\n\n** CORRECT - CommandedApp usage:**\n```elixir\ndefmodule ReckonProfiles do\n  alias ReckonProfiles.CommandedApp  #  CORRECT\n  \n  def create_profile(account_id, display_name, bio) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    CommandedApp.dispatch(command)  #  CORRECT - goes through CommandedApp\n  end\nend\n```\n\n**Why this rule exists:**\n\n1. **Architectural Consistency**: CommandedApp is the proper entry point for commands in Commanded\n2. **Centralized Configuration**: CommandedApp handles event store config, middleware, and other application-level concerns\n3. **Middleware Support**: CommandedApp can add middleware for logging, validation, authentication, etc.\n4. **Error Handling**: Provides consistent error handling and retry logic across all commands\n5. **Testing**: Easier to mock CommandedApp than individual router calls\n6. **Future-Proofing**: Easier to add cross-cutting concerns like audit logging, metrics, etc.\n7. **Commanded Best Practices**: `CommandedApp.dispatch()` is the idiomatic way in Commanded\n\n**Command Flow:**\n```\nBusiness Logic (ReckonProfiles.ex)\n         \n  CommandedApp.dispatch()   CORRECT\n         \n    Router (internal routing)\n         \n  Handler (processes command)\n         \n    Events (domain events)\n```\n\n**NOT:**\n```\nBusiness Logic (ReckonProfiles.ex)\n         \n    Router.dispatch()   FORBIDDEN\n         \n  Handler (processes command)\n         \n    Events (domain events)\n```\n\n**The Router is an internal implementation detail that should NEVER be called directly from business logic.**","title":" FORBIDDEN: Direct Router Usage for Command Dispatch - Implementation Guidelines","ref":"implementation-guidelines.html#forbidden-direct-router-usage-for-command-dispatch"},{"type":"extras","doc":"** BAD - CRUD-focused event names:**\n- `ProfileCreated`\n- `ProfileUpdated` \n- `ProfileDeleted`\n- `AccountCreated`\n- `AccountUpdated`\n- `MembershipCreated`\n\n** GOOD - Business-focused event names:**\n- `ProfileEstablished` (when user completes initial profile setup)\n- `ProfilePersonalized` (when user customizes their profile)\n- `ProfileDeactivated` (when user temporarily hides their profile)\n- `AccountInitialized` (when registration begins)\n- `AccountActivated` (when email is verified)\n- `AccountSuspended` (when account is temporarily disabled)\n- `AccountClosed` (when account is permanently closed)\n- `TrialMembershipGranted` (when free trial begins)\n- `PremiumMembershipUpgraded` (when user pays for premium)\n- `MembershipExpired` (when subscription ends)\n\n**Why this matters:**\n- Business events capture **intent and meaning**, not just state changes\n- They reflect the **ubiquitous language** of the domain\n- They enable **better event sourcing** by preserving business context\n- They make **event streams readable** as a business narrative\n- They support **better analytics** and business intelligence\n- They enable **temporal queries** that make business sense\n\n**Example of business context preservation:**\n```elixir\n#  BAD - Generic CRUD event\ndefmodule ReckonAccounts.AccountUpdated.Event do\n  defstruct [:account_id, :changes, :updated_at]\nend\n\n#  GOOD - Meaningful business events\ndefmodule ReckonAccounts.AccountActivated.Event do\n  defstruct [:account_id, :email, :activated_at, :verification_token]\nend\n\ndefmodule ReckonAccounts.AccountSuspended.Event do\n  defstruct [:account_id, :reason, :suspended_at, :suspended_by]\nend\n\ndefmodule ReckonAccounts.AccountClosed.Event do\n  defstruct [:account_id, :reason, :closed_at, :requested_by]\nend\n```\n\nRemember: The codebase should **scream** what it does, not how it's implemented!","title":"CRUD Events vs Business Events - Implementation Guidelines","ref":"implementation-guidelines.html#crud-events-vs-business-events"},{"type":"extras","doc":"# ExESDB.Debugger\n\nA comprehensive debugging and inspection tool for ExESDB Event Sourcing Database systems.","title":"Debugging and Troubleshooting","ref":"debugging.html"},{"type":"extras","doc":"The ExESDB Debugger provides REPL-friendly functions to investigate all aspects of your ExESDB system, including:\n\n-  **Process Supervision Tree Inspection**\n-  **Configuration Analysis**\n-  **Stream and Event Investigation**  \n-  **Performance Metrics**\n-  **Health Monitoring**\n-  **Emitter Pool Management**\n-  **Function Tracing**\n-  **Benchmarking Tools**","title":"Overview - Debugging and Troubleshooting","ref":"debugging.html#overview"},{"type":"extras","doc":"```elixir\n# In your IEx session\niex> ExESDB.Debugger.overview()\niex> ExESDB.Debugger.help()\n```","title":"Quick Start - Debugging and Troubleshooting","ref":"debugging.html#quick-start"},{"type":"extras","doc":"","title":"Core Functions - Debugging and Troubleshooting","ref":"debugging.html#core-functions"},{"type":"extras","doc":"```elixir\n# Get a complete system overview\nExESDB.Debugger.overview()\nExESDB.Debugger.overview(:my_store)\n\n# Show help with all available commands\nExESDB.Debugger.help()\n```","title":"System Overview - Debugging and Troubleshooting","ref":"debugging.html#system-overview"},{"type":"extras","doc":"```elixir\n# List all ExESDB processes\nExESDB.Debugger.processes()\n\n# Display supervision tree\nExESDB.Debugger.supervision_tree()\n\n# Show emitter pools and workers\nExESDB.Debugger.emitters()\n```","title":"Process Management - Debugging and Troubleshooting","ref":"debugging.html#process-management"},{"type":"extras","doc":"```elixir\n# Show detailed configuration\nExESDB.Debugger.config()\n\n# Configuration shows sources (app config vs environment variables)\n```","title":"Configuration - Debugging and Troubleshooting","ref":"debugging.html#configuration"},{"type":"extras","doc":"```elixir\n# List all streams\nExESDB.Debugger.streams()\n\n# Show events in a specific stream\nExESDB.Debugger.events(\"user-123\", limit: 10)\nExESDB.Debugger.events(\"orders\", start_version: 50, direction: :backward)\n\n# List active subscriptions\nExESDB.Debugger.subscriptions()\n```","title":"Data Investigation - Debugging and Troubleshooting","ref":"debugging.html#data-investigation"},{"type":"extras","doc":"```elixir\n# Comprehensive health check\nExESDB.Debugger.health()\n\n# Performance metrics\nExESDB.Debugger.performance()\n\n# Show top processes by memory/CPU\nExESDB.Debugger.top()\nExESDB.Debugger.top(limit: 5, sort_by: :reductions)\n```","title":"Health & Performance - Debugging and Troubleshooting","ref":"debugging.html#health-performance"},{"type":"extras","doc":"```elixir\n# Start Erlang Observer GUI\nExESDB.Debugger.observer()\n\n# Trace function calls\nExESDB.Debugger.trace(ExESDB.StreamsWriter, :append_events, duration: 5000)\n\n# Benchmark functions\nExESDB.Debugger.benchmark(fn -> expensive_operation() end, times: 100)\n```","title":"Debugging Tools - Debugging and Troubleshooting","ref":"debugging.html#debugging-tools"},{"type":"extras","doc":"All functions work with multiple stores:\n\n```elixir\n# Auto-discover the store (works with single stores)\nExESDB.Debugger.overview()\n\n# Specify a particular store\nExESDB.Debugger.overview(:orders_store)\nExESDB.Debugger.processes(:inventory_store)\nExESDB.Debugger.health(:users_store)\n```","title":"Multi-Store Support - Debugging and Troubleshooting","ref":"debugging.html#multi-store-support"},{"type":"extras","doc":"","title":"Example Output - Debugging and Troubleshooting","ref":"debugging.html#example-output"},{"type":"extras","doc":"```\n ExESDB System Overview\n========================================\n Store: :my_store\n System:  Running\n PID: #PID<0.1234.0>\n Node: :node@localhost\n System healthy\n  Processes: 12/12 alive\n  Config: cluster mode, data: /tmp/data\n\n Use ExESDB.Debugger.help() for available commands\n```","title":"System Overview - Debugging and Troubleshooting","ref":"debugging.html#system-overview-1"},{"type":"extras","doc":"```\n ExESDB Health Check for :my_store\n==================================================\n System Process       : OK\n Configuration        : OK\n Gateway Workers      : 2 gateway worker(s) running\n Store Accessibility  : OK\n Memory Usage        : High memory usage: 156.7 MB\n Process Supervision  : OK\n\n Summary: 6 checks, 0 errors, 1 warnings\n```","title":"Health Check - Debugging and Troubleshooting","ref":"debugging.html#health-check"},{"type":"extras","doc":"```\n  ExESDB Processes for :my_store\n==================================================\n\n System (1 processes)\n   exesdb_system_my_store         #PID<0.1234.0> 2.1MB msgs:0\n\n Gateway (2 processes)  \n   gateway_worker_my_store_1      #PID<0.1235.0> 1.2MB msgs:0\n   gateway_worker_my_store_2      #PID<0.1236.0> 1.1MB msgs:0\n\n Emitter (3 processes)\n   emitter_pool_subscription_1    #PID<0.1237.0> 512KB msgs:0\n\n Total Memory: 4.8MB\n```","title":"Process Listing - Debugging and Troubleshooting","ref":"debugging.html#process-listing"},{"type":"extras","doc":"The health check performs the following validations:\n\n-  **System Process**: Main supervisor is running\n-  **Configuration**: All config values are accessible\n-  **Gateway Workers**: At least one gateway worker is available\n-  **Store Accessibility**: Can communicate with the store\n-  **Memory Usage**: Monitors total memory consumption\n-  **Process Supervision**: All supervised processes are alive","title":"Health Checks - Debugging and Troubleshooting","ref":"debugging.html#health-checks"},{"type":"extras","doc":"```elixir\nExESDB.Debugger.performance()\n```\n\nShows:\n- System memory and CPU info\n- Total ExESDB process count and memory usage\n- Top memory-consuming processes\n- System uptime","title":"Performance Monitoring - Debugging and Troubleshooting","ref":"debugging.html#performance-monitoring"},{"type":"extras","doc":"","title":"Tracing and Debugging - Debugging and Troubleshooting","ref":"debugging.html#tracing-and-debugging"},{"type":"extras","doc":"```elixir\n# Trace all calls to a function for 5 seconds\nExESDB.Debugger.trace(ExESDB.StreamsWriter, :append_events)\n\n# Custom tracing duration\nExESDB.Debugger.trace(MyModule, :my_function, duration: 10_000)\n```","title":"Function Tracing - Debugging and Troubleshooting","ref":"debugging.html#function-tracing"},{"type":"extras","doc":"```elixir\n# Benchmark a function\nExESDB.Debugger.benchmark(fn -> \n  ExESDB.StreamsWriter.append_events(:my_store, \"test\", [%{type: \"test\"}])\nend, times: 100)\n```","title":"Benchmarking - Debugging and Troubleshooting","ref":"debugging.html#benchmarking"},{"type":"extras","doc":"```elixir\n# Opens the Erlang Observer for real-time monitoring\nExESDB.Debugger.observer()\n```","title":"Observer GUI - Debugging and Troubleshooting","ref":"debugging.html#observer-gui"},{"type":"extras","doc":"The debugger uses several built-in and external libraries:\n\n- **:recon** - Process inspection and system information\n- **:observer** - Erlang Observer GUI (included in OTP)\n- **:dbg** - Function tracing (included in OTP)\n- **:sys** - System process inspection (included in OTP)","title":"Dependencies - Debugging and Troubleshooting","ref":"debugging.html#dependencies"},{"type":"extras","doc":"","title":"Troubleshooting - Debugging and Troubleshooting","ref":"debugging.html#troubleshooting"},{"type":"extras","doc":"1. **No processes found**\n   ```elixir\n   # Make sure ExESDB is running\n   ExESDB.Debugger.health()  # Will show what's missing\n   ```\n\n2. **Gateway worker not available**\n   ```elixir\n   # Check if the system is properly started\n   ExESDB.Debugger.supervision_tree()\n   ```\n\n3. **Memory warnings**\n   ```elixir\n   # Investigate top memory consumers\n   ExESDB.Debugger.top(sort_by: :memory)\n   ExESDB.Debugger.observer()  # For detailed analysis\n   ```","title":"Common Issues - Debugging and Troubleshooting","ref":"debugging.html#common-issues"},{"type":"extras","doc":"- Use `help()` to see all available commands\n- All functions work without store_id (auto-discovery)\n- Health checks will identify most common issues\n- Observer GUI provides real-time monitoring\n- Tracing is helpful for performance debugging\n- Performance monitoring shows system resource usage","title":"Tips - Debugging and Troubleshooting","ref":"debugging.html#tips"},{"type":"extras","doc":"The debugger is designed to be used in development and production REPL sessions. It's safe to use in production as all operations are read-only by default.\n\nAdd it to your application by ensuring ExESDB is in your dependencies, then use it directly in IEx:\n\n```elixir\n# In your IEx session after starting your app\niex> ExESDB.Debugger.overview()\n```","title":"Integration - Debugging and Troubleshooting","ref":"debugging.html#integration"},{"type":"extras","doc":"","title":"Examples - Debugging and Troubleshooting","ref":"debugging.html#examples"},{"type":"extras","doc":"```elixir\n# 1. Start your app\niex -S mix\n\n# 2. Check system health\nExESDB.Debugger.health()\n\n# 3. Look at your data\nExESDB.Debugger.streams()\nExESDB.Debugger.events(\"user-123\", limit: 5)\n\n# 4. Monitor performance\nExESDB.Debugger.performance()\nExESDB.Debugger.top()\n\n# 5. Debug issues\nExESDB.Debugger.trace(MyModule, :problematic_function)\n```","title":"Development Workflow - Debugging and Troubleshooting","ref":"debugging.html#development-workflow"},{"type":"extras","doc":"```elixir\n# Quick health check\nExESDB.Debugger.health()\n\n# Check memory usage\nExESDB.Debugger.performance()\n\n# Investigate specific issues\nExESDB.Debugger.processes()\nExESDB.Debugger.supervision_tree()\n```\n\nThe ExESDB Debugger makes it easy to understand, monitor, and debug your Event Sourcing system!","title":"Production Debugging - Debugging and Troubleshooting","ref":"debugging.html#production-debugging"},{"type":"extras","doc":"# ExESDB PubSub Architecture: Foundation for Event-Driven Architecture","title":"PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html"},{"type":"extras","doc":"ExESDB has been significantly enhanced with a comprehensive PubSub (Publish-Subscribe) messaging system that serves as the foundation for transitioning to a fully Event-Driven Architecture (EDA). This architecture provides dedicated communication channels for different system concerns, enabling better separation of responsibilities, enhanced observability, and improved system reliability.","title":"Overview - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#overview"},{"type":"extras","doc":"","title":"Architecture Components - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#architecture-components"},{"type":"extras","doc":"ExESDB now utilizes multiple specialized Phoenix.PubSub instances, each optimized for specific types of communication:\n\n#### 1. `:ex_esdb_events` - Event Distribution\n- **Purpose**: Primary channel for business event distribution\n- **Usage**: Event streaming, subscription delivery, event broadcasting\n- **Subscribers**: EmitterWorkers, event subscribers, external systems\n- **Topics**: Stream-specific topics (e.g., `\"store:stream_name\"`)\n\n#### 2. `:ex_esdb_system` - System Operations\n- **Purpose**: Internal system operations and coordination\n- **Usage**: System state changes, coordination messages, metrics\n- **Subscribers**: System components, metrics collectors\n- **Topics**: System-wide operational topics\n\n#### 3. `:ex_esdb_health` - Health Monitoring\n- **Purpose**: Dedicated health and monitoring communications\n- **Usage**: Health status updates, service availability, circuit breaker events\n- **Subscribers**: Health trackers, monitoring systems, EmitterWorkers\n- **Topics**: Health-specific topics (e.g., `\"store_health:store_id\"`, `\"health_summary:store_id\"`)","title":"Dedicated PubSub Instances - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#dedicated-pubsub-instances"},{"type":"extras","doc":"","title":"Enhanced EmitterWorker System - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#enhanced-emitterworker-system"},{"type":"extras","doc":"The EmitterWorker system now provides comprehensive, color-coded logging for different message types:\n\n#### Message Type Color Coding\n- ** Success Messages (White on Green/Blue)**: Service activation, health subscriptions, successful operations\n- ** Failure Messages (White on Red)**: Termination events, errors, unhealthy states  \n- ** Action Messages (White on Amber)**: Broadcasting, forwarding, dynamic worker creation, metrics\n- ** Health Messages (White on Cyan)**: Health event processing, status changes","title":"Color-Coded Observability - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#color-coded-observability"},{"type":"extras","doc":"#### Health Event Monitoring\n```elixir\n# Health event subscription\n SUBSCRIBED to health events for store: my_store\n\n# Individual health events\n HEALTH EVENT: subscription_name -> event_type (metadata)\n\n# Health summaries\n HEALTH SUMMARY: Store my_store - 5/7 healthy subscriptions\n\n# Health impact on emission\n HEALTH IMPACT: subscription_name is HEALTHY (registration_success)\n```\n\n#### Metrics Event Monitoring\n```elixir\n# Metrics event subscription\n SUBSCRIBED to metrics events for store: my_store\n\n# Individual metrics events\n METRICS EVENT: my_store -> events_per_second=1250 @2025-07-27T11:30:00Z\n\n# Metrics summaries\n METRICS SUMMARY: Store my_store - 1250 eps, 50000 total, 12 active subs\n```\n\n#### Lifecycle Events\n```elixir\n# Worker activation with complete information\n\n EMITTER WORKER ACTIVATION \n\nTopic:      \"my_store:stream_name\"\nStore:      my_store\nScheduler:  2\nPID:        #PID<0.511.0>\nSubscriber: #PID<0.312.0>\n\n\n# Worker termination with subscriber information\n\n EMITTER WORKER TERMINATION \n\nReason:     :shutdown\nStore:      my_store\nSelector:   stream_name\nSubscriber: #PID<0.312.0>\nPID:        #PID<0.511.0>\n\n```","title":"Comprehensive Event Logging - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#comprehensive-event-logging"},{"type":"extras","doc":"","title":"Event-Driven Architecture Benefits - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#event-driven-architecture-benefits"},{"type":"extras","doc":"- **Dedicated channels** prevent cross-contamination of different message types\n- **Specialized handling** for events, system operations, and health monitoring\n- **Clean boundaries** between business logic and operational concerns","title":"1. Separation of Concerns - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#1-separation-of-concerns"},{"type":"extras","doc":"- **Real-time visibility** into system health and performance\n- **Color-coded logging** for immediate visual identification of issues\n- **Comprehensive metrics** collection and reporting\n- **Detailed lifecycle tracking** for all system components","title":"2. Enhanced Observability - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#2-enhanced-observability"},{"type":"extras","doc":"- **Health-aware emission**: EmitterWorkers can pause/resume based on subscription health\n- **Circuit breaker integration**: Automatic handling of degraded services\n- **Graceful degradation**: System continues operating during partial failures","title":"3. Improved Reliability - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#3-improved-reliability"},{"type":"extras","doc":"- **Asynchronous messaging**: Non-blocking communication between components\n- **Efficient broadcasting**: Dedicated channels reduce message routing overhead\n- **Batch processing**: Health and metrics events can be batched for efficiency","title":"4. Better Performance - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#4-better-performance"},{"type":"extras","doc":"","title":"Implementation Details - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#implementation-details"},{"type":"extras","doc":"The `ExESDB.SubscriptionHealthTracker` has been enhanced to use the dedicated `:ex_esdb_health` PubSub instance:\n\n```elixir\n# Subscribe to health events\nPhoenix.PubSub.subscribe(:ex_esdb_health, \"store_health:#{store_id}\")\n\n# Broadcast health events  \nPhoenix.PubSub.broadcast(:ex_esdb_health, \"store_health:#{store_id}\", \n  {:subscription_health, health_event})\n\n# Broadcast health summaries\nPhoenix.PubSub.broadcast(:ex_esdb_health, \"health_summary:#{store_id}\", \n  {:health_summary, summary_data})\n```","title":"Subscription Health Tracking - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#subscription-health-tracking"},{"type":"extras","doc":"EmitterWorkers now subscribe to both health and metrics events:\n\n```elixir\ndef init({store, sub_topic, subscriber}) do\n  # Subscribe to health events\n  Phoenix.PubSub.subscribe(:ex_esdb_health, \"store_health:#{store}\")\n  Phoenix.PubSub.subscribe(:ex_esdb_health, \"health_summary:#{store}\")\n  \n  # Subscribe to metrics events\n  Phoenix.PubSub.subscribe(:ex_esdb_system, \"store_metrics:#{store}\")\n  Phoenix.PubSub.subscribe(:ex_esdb_system, \"metrics_summary:#{store}\")\n  \n  # ... rest of initialization\nend\n```","title":"EmitterWorker Health Integration - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#emitterworker-health-integration"},{"type":"extras","doc":"EmitterWorkers can dynamically adjust their behavior based on health status:\n\n```elixir\ndefp update_emission_state(healthy) do\n  Process.put(:emitter_active, healthy)\n  \n  if healthy do\n    Logger.debug(\"Emission RESUMED due to healthy status\")\n  else\n    Logger.warning(\"Emission PAUSED due to unhealthy status\")\n  end\nend\n```","title":"Health-Aware Emission Control - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#health-aware-emission-control"},{"type":"extras","doc":"","title":"Configuration - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#configuration"},{"type":"extras","doc":"The PubSub instances are automatically configured as part of the ExESDB system:\n\n```elixir\n# In your application's supervision tree\nchildren = [\n  {Phoenix.PubSub, name: :ex_esdb_events},\n  {Phoenix.PubSub, name: :ex_esdb_system}, \n  {Phoenix.PubSub, name: :ex_esdb_health},\n  # ... other children\n]\n```","title":"PubSub Instance Configuration - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#pubsub-instance-configuration"},{"type":"extras","doc":"Health tracking can be configured per store:\n\n```elixir\nconfig :ex_esdb, :health_monitoring,\n  enabled: true,\n  check_interval: 5_000,\n  unhealthy_threshold: 3,\n  circuit_breaker_enabled: true\n```","title":"Health Monitoring Configuration - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#health-monitoring-configuration"},{"type":"extras","doc":"This PubSub architecture serves as the foundation for migrating ExESDB to a fully Event-Driven Architecture:","title":"Migration Path to Full EDA - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#migration-path-to-full-eda"},{"type":"extras","doc":"-  Dedicated PubSub instances\n-  Health event distribution\n-  Metrics event distribution\n-  Enhanced observability","title":"Phase 1: Internal Events (Current) - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#phase-1-internal-events-current"},{"type":"extras","doc":"- [ ] Business domain event modeling\n- [ ] Event sourcing patterns\n- [ ] Saga orchestration\n- [ ] Event replay capabilities","title":"Phase 2: Domain Events (Next) - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#phase-2-domain-events-next"},{"type":"extras","doc":"- [ ] External system notifications\n- [ ] Webhook delivery\n- [ ] Message queue integration\n- [ ] Event streaming to external systems","title":"Phase 3: External Integration (Future) - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#phase-3-external-integration-future"},{"type":"extras","doc":"","title":"Best Practices - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#best-practices"},{"type":"extras","doc":"- Use consistent prefixes: `\"store_health:\"`, `\"store_metrics:\"`, `\"stream:\"`\n- Include store ID for multi-tenant deployments\n- Use descriptive, hierarchical names","title":"1. Topic Naming Conventions - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#1-topic-naming-conventions"},{"type":"extras","doc":"- Include timestamp and correlation IDs\n- Use structured data (maps) for complex events\n- Maintain backward compatibility in message formats","title":"2. Message Structure - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#2-message-structure"},{"type":"extras","doc":"- Implement proper error handling in all subscribers\n- Use circuit breakers for external integrations\n- Log errors with appropriate context","title":"3. Error Handling - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#3-error-handling"},{"type":"extras","doc":"- Batch messages when possible\n- Use async processing for non-critical events\n- Monitor PubSub performance and tune accordingly","title":"4. Performance Considerations - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#4-performance-considerations"},{"type":"extras","doc":"","title":"Monitoring and Debugging - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#monitoring-and-debugging"},{"type":"extras","doc":"Monitor system health through dedicated health events:\n- Subscription health status\n- Circuit breaker states\n- Service availability metrics","title":"Health Dashboard - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#health-dashboard"},{"type":"extras","doc":"Track system performance through metrics events:\n- Events per second\n- Processing latency\n- Active subscription counts\n- Memory and CPU usage","title":"Performance Metrics - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#performance-metrics"},{"type":"extras","doc":"Use the enhanced logging for debugging:\n- Color-coded message identification\n- Detailed lifecycle tracking\n- Health event correlation\n- Performance bottleneck identification","title":"Debugging Tools - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#debugging-tools"},{"type":"extras","doc":"The enhanced PubSub architecture transforms ExESDB into a highly observable, resilient, and scalable event store system. By providing dedicated communication channels for different concerns and comprehensive observability features, this architecture serves as a solid foundation for evolving toward a fully Event-Driven Architecture while maintaining operational excellence and system reliability.\n\nThe color-coded logging, health-aware emission control, and comprehensive metrics collection provide unprecedented visibility into system operations, making it easier to develop, debug, and operate ExESDB-based applications in production environments.","title":"Conclusion - PubSub Architecture and Event-Driven Design","ref":"pubsub-architecture.html#conclusion"},{"type":"extras","doc":"# ExESDB - A BEAM-native Event Store\n\n`ExESDB` is a BEAM-native Event Store, built on top of the [khepri](https://github.com/rabbitmq/khepri) and [ra](https://github.com/rabbitmq/ra) subsystems.","title":"Read Me","ref":"readme.html"},{"type":"extras","doc":"One of the arguments for BEAM development is that it comes \"batteries included\". Be it caching, storage, pub/sub, observability etc... the Erlang ecosystem always has the option to avoid external dependencies.\n\nFor Event Sourcing use cases however, the Event Store is often a separate service.\n\nThis project is an attempt at addressing this point, by building further upon the work of the `rabbitmq/khepri` and `rabbitmq/ra` subsystems.","title":"Motivation - Read Me","ref":"readme.html#motivation"},{"type":"extras","doc":"ExESDB is a distributed, BEAM-native Event Store that provides high-availability event sourcing capabilities with automatic cluster formation and coordination. Built on top of Khepri and Ra (Raft consensus), it offers enterprise-grade reliability and performance.","title":"Features - Read Me","ref":"readme.html#features"},{"type":"extras","doc":"#### Event Stream Management\n- **Stream Creation**: Automatic stream creation on first event append\n- **Event Appending**: Atomic append operations with optimistic concurrency control\n- **Event Retrieval**: Query events with forward/backward traversal support\n- **Stream Versioning**: Track stream versions for conflict detection and resolution\n- **Stream Listing**: Enumerate all streams in the store\n\n#### Event Storage\n- **Persistent Storage**: Durable event storage using Khepri's distributed key-value store\n- **ACID Compliance**: Atomic, consistent, isolated, and durable operations\n- **Conflict Resolution**: Built-in optimistic concurrency control\n- **Data Integrity**: Checksum validation and corruption detection\n\n#### Subscription System\n- **Multiple Subscription Types**:\n  - `:by_stream` - Subscribe to specific event streams\n  - `:by_event_type` - Subscribe to events by type classification\n  - `:by_event_pattern` - Pattern-based event matching\n  - `:by_event_payload` - Content-based subscription filtering\n- **Persistent Subscriptions**: Durable subscriptions that survive node restarts\n- **Transient Subscriptions**: Temporary subscriptions for real-time processing\n- **Event Replay**: Start subscriptions from any stream version\n- **Acknowledgment System**: Reliable event delivery with ACK/NACK support\n- **\"Follow-the-Leader\"**: Subscription processes automatically migrate to cluster leader\n\n#### Snapshot Management\n- **Aggregate Snapshots**: Store and retrieve aggregate state snapshots\n- **Version-based Snapshots**: Snapshots tied to specific stream versions\n- **Snapshot Lifecycle**: Create, read, update, and delete snapshot operations\n- **Performance Optimization**: Reduce replay time for large aggregates\n- **Distributed Storage**: Snapshots stored across the cluster for availability","title":"Core Event Store Functionality - Read Me","ref":"readme.html#core-event-store-functionality"},{"type":"extras","doc":"#### LibCluster Integration\nExESDB uses LibCluster for automatic cluster discovery and formation:\n\n- **Strategy**: Gossip-based multicast discovery\n- **Protocol**: UDP multicast on configurable port (default: 45892)\n- **Network**: Automatic node discovery on shared networks\n- **Security**: Shared secret authentication for cluster membership\n- **Broadcast Discovery**: Configurable multicast addressing\n\n#### Cluster Formation Process\n1. **Initialization**: Node starts and initializes LibCluster topology\n2. **Discovery**: Uses gossip multicast to discover peer nodes\n3. **Authentication**: Validates cluster membership using shared secrets\n4. **Coordination**: ClusterCoordinator manages join/leave operations\n5. **Consensus**: Khepri cluster formation using Raft consensus\n6. **Monitoring**: Continuous health monitoring and leader election\n\n#### High Availability Features\n- **Automatic Clustering**: Nodes automatically discover and join clusters\n- **Split-Brain Prevention**: ClusterCoordinator prevents network partition issues\n- **Leader Election**: Automatic leader election using Raft consensus\n- **Failover**: Seamless handling of node failures\n- **Data Replication**: Events replicated across cluster nodes\n- **Consensus Protocol**: Ra/Raft ensures data consistency","title":"Distributed Architecture & Clustering - Read Me","ref":"readme.html#distributed-architecture-clustering"},{"type":"extras","doc":"#### Khepri Integration\n- **Distributed Tree Store**: Hierarchical key-value storage\n- **MVCC**: Multi-version concurrency control\n- **Transactions**: ACID transaction support\n- **Schema Evolution**: Support for data structure changes\n- **Triggers**: Event-driven data processing\n\n#### Ra (Raft) Consensus\n- **Strong Consistency**: Linearizable read/write operations\n- **Partition Tolerance**: Operates correctly during network partitions\n- **Leader-based Replication**: Single leader for write operations\n- **Log Compaction**: Automatic cleanup of old log entries\n- **Snapshot Support**: Efficient state transfer for new nodes","title":"Storage Engine - Read Me","ref":"readme.html#storage-engine"},{"type":"extras","doc":"#### Environment Configuration\n- `EX_ESDB_STORE_ID`: Unique identifier for the store instance\n- `EX_ESDB_DB_TYPE`: Deployment type (`:single` or `:cluster`)\n- `EX_ESDB_DATA_DIR`: Data directory for persistent storage\n- `EX_ESDB_TIMEOUT`: Operation timeout configuration\n- `EX_ESDB_CLUSTER_SECRET`: Shared secret for cluster authentication\n- `EX_ESDB_COOKIE`: Erlang distribution cookie\n- `EX_ESDB_PUB_SUB`: PubSub configuration for event broadcasting\n\n#### LibCluster Configuration\n```elixir\nconfig :libcluster,\n  topologies: [\n    ex_esdb_cluster: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45_892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"255.255.255.255\",\n        broadcast_only: true,\n        secret: System.get_env(\"EX_ESDB_CLUSTER_SECRET\")\n      ]\n    ]\n  ]\n```","title":"Configuration & Deployment - Read Me","ref":"readme.html#configuration-deployment"},{"type":"extras","doc":"#### High-Availability Proxy\n- **Load Balancing**: Distribute requests across gateway workers\n- **Service Discovery**: Automatic discovery of available gateway workers\n- **Fault Tolerance**: Handle worker failures gracefully\n- **Request Routing**: Smart routing based on operation type\n\n#### Worker Distribution\n- **Swarm Integration**: Distributed worker management\n- **Process Migration**: Workers can move between cluster nodes\n- **Resource Management**: Efficient resource utilization across cluster\n- **Monitoring**: Real-time worker health and performance tracking","title":"Gateway API Integration - Read Me","ref":"readme.html#gateway-api-integration"},{"type":"extras","doc":"#### Monitoring & Observability\n- **Cluster Status**: Real-time cluster membership and health\n- **Leader Tracking**: Monitor current cluster leader\n- **Performance Metrics**: Operation latency and throughput\n- **Error Tracking**: Comprehensive error logging and reporting\n- **Health Checks**: Built-in health check endpoints\n\n#### Development Tools\n- **Cluster Manager**: Interactive cluster management script\n- **Docker Compose**: Multi-node development environment\n- **Chaos Engineering**: Built-in chaos testing capabilities\n- **Validation Scripts**: Automated cluster validation tools","title":"Operational Features - Read Me","ref":"readme.html#operational-features"},{"type":"extras","doc":"```\n        \n   ExESDB Node          ExESDB Node          ExESDB Node   \n    (Leader)        (Follower)       (Follower)    \n   Khepri + Ra          Khepri + Ra          Khepri + Ra   \n        \n                                                       \n                                                       \n              Gossip Multicast Network (UDP:45892)     \n                                                       \n                                                       \n    Raft Consensus           Event Storage           Subscription\n     & Replication          & Retrieval             Management\n```","title":"Network Topology - Read Me","ref":"readme.html#network-topology"},{"type":"extras","doc":"#### Single Node Deployment\n- **Development**: Local development and testing\n- **Small Applications**: Simple event sourcing needs\n- **Embedded Usage**: Integration within existing applications\n\n#### Multi-Node Cluster\n- **Production**: High-availability production deployments\n- **Horizontal Scaling**: Scale read/write capacity\n- **Geographic Distribution**: Multi-region deployments\n- **Fault Tolerance**: Survive individual node failures\n\n#### Container Orchestration\n- **Docker Compose**: Development and testing environments\n- **Kubernetes**: Production container orchestration\n- **Docker Swarm**: Simplified container clustering\n- **Health Checks**: Container-level health monitoring","title":"Deployment Scenarios - Read Me","ref":"readme.html#deployment-scenarios"},{"type":"extras","doc":"#### Throughput\n- **Write Performance**: Optimized for high-volume event appending\n- **Read Performance**: Efficient event retrieval and streaming\n- **Concurrent Operations**: Handle multiple simultaneous operations\n- **Batch Processing**: Support for batch event operations\n\n#### Scalability\n- **Horizontal Scaling**: Add nodes to increase capacity\n- **Storage Scalability**: Distributed storage across cluster\n- **Subscription Scaling**: Distribute subscription load\n- **Resource Utilization**: Efficient use of available resources","title":"Performance Characteristics - Read Me","ref":"readme.html#performance-characteristics"},{"type":"extras","doc":"#### BEAM Ecosystem\n- **Phoenix Integration**: Real-time web applications\n- **LiveView Support**: Real-time UI updates\n- **GenServer Integration**: Native BEAM process integration\n- **OTP Supervision**: Fault-tolerant supervision trees\n\n#### External Systems\n- **REST APIs**: HTTP-based integration\n- **Message Queues**: Integration with external queuing systems\n- **Databases**: Projection and read model support\n- **Monitoring Systems**: Metrics and alerting integration","title":"Integration Capabilities - Read Me","ref":"readme.html#integration-capabilities"},{"type":"extras","doc":"","title":"Installation - Read Me","ref":"readme.html#installation"},{"type":"extras","doc":"ExESDB is available as a Docker image on Docker Hub with automatic versioning based on the `mix.exs` version.\n\n#### Available Tags\n- `beamcampus/ex_esdb:latest` - Latest build from master branch\n- `beamcampus/ex_esdb:0.0.18` - Specific version (current version)\n- `beamcampus/ex_esdb:0.0.x` - Any specific version tag\n\n#### Quick Start\n\n**Single Node:**\n```bash\ndocker run -d \\\n  --name ex-esdb \\\n  -p 4369:4369 \\\n  -p 9000-9100:9000-9100 \\\n  -p 45892:45892/udp \\\n  -e EX_ESDB_STORE_ID=\"my-store\" \\\n  -e EX_ESDB_DB_TYPE=\"single\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -v ex-esdb-data:/data \\\n  beamcampus/ex_esdb:latest\n```\n\n**Multi-Node Cluster:**\n```bash\n# Node 1 (seed node)\ndocker run -d \\\n  --name ex-esdb-node1 \\\n  --network ex-esdb-net \\\n  -p 4369:4369 \\\n  -p 9001:9000 \\\n  -p 45892:45892/udp \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node1-data:/data \\\n  beamcampus/ex_esdb:latest\n\n# Node 2\ndocker run -d \\\n  --name ex-esdb-node2 \\\n  --network ex-esdb-net \\\n  -p 9002:9000 \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node2-data:/data \\\n  beamcampus/ex_esdb:latest\n\n# Node 3\ndocker run -d \\\n  --name ex-esdb-node3 \\\n  --network ex-esdb-net \\\n  -p 9003:9000 \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node3-data:/data \\\n  beamcampus/ex_esdb:latest\n```\n\n#### Docker Compose\n\nFor development and testing, use the provided Docker Compose setup:\n\n```bash\n# Clone the repository\ngit clone https://github.com/beam-campus/ex-esdb.git\ncd ex-esdb/dev-env\n\n# Start a 3-node cluster\n./start-core-only.sh\n\n# Or use the interactive cluster manager\n./ez-cluster.sh\n```\n\nThe Docker Compose setup includes:\n- **Core Cluster**: 3-node ExESDB cluster (ex-esdb0, ex-esdb1, ex-esdb2)\n- **Extended Tier**: Additional 2 nodes (ex-esdb10, ex-esdb11)\n- **Massive Tier**: Additional 8 nodes (ex-esdb20-27)\n- **Automatic Networking**: Configured Docker networks for cluster communication\n- **Data Persistence**: Named volumes for data persistence\n- **Health Checks**: Built-in container health monitoring\n\n#### Environment Variables\n\n> ** Important**: When using the dev-env Docker Compose configurations, you must export the `EX_ESDB_COOKIE` environment variable on your host machine. This single environment variable is used for all cluster authentication purposes (cookies, secrets, etc.).\n> \n> ```bash\n> export EX_ESDB_COOKIE=\"your-secure-cookie-value\"\n> ```\n\n| Variable | Description | Default | Required |\n|----------|-------------|---------|----------|\n| `EX_ESDB_STORE_ID` | Unique store identifier | - | Yes |\n| `EX_ESDB_DB_TYPE` | Deployment type (`single` or `cluster`) | `single` | No |\n| `EX_ESDB_DATA_DIR` | Data directory path | `/data` | No |\n| `EX_ESDB_TIMEOUT` | Operation timeout (ms) | `5000` | No |\n| `EX_ESDB_CLUSTER_SECRET` | Cluster authentication secret | - | Yes (cluster) |\n| `EX_ESDB_COOKIE` | Erlang distribution cookie | - | Yes (cluster) |\n| `EX_ESDB_PUB_SUB` | PubSub process name | `:ex_esdb_pubsub` | No |\n\n#### Ports\n\n| Port | Protocol | Description |\n|------|----------|-------------|\n| `4369` | TCP | EPMD (Erlang Port Mapper Daemon) |\n| `9000-9100` | TCP | Erlang distribution ports |\n| `45892` | UDP | LibCluster gossip multicast |\n\n#### Health Checks\n\nThe Docker image includes a built-in health check script:\n\n```bash\n# Check container health\ndocker exec ex-esdb ./check-ex-esdb.sh\n\n# View health status\ndocker inspect --format='{{.State.Health.Status}}' ex-esdb\n```\n\n#### Production Considerations\n\n1. **Security**: Use strong, unique values for `EX_ESDB_CLUSTER_SECRET` and `EX_ESDB_COOKIE`\n2. **Networking**: Ensure proper firewall rules for cluster communication\n3. **Storage**: Use named volumes or bind mounts for data persistence\n4. **Monitoring**: Implement external monitoring for cluster health\n5. **Backups**: Regular backup of data volumes\n6. **Resource Limits**: Set appropriate CPU and memory limits","title":"Docker Installation - Read Me","ref":"readme.html#docker-installation"},{"type":"extras","doc":"ExESDB is also available as a Hex package for direct integration:\n\n```elixir\ndef deps do\n  [\n    {:ex_esdb, \"~> 0.0.18\"}\n  ]\nend\n```","title":"Hex Installation - Read Me","ref":"readme.html#hex-installation"},{"type":"extras","doc":"- [Getting Started](system/guides/getting_started.md)","title":"Contents - Read Me","ref":"readme.html#contents"},{"type":"extras","doc":"- [On Hex](https://hex.pm/packages/ex_esdb)\n- [Release Documentation](https://hexdocs.pm/ex_esdb/index.html)","title":"Releases - Read Me","ref":"readme.html#releases"}],"proglang":"elixir","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.38.2"}}