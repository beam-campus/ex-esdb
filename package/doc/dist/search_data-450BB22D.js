searchData={"items":[{"type":"module","title":"ExESDB","doc":"ExESDB is a wrapper around the khepri library. \n  Its intention is to provide an interface to khepri,\n  with a focus on event sourcing.","ref":"ExESDB.html"},{"type":"module","title":"ExESDB.Aggregator","doc":"Aggregates events from an event stream using tagged rules:\n  GIVEN: an Event of roughly this format:\n   %{\n    event_id: \"1234567890\",\n    event_type: \"user.birthday_celebrated:v1\",\n    stream_id: \"celebrate-user-birthday-john\",\n    version: 1,\n    data: %{\n      name: \"John\",\n      age: {:sum, 1},\n      venue: {:overwrite, \"New York\"}\n    },\n    timestamp: ~U[2022-01-01 12:00:00Z],\n    epoch: 1641013200,\n    metadata: %{\n      source_id: \"1234567890\"\n    }\n  }","ref":"ExESDB.Aggregator.html"},{"type":"function","title":"ExESDB.Aggregator.finalize_map/1","doc":"","ref":"ExESDB.Aggregator.html#finalize_map/1"},{"type":"function","title":"ExESDB.Aggregator.foldl/2","doc":"Folds a list of events into a single map.","ref":"ExESDB.Aggregator.html#foldl/2"},{"type":"module","title":"ExESDB.ClusterSystem","doc":"Supervisor for cluster coordination components.\n\nThis supervisor manages cluster-specific coordination components:\n- ClusterCoordinator: Handles coordination logic and split-brain prevention\n- NodeMonitor: Monitors node health and handles failures\n\nNote: KhepriCluster is managed at the System level since it's mode-aware.","ref":"ExESDB.ClusterSystem.html"},{"type":"function","title":"ExESDB.ClusterSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.ClusterSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.ClusterSystem.start_link/1","doc":"","ref":"ExESDB.ClusterSystem.html#start_link/1"},{"type":"module","title":"ExESDB.ConsistencyChecker","doc":"Provides tools for verifying consistency across ExESDB cluster stores.\n\nThis module leverages Khepri and Ra APIs to verify that stores are consistent\nacross cluster nodes, detect split-brain scenarios, and ensure data integrity.","ref":"ExESDB.ConsistencyChecker.html"},{"type":"function","title":"ExESDB.ConsistencyChecker.check_raft_log_consistency/1","doc":"Checks Raft log consistency across cluster members.\nThis is a more intensive check that examines log indices and terms.","ref":"ExESDB.ConsistencyChecker.html#check_raft_log_consistency/1"},{"type":"type","title":"ExESDB.ConsistencyChecker.check_result/0","doc":"","ref":"ExESDB.ConsistencyChecker.html#t:check_result/0"},{"type":"type","title":"ExESDB.ConsistencyChecker.consistency_result/0","doc":"","ref":"ExESDB.ConsistencyChecker.html#t:consistency_result/0"},{"type":"type","title":"ExESDB.ConsistencyChecker.node_name/0","doc":"","ref":"ExESDB.ConsistencyChecker.html#t:node_name/0"},{"type":"function","title":"ExESDB.ConsistencyChecker.quick_health_check/1","doc":"Quick health check to verify if a store is accessible and responsive across nodes.","ref":"ExESDB.ConsistencyChecker.html#quick_health_check/1"},{"type":"function","title":"ExESDB.ConsistencyChecker.start_consistency_monitoring/2","doc":"Monitors consistency over time and reports any deviations.","ref":"ExESDB.ConsistencyChecker.html#start_consistency_monitoring/2"},{"type":"type","title":"ExESDB.ConsistencyChecker.store_id/0","doc":"","ref":"ExESDB.ConsistencyChecker.html#t:store_id/0"},{"type":"function","title":"ExESDB.ConsistencyChecker.verify_cluster_consistency/1","doc":"Performs a comprehensive consistency check across all cluster nodes for a given store.","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1"},{"type":"function","title":"Returns - ExESDB.ConsistencyChecker.verify_cluster_consistency/1","doc":"- `{:ok, report}` - Detailed consistency report\n- `{:error, reason}` - Error occurred during check","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1-returns"},{"type":"function","title":"Example - ExESDB.ConsistencyChecker.verify_cluster_consistency/1","doc":"iex> ExESDB.ConsistencyChecker.verify_cluster_consistency(:my_store)\n    {:ok, %{\n      status: :consistent,\n      nodes_checked: 3,\n      leader: :\"node1@host\",\n      members: [:\"node1@host\", :\"node2@host\", :\"node3@host\"],\n      raft_status: :healthy,\n      potential_issues: []\n    }}","ref":"ExESDB.ConsistencyChecker.html#verify_cluster_consistency/1-example"},{"type":"function","title":"ExESDB.ConsistencyChecker.verify_membership_consensus/1","doc":"Verifies that all nodes agree on cluster membership.","ref":"ExESDB.ConsistencyChecker.html#verify_membership_consensus/1"},{"type":"module","title":"ExESDB.CoreSystem","doc":"Critical infrastructure supervisor that manages core ExESDB components.\n\nThis supervisor uses :one_for_all strategy because these components are \ntightly coupled and must restart together to maintain consistency.\n\nStartup order:\n1. PersistenceSystem: Manages streams, snapshots, and subscriptions (foundation)\n2. NotificationSystem: Manages leadership and event emission (depends on persistence)\n3. StoreSystem: Manages store lifecycle and clustering (depends on persistence & notification)\n\nThe NotificationSystem includes:\n- LeaderSystem: Leadership responsibilities and subscription management\n- EmitterSystem: Event emission and distribution\n\nThis ensures that leadership and event distribution are core capabilities\navailable in both single-node and cluster modes.","ref":"ExESDB.CoreSystem.html"},{"type":"function","title":"ExESDB.CoreSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.CoreSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.CoreSystem.start_link/1","doc":"","ref":"ExESDB.CoreSystem.html#start_link/1"},{"type":"module","title":"ExESDB.Debugger","doc":"Comprehensive debugging and inspection API for ExESDB systems.\n\nThis module provides REPL-friendly functions to investigate all aspects\nof an ExESDB Event Sourcing Database by delegating to specialized subsystems:\n\n- Process supervision tree inspection (Inspector)\n- Health and performance monitoring (Monitor)\n- Scenario testing and simulation (ScenarioManager)\n- Real-time observation (Observer)","ref":"ExESDB.Debugger.html"},{"type":"module","title":"Usage in REPL - ExESDB.Debugger","doc":"iex> ExESDB.Debugger.overview()\n    iex> ExESDB.Debugger.start_scenario(:high_load, intensity: :medium)\n    iex> ExESDB.Debugger.observe(:system_metrics)","ref":"ExESDB.Debugger.html#module-usage-in-repl"},{"type":"function","title":"ExESDB.Debugger.benchmark/2","doc":"Benchmark a function.","ref":"ExESDB.Debugger.html#benchmark/2"},{"type":"function","title":"ExESDB.Debugger.config/1","doc":"Show detailed configuration for the store.","ref":"ExESDB.Debugger.html#config/1"},{"type":"function","title":"ExESDB.Debugger.health/1","doc":"Perform comprehensive health check.","ref":"ExESDB.Debugger.html#health/1"},{"type":"function","title":"ExESDB.Debugger.help/0","doc":"Show help information for all available debugging commands.","ref":"ExESDB.Debugger.html#help/0"},{"type":"function","title":"ExESDB.Debugger.list_observations/0","doc":"List all active observations.","ref":"ExESDB.Debugger.html#list_observations/0"},{"type":"function","title":"ExESDB.Debugger.list_scenarios/0","doc":"List all running scenarios.","ref":"ExESDB.Debugger.html#list_scenarios/0"},{"type":"function","title":"ExESDB.Debugger.observation_data/1","doc":"Get observation data for analysis.","ref":"ExESDB.Debugger.html#observation_data/1"},{"type":"function","title":"ExESDB.Debugger.observe/2","doc":"Start real-time observation of system components.\n\nAvailable targets:\n- `:system_metrics` - Overall system metrics\n- `:process_metrics` - Specific process metrics (requires :target_process opt)\n- `:memory_usage` - Memory usage patterns","ref":"ExESDB.Debugger.html#observe/2"},{"type":"function","title":"Examples - ExESDB.Debugger.observe/2","doc":"ExESDB.Debugger.observe(:system_metrics, interval: 1000)\n    ExESDB.Debugger.observe(:process_metrics, target_process: :my_process, interval: 2000)","ref":"ExESDB.Debugger.html#observe/2-examples"},{"type":"function","title":"ExESDB.Debugger.overview/1","doc":"Display a comprehensive overview of the ExESDB system.","ref":"ExESDB.Debugger.html#overview/1"},{"type":"function","title":"ExESDB.Debugger.performance/1","doc":"Show performance metrics.","ref":"ExESDB.Debugger.html#performance/1"},{"type":"function","title":"ExESDB.Debugger.processes/1","doc":"List all ExESDB-related processes with formatted output.","ref":"ExESDB.Debugger.html#processes/1"},{"type":"function","title":"ExESDB.Debugger.start_scenario/2","doc":"Start a test scenario.\n\nAvailable scenarios:\n- `:high_load` - Simulate high CPU/memory load\n- `:node_failure` - Simulate node failures\n- `:custom` - Load custom scenario from config","ref":"ExESDB.Debugger.html#start_scenario/2"},{"type":"function","title":"Examples - ExESDB.Debugger.start_scenario/2","doc":"ExESDB.Debugger.start_scenario(:high_load, intensity: :medium, duration: 30_000)\n    ExESDB.Debugger.start_scenario(:custom, config_path: \"scenarios/custom.json\")","ref":"ExESDB.Debugger.html#start_scenario/2-examples"},{"type":"function","title":"ExESDB.Debugger.stop_observation/1","doc":"Stop a running observation.","ref":"ExESDB.Debugger.html#stop_observation/1"},{"type":"function","title":"ExESDB.Debugger.stop_scenario/1","doc":"Stop a running scenario.","ref":"ExESDB.Debugger.html#stop_scenario/1"},{"type":"function","title":"ExESDB.Debugger.supervision_tree/1","doc":"Display the supervision tree for ExESDB.","ref":"ExESDB.Debugger.html#supervision_tree/1"},{"type":"function","title":"ExESDB.Debugger.top/1","doc":"Show top processes by memory/CPU usage.","ref":"ExESDB.Debugger.html#top/1"},{"type":"function","title":"ExESDB.Debugger.trace/3","doc":"Trace function calls for debugging.","ref":"ExESDB.Debugger.html#trace/3"},{"type":"module","title":"ExESDB.DebuggerSystem","doc":"Supervisor for the ExESDB Debugger system.\n\nManages all debugger subsystems including inspection, monitoring,\nscenario management, and observation.","ref":"ExESDB.DebuggerSystem.html"},{"type":"function","title":"ExESDB.DebuggerSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.DebuggerSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.DebuggerSystem.start_link/1","doc":"","ref":"ExESDB.DebuggerSystem.html#start_link/1"},{"type":"module","title":"ExESDB.EmitterSystem","doc":"Supervisor for event emission components.\n\nThis supervisor manages the emitter pools that handle event distribution\nto subscribers. Only active when this node is the cluster leader.\n\nComponents:\n- EmitterPools: PartitionSupervisor managing dynamic emitter pools","ref":"ExESDB.EmitterSystem.html"},{"type":"function","title":"ExESDB.EmitterSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.EmitterSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.EmitterSystem.start_link/1","doc":"","ref":"ExESDB.EmitterSystem.html#start_link/1"},{"type":"module","title":"ExESDB.EmitterWorker","doc":"As part of the ExESDB.System, \n  the EmitterWorker is responsible for managing the communication \n  between the Event Store and the PubSub mechanism.","ref":"ExESDB.EmitterWorker.html"},{"type":"function","title":"ExESDB.EmitterWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.EmitterWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.EmitterWorker.start_link/1","doc":"","ref":"ExESDB.EmitterWorker.html#start_link/1"},{"type":"module","title":"ExESDB.Emitters","doc":"As part of the ExESDB.System, ExESDB.Emitters is responsible for managing the\n  lifetime of the Emitter processes.","ref":"ExESDB.Emitters.html"},{"type":"function","title":"ExESDB.Emitters.start_emitter_pool/3","doc":"","ref":"ExESDB.Emitters.html#start_emitter_pool/3"},{"type":"function","title":"ExESDB.Emitters.stop_emitter_pool/2","doc":"Stops an EmitterPool for a given subscription.","ref":"ExESDB.Emitters.html#stop_emitter_pool/2"},{"type":"function","title":"ExESDB.Emitters.update_emitter_pool/2","doc":"Updates an EmitterPool with new subscription data.\nThis is typically called when the subscriber PID changes.","ref":"ExESDB.Emitters.html#update_emitter_pool/2"},{"type":"module","title":"ExESDB.EnVars","doc":"This module contains the environment variables that are used by ExESDB","ref":"ExESDB.EnVars.html"},{"type":"function","title":"ExESDB.EnVars.cluster_secret/0","doc":"","ref":"ExESDB.EnVars.html#cluster_secret/0"},{"type":"function","title":"ExESDB.EnVars.data_dir/0","doc":"Returns the data directory. default: `/data`","ref":"ExESDB.EnVars.html#data_dir/0"},{"type":"function","title":"ExESDB.EnVars.db_type/0","doc":"Returns the db type. `single` or `cluster`. default: `single`","ref":"ExESDB.EnVars.html#db_type/0"},{"type":"function","title":"ExESDB.EnVars.gossip_multicast_addr/0","doc":"Returns the gossip multicast address. default: `255.255.255.255`","ref":"ExESDB.EnVars.html#gossip_multicast_addr/0"},{"type":"function","title":"ExESDB.EnVars.persistence_enabled/0","doc":"Returns whether persistence is enabled. default: `true`","ref":"ExESDB.EnVars.html#persistence_enabled/0"},{"type":"function","title":"ExESDB.EnVars.persistence_interval/0","doc":"Returns the persistence interval in milliseconds. default: `5_000`","ref":"ExESDB.EnVars.html#persistence_interval/0"},{"type":"function","title":"ExESDB.EnVars.reader_idle_ms/0","doc":"Returns the idle readers timeout in milliseconds. default: `10_000`","ref":"ExESDB.EnVars.html#reader_idle_ms/0"},{"type":"function","title":"ExESDB.EnVars.store_description/0","doc":"Returns the store description. default: `nil`","ref":"ExESDB.EnVars.html#store_description/0"},{"type":"function","title":"ExESDB.EnVars.store_id/0","doc":"Returns the khepri store id. default: `ex_esdb_store`","ref":"ExESDB.EnVars.html#store_id/0"},{"type":"function","title":"ExESDB.EnVars.store_tags/0","doc":"Returns the store tags as comma-separated values. default: `nil`","ref":"ExESDB.EnVars.html#store_tags/0"},{"type":"function","title":"ExESDB.EnVars.timeout/0","doc":"Returns the timeout in milliseconds. default: `10_000`","ref":"ExESDB.EnVars.html#timeout/0"},{"type":"function","title":"ExESDB.EnVars.writer_idle_ms/0","doc":"Returns the idle writers timeout in milliseconds. default: `10_000`","ref":"ExESDB.EnVars.html#writer_idle_ms/0"},{"type":"module","title":"ExESDB.EventProjector","doc":"This module contains the event projector functionality","ref":"ExESDB.EventProjector.html"},{"type":"function","title":"ExESDB.EventProjector.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.EventProjector.html#child_spec/1"},{"type":"function","title":"ExESDB.EventProjector.start_link/1","doc":"","ref":"ExESDB.EventProjector.html#start_link/1"},{"type":"module","title":"ExESDB.Events","doc":"Event definitions and schemas for ExESDB subsystems.\n\nThis module provides centralized event type definitions and helper functions\nfor building event payloads. Each module is responsible for publishing its\nown events using Phoenix.PubSub directly.","ref":"ExESDB.Events.html"},{"type":"module","title":"Event Categories - ExESDB.Events","doc":"- `:system` - System lifecycle and initialization events\n- `:cluster` - Cluster membership and coordination events\n- `:leadership` - Leadership changes and responsibilities\n- `:persistence` - Data persistence and storage events\n- `:gateway` - External interface and API events\n- `:coordination` - General coordination events\n- `:subscriptions` - Subscription management events","ref":"ExESDB.Events.html#module-event-categories"},{"type":"module","title":"Usage - ExESDB.Events","doc":"```elixir\n# Each module publishes its own events:\n\n# In PersistenceWorker:\nevent = Events.build_event(:events_persisted, %{stream_id: \"orders\", event_count: 5})\nPhoenix.PubSub.broadcast(ExESDB.PubSub, \"exesdb:control:store_id:persistence\", \n  {:persistence_event, event})\n\n# In StoreCluster:\nevent = Events.build_event(:cluster_joined, %{via_node: target_node})\nPhoenix.PubSub.broadcast(ExESDB.PubSub, \"exesdb:control:store_id:cluster\", \n  {:cluster_event, event})\n```","ref":"ExESDB.Events.html#module-usage"},{"type":"function","title":"ExESDB.Events.all_events/0","doc":"Returns all event types organized by category","ref":"ExESDB.Events.html#all_events/0"},{"type":"function","title":"ExESDB.Events.build_event/3","doc":"Builds a standard event map with common fields.","ref":"ExESDB.Events.html#build_event/3"},{"type":"function","title":"Parameters - ExESDB.Events.build_event/3","doc":"- `event_type` - The event type (atom)\n- `data` - Event-specific data (map)\n- `opts` - Optional fields like `:store_id` (keyword list)","ref":"ExESDB.Events.html#build_event/3-parameters"},{"type":"function","title":"Examples - ExESDB.Events.build_event/3","doc":"Events.build_event(:cluster_joined, %{via_node: node1})\n    Events.build_event(:events_persisted, %{stream_id: \"orders\", count: 5}, store_id: :my_store)","ref":"ExESDB.Events.html#build_event/3-examples"},{"type":"function","title":"ExESDB.Events.build_payload/4","doc":"Helper to build event payload with standardized event wrapper.","ref":"ExESDB.Events.html#build_payload/4"},{"type":"function","title":"Examples - ExESDB.Events.build_payload/4","doc":"Events.build_payload(:cluster_event, :cluster_joined, %{via_node: node1})\n    # => {:cluster_event, %{event_type: :cluster_joined, ...}}","ref":"ExESDB.Events.html#build_payload/4-examples"},{"type":"function","title":"ExESDB.Events.build_topic/2","doc":"Builds a topic string for a given store and category.","ref":"ExESDB.Events.html#build_topic/2"},{"type":"function","title":"Examples - ExESDB.Events.build_topic/2","doc":"Events.build_topic(:my_store, :cluster)\n    # => \"exesdb:control:my_store:cluster\"","ref":"ExESDB.Events.html#build_topic/2-examples"},{"type":"function","title":"ExESDB.Events.cluster_events/0","doc":"Returns all defined cluster events","ref":"ExESDB.Events.html#cluster_events/0"},{"type":"function","title":"ExESDB.Events.coordination_events/0","doc":"Returns all defined coordination events","ref":"ExESDB.Events.html#coordination_events/0"},{"type":"function","title":"ExESDB.Events.gateway_events/0","doc":"Returns all defined gateway events","ref":"ExESDB.Events.html#gateway_events/0"},{"type":"function","title":"ExESDB.Events.leadership_events/0","doc":"Returns all defined leadership events","ref":"ExESDB.Events.html#leadership_events/0"},{"type":"function","title":"ExESDB.Events.persistence_events/0","doc":"Returns all defined persistence events","ref":"ExESDB.Events.html#persistence_events/0"},{"type":"function","title":"ExESDB.Events.subscription_events/0","doc":"Returns all defined subscription events","ref":"ExESDB.Events.html#subscription_events/0"},{"type":"function","title":"ExESDB.Events.system_events/0","doc":"Returns all defined system events","ref":"ExESDB.Events.html#system_events/0"},{"type":"function","title":"ExESDB.Events.valid_event?/2","doc":"Checks if an event type is valid for a given category","ref":"ExESDB.Events.html#valid_event?/2"},{"type":"function","title":"ExESDB.Events.valid_event_structure?/1","doc":"Validates that an event has all required fields","ref":"ExESDB.Events.html#valid_event_structure?/1"},{"type":"module","title":"ExESDB.GatewaySupervisor","doc":"@deprecated \"Use ExESDB.GatewaySystem instead\"\n  \n  The GatewaySupervisor is responsible for starting and supervising the\n  GatewayWorkers.\n  \n  This module is deprecated in favor of ExESDB.GatewaySystem which provides\n  improved fault tolerance with pooled workers.","ref":"ExESDB.GatewaySupervisor.html"},{"type":"function","title":"ExESDB.GatewaySupervisor.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.GatewaySupervisor.html#child_spec/1"},{"type":"function","title":"ExESDB.GatewaySupervisor.start_link/1","doc":"","ref":"ExESDB.GatewaySupervisor.html#start_link/1"},{"type":"module","title":"ExESDB.GatewaySystem","doc":"Supervisor for gateway components providing external interface.\n\nThis supervisor manages a pool of gateway workers for high availability\nand load distribution.\n\nComponents:\n- GatewayWorkers: Pool of GatewayWorkers via PartitionSupervisor\n# PubSub: External communication now handled externally","ref":"ExESDB.GatewaySystem.html"},{"type":"function","title":"ExESDB.GatewaySystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.GatewaySystem.html#child_spec/1"},{"type":"function","title":"ExESDB.GatewaySystem.start_link/1","doc":"","ref":"ExESDB.GatewaySystem.html#start_link/1"},{"type":"module","title":"ExESDB.GatewayWorker","doc":"GatewayWorker processes are started on each node in the cluster,\n  and contain the implementation functions for the Gater.API.","ref":"ExESDB.GatewayWorker.html"},{"type":"function","title":"ExESDB.GatewayWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.GatewayWorker.html#child_spec/1"},{"type":"type","title":"ExESDB.GatewayWorker.error/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:error/0"},{"type":"function","title":"ExESDB.GatewayWorker.gateway_worker_name/1","doc":"","ref":"ExESDB.GatewayWorker.html#gateway_worker_name/1"},{"type":"type","title":"ExESDB.GatewayWorker.selector_type/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:selector_type/0"},{"type":"function","title":"ExESDB.GatewayWorker.start_link/1","doc":"","ref":"ExESDB.GatewayWorker.html#start_link/1"},{"type":"type","title":"ExESDB.GatewayWorker.store/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:store/0"},{"type":"type","title":"ExESDB.GatewayWorker.stream/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:stream/0"},{"type":"type","title":"ExESDB.GatewayWorker.subscription_name/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:subscription_name/0"},{"type":"type","title":"ExESDB.GatewayWorker.subscription_type/0","doc":"","ref":"ExESDB.GatewayWorker.html#t:subscription_type/0"},{"type":"module","title":"ExESDB.HashRing","doc":"Provides functions for distributing load over the cluster.","ref":"ExESDB.HashRing.html"},{"type":"function","title":"ExESDB.HashRing.get_core_for_stream/1","doc":"","ref":"ExESDB.HashRing.html#get_core_for_stream/1"},{"type":"function","title":"ExESDB.HashRing.get_node_for_stream/1","doc":"","ref":"ExESDB.HashRing.html#get_node_for_stream/1"},{"type":"module","title":"ExESDB.Inspection.ConfigInspector","doc":"Inspects configuration settings for the ExESDB system.\n\nProvides functionality to retrieve and analyze system configuration\nacross different stores and environments.","ref":"ExESDB.Inspection.ConfigInspector.html"},{"type":"function","title":"ExESDB.Inspection.ConfigInspector.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Inspection.ConfigInspector.html#child_spec/1"},{"type":"function","title":"ExESDB.Inspection.ConfigInspector.get_config/1","doc":"","ref":"ExESDB.Inspection.ConfigInspector.html#get_config/1"},{"type":"function","title":"ExESDB.Inspection.ConfigInspector.get_environment_config/0","doc":"","ref":"ExESDB.Inspection.ConfigInspector.html#get_environment_config/0"},{"type":"function","title":"ExESDB.Inspection.ConfigInspector.start_link/1","doc":"","ref":"ExESDB.Inspection.ConfigInspector.html#start_link/1"},{"type":"module","title":"ExESDB.Inspection.ProcessInspector","doc":"Inspects processes related to the ExESDB system.\n\nProvides functionality to list, categorize, and analyze processes\nfor health monitoring and debugging.","ref":"ExESDB.Inspection.ProcessInspector.html"},{"type":"function","title":"ExESDB.Inspection.ProcessInspector.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Inspection.ProcessInspector.html#child_spec/1"},{"type":"function","title":"ExESDB.Inspection.ProcessInspector.list_processes/0","doc":"","ref":"ExESDB.Inspection.ProcessInspector.html#list_processes/0"},{"type":"function","title":"ExESDB.Inspection.ProcessInspector.start_link/1","doc":"","ref":"ExESDB.Inspection.ProcessInspector.html#start_link/1"},{"type":"module","title":"ExESDB.Inspection.TreeInspector","doc":"Inspects the supervision tree of the ExESDB system.\n\nProvides visualization and analysis of the system's supervision\nhierarchy to aid in debugging and monitoring.","ref":"ExESDB.Inspection.TreeInspector.html"},{"type":"function","title":"ExESDB.Inspection.TreeInspector.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Inspection.TreeInspector.html#child_spec/1"},{"type":"function","title":"ExESDB.Inspection.TreeInspector.start_link/1","doc":"","ref":"ExESDB.Inspection.TreeInspector.html#start_link/1"},{"type":"function","title":"ExESDB.Inspection.TreeInspector.view_supervision_tree/1","doc":"","ref":"ExESDB.Inspection.TreeInspector.html#view_supervision_tree/1"},{"type":"module","title":"ExESDB.InspectionSystem","doc":"Supervisor for the Inspection subsystem.\n\nManages all process, configuration, and supervision tree inspection components.","ref":"ExESDB.InspectionSystem.html"},{"type":"function","title":"ExESDB.InspectionSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.InspectionSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.InspectionSystem.start_link/1","doc":"","ref":"ExESDB.InspectionSystem.html#start_link/1"},{"type":"module","title":"ExESDB.LeaderSystem","doc":"This module supervises the Leader Subsystem.","ref":"ExESDB.LeaderSystem.html"},{"type":"function","title":"ExESDB.LeaderSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.LeaderSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.LeaderSystem.start_link/1","doc":"","ref":"ExESDB.LeaderSystem.html#start_link/1"},{"type":"module","title":"ExESDB.LeaderTracker","doc":"As part of the ExESDB.System, the SubscriptionsTracker is responsible for\n  observing the subscriptions that are maintained in the Store.\n\n  Since Khepri triggers are executed on the leader node, the SubscriptionsTracker\n  will be instructed to start the Emitters system on the leader node whenever a new subscription\n  is registered.\n\n  When a Subscription is deleted, the SubscriptionsTracker will instruct the Emitters system to stop \n  the associated EmitterPool.","ref":"ExESDB.LeaderTracker.html"},{"type":"function","title":"ExESDB.LeaderTracker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.LeaderTracker.html#child_spec/1"},{"type":"function","title":"ExESDB.LeaderTracker.start_link/1","doc":"","ref":"ExESDB.LeaderTracker.html#start_link/1"},{"type":"module","title":"ExESDB.LeaderWorker","doc":"This module contains the leader's reponsibilities for the cluster.","ref":"ExESDB.LeaderWorker.html"},{"type":"function","title":"ExESDB.LeaderWorker.activate/1","doc":"Activates the LeaderWorker for the given store.\n\nThis function is called when this node becomes the cluster leader.","ref":"ExESDB.LeaderWorker.html#activate/1"},{"type":"function","title":"ExESDB.LeaderWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.LeaderWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.LeaderWorker.start_link/1","doc":"","ref":"ExESDB.LeaderWorker.html#start_link/1"},{"type":"module","title":"ExESDB.LoggerFilters","doc":"Custom logger filters to reduce noise from Ra, Khepri, and other verbose components","ref":"ExESDB.LoggerFilters.html"},{"type":"function","title":"ExESDB.LoggerFilters.filter_khepri/1","doc":"Filter Khepri noise - reduce verbose operational messages","ref":"ExESDB.LoggerFilters.html#filter_khepri/1"},{"type":"function","title":"ExESDB.LoggerFilters.filter_libcluster/1","doc":"Filter libcluster noise - reduce cluster formation chatter","ref":"ExESDB.LoggerFilters.html#filter_libcluster/1"},{"type":"function","title":"ExESDB.LoggerFilters.filter_ra/1","doc":"Filter Ra consensus library noise - only show errors and warnings","ref":"ExESDB.LoggerFilters.html#filter_ra/1"},{"type":"function","title":"ExESDB.LoggerFilters.filter_swarm/1","doc":"Filter Swarm noise - already provided by BCUtils but adding our own","ref":"ExESDB.LoggerFilters.html#filter_swarm/1"},{"type":"module","title":"ExESDB.LoggerWorker","doc":"LoggerWorker subscribes to events for a specific subsystem and logs events for monitoring and debugging.\n\nEach subsystem should supervise its own LoggerWorker to provide dedicated logging\nfor that subsystem's events and operations.","ref":"ExESDB.LoggerWorker.html"},{"type":"function","title":"ExESDB.LoggerWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.LoggerWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.LoggerWorker.start_link/1","doc":"Starts the LoggerWorker for a specific subsystem.","ref":"ExESDB.LoggerWorker.html#start_link/1"},{"type":"function","title":"Parameters - ExESDB.LoggerWorker.start_link/1","doc":"- opts: Configuration options including store_id and subsystem_name","ref":"ExESDB.LoggerWorker.html#start_link/1-parameters"},{"type":"module","title":"ExESDB.Metrics","doc":"Provides metrics for the event store.","ref":"ExESDB.Metrics.html"},{"type":"module","title":"ExESDB.Monitoring.HealthChecker","doc":"Performs comprehensive health checks on the ExESDB system.\n\nMonitors system processes, configuration validity, resource usage,\nand overall system health to detect potential issues.","ref":"ExESDB.Monitoring.HealthChecker.html"},{"type":"function","title":"ExESDB.Monitoring.HealthChecker.check_resource_usage/0","doc":"","ref":"ExESDB.Monitoring.HealthChecker.html#check_resource_usage/0"},{"type":"function","title":"ExESDB.Monitoring.HealthChecker.check_system_processes/1","doc":"","ref":"ExESDB.Monitoring.HealthChecker.html#check_system_processes/1"},{"type":"function","title":"ExESDB.Monitoring.HealthChecker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Monitoring.HealthChecker.html#child_spec/1"},{"type":"function","title":"ExESDB.Monitoring.HealthChecker.perform_health_check/1","doc":"","ref":"ExESDB.Monitoring.HealthChecker.html#perform_health_check/1"},{"type":"function","title":"ExESDB.Monitoring.HealthChecker.start_link/1","doc":"","ref":"ExESDB.Monitoring.HealthChecker.html#start_link/1"},{"type":"module","title":"ExESDB.MonitoringSystem","doc":"Supervisor for the Monitoring subsystem.\n\nManages health checking, performance tracking, and system monitoring components.","ref":"ExESDB.MonitoringSystem.html"},{"type":"function","title":"ExESDB.MonitoringSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.MonitoringSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.MonitoringSystem.start_link/1","doc":"","ref":"ExESDB.MonitoringSystem.html#start_link/1"},{"type":"module","title":"ExESDB.NodeMonitor","doc":"Provides fast failure detection and cluster health monitoring to handle hard node crashes.\n\nThis module implements a multi-layer approach:\n1. Active health probing of cluster nodes\n2. Fast detection of unresponsive nodes\n3. Proactive cleanup of Swarm registrations\n4. Coordination with Khepri cluster management","ref":"ExESDB.NodeMonitor.html"},{"type":"function","title":"ExESDB.NodeMonitor.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.NodeMonitor.html#child_spec/1"},{"type":"function","title":"ExESDB.NodeMonitor.health_status/1","doc":"Get current cluster health status","ref":"ExESDB.NodeMonitor.html#health_status/1"},{"type":"function","title":"ExESDB.NodeMonitor.probe_node/2","doc":"Force probe a specific node","ref":"ExESDB.NodeMonitor.html#probe_node/2"},{"type":"function","title":"ExESDB.NodeMonitor.start_link/1","doc":"Start the node monitor","ref":"ExESDB.NodeMonitor.html#start_link/1"},{"type":"module","title":"ExESDB.NotificationSystem","doc":"Supervisor for event notification and distribution components.\n\nThis supervisor manages the core event notification functionality:\n- LeaderSystem: Leadership responsibilities and subscription management\n- EmitterSystem: Event emission and distribution\n\nThis is a core component that runs in both single-node and cluster modes.\nThe leadership determination happens at the store level, not the clustering level.","ref":"ExESDB.NotificationSystem.html"},{"type":"function","title":"ExESDB.NotificationSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.NotificationSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.NotificationSystem.start_link/1","doc":"","ref":"ExESDB.NotificationSystem.html#start_link/1"},{"type":"module","title":"ExESDB.Observation.MetricsCollector","doc":"Collects various system metrics for observation and analysis.\n\nHandles collection of CPU, memory, process count, and other\nsystem-wide metrics for real-time monitoring.","ref":"ExESDB.Observation.MetricsCollector.html"},{"type":"function","title":"ExESDB.Observation.MetricsCollector.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Observation.MetricsCollector.html#child_spec/1"},{"type":"function","title":"ExESDB.Observation.MetricsCollector.collect_memory_metrics/0","doc":"","ref":"ExESDB.Observation.MetricsCollector.html#collect_memory_metrics/0"},{"type":"function","title":"ExESDB.Observation.MetricsCollector.collect_process_metrics/1","doc":"","ref":"ExESDB.Observation.MetricsCollector.html#collect_process_metrics/1"},{"type":"function","title":"ExESDB.Observation.MetricsCollector.collect_system_metrics/0","doc":"","ref":"ExESDB.Observation.MetricsCollector.html#collect_system_metrics/0"},{"type":"function","title":"ExESDB.Observation.MetricsCollector.start_link/1","doc":"","ref":"ExESDB.Observation.MetricsCollector.html#start_link/1"},{"type":"module","title":"ExESDB.ObservationSystem","doc":"Supervisor for the Observation subsystem.\n\nManages real-time monitoring, metrics collection, and observation components.","ref":"ExESDB.ObservationSystem.html"},{"type":"function","title":"ExESDB.ObservationSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.ObservationSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.ObservationSystem.start_link/1","doc":"","ref":"ExESDB.ObservationSystem.html#start_link/1"},{"type":"module","title":"ExESDB.Options","doc":"This module contains the options helper functions for ExESDB\n  \n  Modified to support umbrella configuration patterns where configurations\n  are stored under individual app names instead of the global :ex_esdb key.","ref":"ExESDB.Options.html"},{"type":"function","title":"ExESDB.Options.app_env/1","doc":"","ref":"ExESDB.Options.html#app_env/1"},{"type":"function","title":"ExESDB.Options.app_env/3","doc":"","ref":"ExESDB.Options.html#app_env/3"},{"type":"function","title":"ExESDB.Options.data_dir/0","doc":"","ref":"ExESDB.Options.html#data_dir/0"},{"type":"function","title":"ExESDB.Options.data_dir/1","doc":"","ref":"ExESDB.Options.html#data_dir/1"},{"type":"function","title":"ExESDB.Options.db_type/0","doc":"","ref":"ExESDB.Options.html#db_type/0"},{"type":"function","title":"ExESDB.Options.db_type/1","doc":"","ref":"ExESDB.Options.html#db_type/1"},{"type":"function","title":"ExESDB.Options.get_context/0","doc":"","ref":"ExESDB.Options.html#get_context/0"},{"type":"function","title":"ExESDB.Options.get_context_or_discover/0","doc":"","ref":"ExESDB.Options.html#get_context_or_discover/0"},{"type":"function","title":"ExESDB.Options.persistence_enabled/0","doc":"","ref":"ExESDB.Options.html#persistence_enabled/0"},{"type":"function","title":"ExESDB.Options.persistence_enabled/1","doc":"","ref":"ExESDB.Options.html#persistence_enabled/1"},{"type":"function","title":"ExESDB.Options.persistence_interval/0","doc":"","ref":"ExESDB.Options.html#persistence_interval/0"},{"type":"function","title":"ExESDB.Options.persistence_interval/1","doc":"","ref":"ExESDB.Options.html#persistence_interval/1"},{"type":"function","title":"ExESDB.Options.reader_idle_ms/0","doc":"","ref":"ExESDB.Options.html#reader_idle_ms/0"},{"type":"function","title":"ExESDB.Options.reader_idle_ms/1","doc":"","ref":"ExESDB.Options.html#reader_idle_ms/1"},{"type":"function","title":"ExESDB.Options.set_context/1","doc":"","ref":"ExESDB.Options.html#set_context/1"},{"type":"function","title":"ExESDB.Options.store_description/0","doc":"","ref":"ExESDB.Options.html#store_description/0"},{"type":"function","title":"ExESDB.Options.store_description/1","doc":"","ref":"ExESDB.Options.html#store_description/1"},{"type":"function","title":"ExESDB.Options.store_id/0","doc":"","ref":"ExESDB.Options.html#store_id/0"},{"type":"function","title":"ExESDB.Options.store_id/1","doc":"","ref":"ExESDB.Options.html#store_id/1"},{"type":"function","title":"ExESDB.Options.store_tags/0","doc":"","ref":"ExESDB.Options.html#store_tags/0"},{"type":"function","title":"ExESDB.Options.store_tags/1","doc":"","ref":"ExESDB.Options.html#store_tags/1"},{"type":"function","title":"ExESDB.Options.sys_env/1","doc":"","ref":"ExESDB.Options.html#sys_env/1"},{"type":"function","title":"ExESDB.Options.timeout/0","doc":"","ref":"ExESDB.Options.html#timeout/0"},{"type":"function","title":"ExESDB.Options.timeout/1","doc":"","ref":"ExESDB.Options.html#timeout/1"},{"type":"function","title":"ExESDB.Options.topologies/0","doc":"","ref":"ExESDB.Options.html#topologies/0"},{"type":"function","title":"ExESDB.Options.with_context/2","doc":"","ref":"ExESDB.Options.html#with_context/2"},{"type":"function","title":"ExESDB.Options.writer_idle_ms/0","doc":"","ref":"ExESDB.Options.html#writer_idle_ms/0"},{"type":"function","title":"ExESDB.Options.writer_idle_ms/1","doc":"","ref":"ExESDB.Options.html#writer_idle_ms/1"},{"type":"module","title":"ExESDB.Options.Simplified","doc":"Simplified ExESDB configuration module using automatic OTP app discovery.\n\nThis module dramatically reduces complexity by:\n1. Automatically discovering the calling application\n2. Using a macro to generate config functions with less duplication\n3. Providing sensible defaults and environment variable overrides\n\nUsage:\n  # Automatic discovery (recommended)\n  ExESDB.Options.data_dir()      # Discovers calling app automatically\n  \n  # Explicit app specification\n  ExESDB.Options.data_dir(:my_app)\n  \n  # Context-based (for umbrella apps)\n  ExESDB.Options.with_context(:my_app, fn ->\n    ExESDB.Options.data_dir()    # Uses :my_app context\n  end)","ref":"ExESDB.Options.Simplified.html"},{"type":"function","title":"ExESDB.Options.Simplified.app_env/1","doc":"Get complete configuration for an OTP app","ref":"ExESDB.Options.Simplified.html#app_env/1"},{"type":"function","title":"ExESDB.Options.Simplified.app_env/3","doc":"Get a specific config value from app environment with default.","ref":"ExESDB.Options.Simplified.html#app_env/3"},{"type":"function","title":"ExESDB.Options.Simplified.data_dir/1","doc":"","ref":"ExESDB.Options.Simplified.html#data_dir/1"},{"type":"function","title":"ExESDB.Options.Simplified.db_type/1","doc":"","ref":"ExESDB.Options.Simplified.html#db_type/1"},{"type":"function","title":"ExESDB.Options.Simplified.discover_app_name/0","doc":"Discovers the current OTP application name using multiple strategies.","ref":"ExESDB.Options.Simplified.html#discover_app_name/0"},{"type":"function","title":"ExESDB.Options.Simplified.get_config_value/2","doc":"Generic configuration value getter with automatic app discovery.\n\nPriority order:\n1. Environment variable\n2. Application configuration\n3. Default value","ref":"ExESDB.Options.Simplified.html#get_config_value/2"},{"type":"function","title":"ExESDB.Options.Simplified.get_context/0","doc":"Get current application context","ref":"ExESDB.Options.Simplified.html#get_context/0"},{"type":"function","title":"ExESDB.Options.Simplified.persistence_enabled/1","doc":"","ref":"ExESDB.Options.Simplified.html#persistence_enabled/1"},{"type":"function","title":"ExESDB.Options.Simplified.persistence_interval/1","doc":"","ref":"ExESDB.Options.Simplified.html#persistence_interval/1"},{"type":"function","title":"ExESDB.Options.Simplified.pub_sub/1","doc":"","ref":"ExESDB.Options.Simplified.html#pub_sub/1"},{"type":"function","title":"ExESDB.Options.Simplified.reader_idle_ms/1","doc":"","ref":"ExESDB.Options.Simplified.html#reader_idle_ms/1"},{"type":"function","title":"ExESDB.Options.Simplified.set_context/1","doc":"Set application context for configuration lookup","ref":"ExESDB.Options.Simplified.html#set_context/1"},{"type":"function","title":"ExESDB.Options.Simplified.store_description/1","doc":"","ref":"ExESDB.Options.Simplified.html#store_description/1"},{"type":"function","title":"ExESDB.Options.Simplified.store_id/1","doc":"","ref":"ExESDB.Options.Simplified.html#store_id/1"},{"type":"function","title":"ExESDB.Options.Simplified.store_tags/1","doc":"","ref":"ExESDB.Options.Simplified.html#store_tags/1"},{"type":"function","title":"ExESDB.Options.Simplified.timeout/1","doc":"","ref":"ExESDB.Options.Simplified.html#timeout/1"},{"type":"function","title":"ExESDB.Options.Simplified.topologies/0","doc":"","ref":"ExESDB.Options.Simplified.html#topologies/0"},{"type":"function","title":"ExESDB.Options.Simplified.with_context/2","doc":"Execute function within specific application context","ref":"ExESDB.Options.Simplified.html#with_context/2"},{"type":"function","title":"ExESDB.Options.Simplified.writer_idle_ms/1","doc":"","ref":"ExESDB.Options.Simplified.html#writer_idle_ms/1"},{"type":"module","title":"ExESDB.PersistenceSystem","doc":"Supervisor for persistence layer components.\n\nThis supervisor manages all data persistence components that can\noperate independently of each other.\n\nComponents:\n- PersistenceWorker: Asynchronous disk persistence management\n- Streams: Stream read/write operations\n- Snapshots: Snapshot management\n- Subscriptions: Subscription management","ref":"ExESDB.PersistenceSystem.html"},{"type":"function","title":"ExESDB.PersistenceSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.PersistenceSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.PersistenceSystem.start_link/1","doc":"","ref":"ExESDB.PersistenceSystem.html#start_link/1"},{"type":"module","title":"ExESDB.PersistenceWorker","doc":"A GenServer that handles periodic disk persistence operations.\n\nThis worker batches and schedules fence operations to ensure data is\npersisted to disk without blocking event append operations.\n\nFeatures:\n- Configurable persistence interval (default: 5 seconds)\n- Batching of fence operations to reduce disk I/O\n- Graceful shutdown with final persistence\n- Per-store persistence workers","ref":"ExESDB.PersistenceWorker.html"},{"type":"function","title":"ExESDB.PersistenceWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.PersistenceWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.PersistenceWorker.force_persistence/1","doc":"Forces immediate persistence of all pending stores.\nThis is a synchronous call that blocks until persistence is complete.","ref":"ExESDB.PersistenceWorker.html#force_persistence/1"},{"type":"function","title":"ExESDB.PersistenceWorker.request_persistence/1","doc":"Requests that a store's data be persisted to disk.\nThis is a non-blocking call that queues the store for persistence.","ref":"ExESDB.PersistenceWorker.html#request_persistence/1"},{"type":"function","title":"ExESDB.PersistenceWorker.start_link/1","doc":"Starts a persistence worker for a specific store.","ref":"ExESDB.PersistenceWorker.html#start_link/1"},{"type":"module","title":"ExESDB.Scenario.ConfigLoader","doc":"Loads and validates scenario configurations from various sources.\n\nSupports loading from JSON files, YAML files, and Elixir configuration\nfor custom test scenarios.","ref":"ExESDB.Scenario.ConfigLoader.html"},{"type":"function","title":"ExESDB.Scenario.ConfigLoader.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Scenario.ConfigLoader.html#child_spec/1"},{"type":"function","title":"ExESDB.Scenario.ConfigLoader.list_builtin_scenarios/0","doc":"","ref":"ExESDB.Scenario.ConfigLoader.html#list_builtin_scenarios/0"},{"type":"function","title":"ExESDB.Scenario.ConfigLoader.load_config/1","doc":"","ref":"ExESDB.Scenario.ConfigLoader.html#load_config/1"},{"type":"function","title":"ExESDB.Scenario.ConfigLoader.start_link/1","doc":"","ref":"ExESDB.Scenario.ConfigLoader.html#start_link/1"},{"type":"function","title":"ExESDB.Scenario.ConfigLoader.validate_config/1","doc":"","ref":"ExESDB.Scenario.ConfigLoader.html#validate_config/1"},{"type":"module","title":"ExESDB.ScenarioSystem","doc":"Supervisor for the Scenario subsystem.\n\nManages scenario execution, configuration loading, and testing orchestration.","ref":"ExESDB.ScenarioSystem.html"},{"type":"function","title":"ExESDB.ScenarioSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.ScenarioSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.ScenarioSystem.start_link/1","doc":"","ref":"ExESDB.ScenarioSystem.html#start_link/1"},{"type":"module","title":"ExESDB.Snapshots","doc":"The ExESDB Snapshots SubSystem.","ref":"ExESDB.Snapshots.html"},{"type":"function","title":"ExESDB.Snapshots.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Snapshots.html#child_spec/1"},{"type":"function","title":"ExESDB.Snapshots.path/3","doc":"","ref":"ExESDB.Snapshots.html#path/3"},{"type":"function","title":"Description - ExESDB.Snapshots.path/3","doc":"Returns the key for a snapshot as a Khepri Path.","ref":"ExESDB.Snapshots.html#path/3-description"},{"type":"function","title":"Examples - ExESDB.Snapshots.path/3","doc":"iex> ExESDB.Snapshots.path(\"source_uuid\", \"stream_uuid\", 1)\n    [:snapshots, \"source_uuid\", \"stream_uuid\", \"0000000001\"]","ref":"ExESDB.Snapshots.html#path/3-examples"},{"type":"function","title":"ExESDB.Snapshots.start_link/1","doc":"","ref":"ExESDB.Snapshots.html#start_link/1"},{"type":"module","title":"ExESDB.SnapshotsReader","doc":"Provides functions for reading snapshots","ref":"ExESDB.SnapshotsReader.html"},{"type":"function","title":"ExESDB.SnapshotsReader.cluster_id/3","doc":"","ref":"ExESDB.SnapshotsReader.html#cluster_id/3"},{"type":"function","title":"ExESDB.SnapshotsReader.hr_snapshots_reader_name/3","doc":"","ref":"ExESDB.SnapshotsReader.html#hr_snapshots_reader_name/3"},{"type":"function","title":"ExESDB.SnapshotsReader.list_snapshots/3","doc":"","ref":"ExESDB.SnapshotsReader.html#list_snapshots/3"},{"type":"function","title":"ExESDB.SnapshotsReader.read_snapshot/4","doc":"","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4"},{"type":"function","title":"Description - ExESDB.SnapshotsReader.read_snapshot/4","doc":"Reads a snapshot version from the store \n  for the given source and stream uuids","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-description"},{"type":"function","title":"Parameters - ExESDB.SnapshotsReader.read_snapshot/4","doc":"* `store` - the store to read from\n   * `source_uuid` - the source uuid\n   * `stream_uuid` - the stream uuid\n   * `version` - the version of the snapshot to read","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-parameters"},{"type":"function","title":"Returns - ExESDB.SnapshotsReader.read_snapshot/4","doc":"* `{:ok, map()}` - the snapshot","ref":"ExESDB.SnapshotsReader.html#read_snapshot/4-returns"},{"type":"function","title":"ExESDB.SnapshotsReader.start_worker/3","doc":"","ref":"ExESDB.SnapshotsReader.html#start_worker/3"},{"type":"module","title":"ExESDB.SnapshotsReaderPool","doc":"A pool of `ExESDB.SnapshotsReaderWorker` processes.","ref":"ExESDB.SnapshotsReaderPool.html"},{"type":"module","title":"ExESDB.SnapshotsReaderWorker","doc":"A worker process for reading snapshots from the event store.","ref":"ExESDB.SnapshotsReaderWorker.html"},{"type":"function","title":"ExESDB.SnapshotsReaderWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.SnapshotsReaderWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.SnapshotsReaderWorker.start_link/1","doc":"","ref":"ExESDB.SnapshotsReaderWorker.html#start_link/1"},{"type":"module","title":"ExESDB.SnapshotsWriter","doc":"The API for interacting with ExESDB Snapshots Writers.\n It functions as an API for SnapshotsWriterWorkers, \n by requesting a worker from the Cluster. \n If no worker is available for the specific combination of store, source_uuid, and stream_uuid,\n then a new worker is started.","ref":"ExESDB.SnapshotsWriter.html"},{"type":"function","title":"ExESDB.SnapshotsWriter.cluster_id/3","doc":"","ref":"ExESDB.SnapshotsWriter.html#cluster_id/3"},{"type":"function","title":"ExESDB.SnapshotsWriter.delete_snapshot/4","doc":"","ref":"ExESDB.SnapshotsWriter.html#delete_snapshot/4"},{"type":"function","title":"ExESDB.SnapshotsWriter.hr_snapshots_writer_name/3","doc":"","ref":"ExESDB.SnapshotsWriter.html#hr_snapshots_writer_name/3"},{"type":"function","title":"ExESDB.SnapshotsWriter.record_snapshot/5","doc":"","ref":"ExESDB.SnapshotsWriter.html#record_snapshot/5"},{"type":"module","title":"ExESDB.SnapshotsWriterPool","doc":"A pool of `ExESDB.SnapshotsWriterWorker` processes.","ref":"ExESDB.SnapshotsWriterPool.html"},{"type":"module","title":"ExESDB.SnapshotsWriterWorker","doc":"A worker process for writing snapshots to the event store.","ref":"ExESDB.SnapshotsWriterWorker.html"},{"type":"function","title":"ExESDB.SnapshotsWriterWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.SnapshotsWriterWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.SnapshotsWriterWorker.start_link/1","doc":"Starts a new `ExESDB.SnapshotsWriterWorker` process.","ref":"ExESDB.SnapshotsWriterWorker.html#start_link/1"},{"type":"module","title":"ExESDB.Store","doc":"A GenServer wrapper around :khepri to act as a distributed event store.","ref":"ExESDB.Store.html"},{"type":"function","title":"ExESDB.Store.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Store.html#child_spec/1"},{"type":"function","title":"ExESDB.Store.get_state/1","doc":"Get the current state of the store.","ref":"ExESDB.Store.html#get_state/1"},{"type":"function","title":"Returns - ExESDB.Store.get_state/1","doc":"- `{:ok, state}`  if successful.\n    - `{:error, reason}` if unsuccessful.","ref":"ExESDB.Store.html#get_state/1-returns"},{"type":"function","title":"ExESDB.Store.start_link/1","doc":"","ref":"ExESDB.Store.html#start_link/1"},{"type":"function","title":"ExESDB.Store.store_name/1","doc":"Get the store-specific GenServer name.\n\nThis function returns the name used to register this store GenServer,\nallowing multiple stores to run on the same node.","ref":"ExESDB.Store.html#store_name/1"},{"type":"function","title":"Parameters - ExESDB.Store.store_name/1","doc":"* `store_id` - The store identifier (optional)","ref":"ExESDB.Store.html#store_name/1-parameters"},{"type":"function","title":"Examples - ExESDB.Store.store_name/1","doc":"iex> ExESDB.Store.store_name(\"my_store\")\n    {:ex_esdb_store, \"my_store\"}\n    \n    iex> ExESDB.Store.store_name(nil)\n    ExESDB.Store","ref":"ExESDB.Store.html#store_name/1-examples"},{"type":"module","title":"ExESDB.StoreCoordinator","doc":"GenServer responsible for coordinating Khepri cluster formation and preventing split-brain scenarios.\n\nThis module handles:\n- Detecting existing clusters\n- Coordinator election\n- Coordinated cluster joining\n- Split-brain prevention","ref":"ExESDB.StoreCoordinator.html"},{"type":"function","title":"ExESDB.StoreCoordinator.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StoreCoordinator.html#child_spec/1"},{"type":"function","title":"ExESDB.StoreCoordinator.join_cluster/1","doc":"Attempts to join a Khepri cluster using coordinated approach to prevent split-brain.\nReturns one of: :ok, :coordinator, :no_nodes, :waiting, :failed","ref":"ExESDB.StoreCoordinator.html#join_cluster/1"},{"type":"function","title":"ExESDB.StoreCoordinator.should_handle_nodeup?/1","doc":"Checks if this node should handle nodeup events (i.e., not already in a cluster)","ref":"ExESDB.StoreCoordinator.html#should_handle_nodeup?/1"},{"type":"function","title":"ExESDB.StoreCoordinator.start_link/1","doc":"","ref":"ExESDB.StoreCoordinator.html#start_link/1"},{"type":"module","title":"ExESDB.StoreInfo","doc":"This module provides functions to get information about the EXESDB event store.","ref":"ExESDB.StoreInfo.html"},{"type":"function","title":"ExESDB.StoreInfo.get_streams!/1","doc":"Returns the list of streams in the store.","ref":"ExESDB.StoreInfo.html#get_streams!/1"},{"type":"function","title":"ExESDB.StoreInfo.get_streams_raw/1","doc":"","ref":"ExESDB.StoreInfo.html#get_streams_raw/1"},{"type":"module","title":"ExESDB.StoreNaming","doc":"Helper module for creating store-specific GenServer names.\n\nThis module provides utilities for generating store-specific names for GenServers,\nallowing multiple ExESDB instances to run on the same node with different stores.","ref":"ExESDB.StoreNaming.html"},{"type":"module","title":"Usage - ExESDB.StoreNaming","doc":"Instead of using `name: __MODULE__` in GenServer registration, use:\n\n    name: ExESDB.StoreNaming.genserver_name(__MODULE__, store_id)\n\nThis will create store-specific names like `:\"ex_esdb_store_my_store\"` when\na store_id is provided, or fall back to the module name for backward compatibility.","ref":"ExESDB.StoreNaming.html#module-usage"},{"type":"module","title":"Valid Process Names - ExESDB.StoreNaming","doc":"This module ensures that all generated names are valid for use with GenServers,\nSupervisors, and other OTP processes. The generated names are simple atoms that\ncan be registered locally, which is compatible with Elixir's process naming\nrequirements.","ref":"ExESDB.StoreNaming.html#module-valid-process-names"},{"type":"function","title":"ExESDB.StoreNaming.child_spec_id/2","doc":"Generate a store-specific child spec id.\n\nThis function creates unique child spec IDs based on the module and store_id,\nallowing multiple instances of the same supervisor child to run with different stores.","ref":"ExESDB.StoreNaming.html#child_spec_id/2"},{"type":"function","title":"Parameters - ExESDB.StoreNaming.child_spec_id/2","doc":"* `module` - The GenServer module (typically `__MODULE__`)\n* `store_id` - The store identifier (string or atom)","ref":"ExESDB.StoreNaming.html#child_spec_id/2-parameters"},{"type":"function","title":"Examples - ExESDB.StoreNaming.child_spec_id/2","doc":"iex> ExESDB.StoreNaming.child_spec_id(ExESDB.Store, \"my_store\")\n    :\"ex_esdb_store_my_store\"\n    \n    iex> ExESDB.StoreNaming.child_spec_id(ExESDB.Store, nil)\n    ExESDB.Store","ref":"ExESDB.StoreNaming.html#child_spec_id/2-examples"},{"type":"function","title":"ExESDB.StoreNaming.extract_store_id/1","doc":"Extract store_id from options.\n\nThis is a convenience function to extract the store_id from the standard\noptions keyword list passed to GenServers.","ref":"ExESDB.StoreNaming.html#extract_store_id/1"},{"type":"function","title":"Examples - ExESDB.StoreNaming.extract_store_id/1","doc":"iex> ExESDB.StoreNaming.extract_store_id([store_id: \"my_store\", timeout: 5000])\n    \"my_store\"\n    \n    iex> ExESDB.StoreNaming.extract_store_id([timeout: 5000])\n    nil","ref":"ExESDB.StoreNaming.html#extract_store_id/1-examples"},{"type":"function","title":"ExESDB.StoreNaming.genserver_name/2","doc":"Generate a store-specific name for a GenServer.\n\nThis function creates unique GenServer names based on the module and store_id,\nallowing multiple instances of the same GenServer to run with different stores.\n\nThe function returns a valid process name that can be used for GenServers,\nSupervisors, and other OTP processes. When a store_id is provided, it creates\na unique atom by combining the module name with the store_id. When no store_id\nis provided, it falls back to the module name for backward compatibility.","ref":"ExESDB.StoreNaming.html#genserver_name/2"},{"type":"function","title":"Parameters - ExESDB.StoreNaming.genserver_name/2","doc":"* `module` - The GenServer module (typically `__MODULE__`)\n* `store_id` - The store identifier (string or atom)","ref":"ExESDB.StoreNaming.html#genserver_name/2-parameters"},{"type":"function","title":"Examples - ExESDB.StoreNaming.genserver_name/2","doc":"iex> ExESDB.StoreNaming.genserver_name(ExESDB.Store, \"my_store\")\n    :\"ex_esdb_store_my_store\"\n    \n    iex> ExESDB.StoreNaming.genserver_name(ExESDB.Store, nil)\n    ExESDB.Store\n    \n    iex> ExESDB.StoreNaming.genserver_name(ExESDB.LeaderWorker, \"cluster_store\")\n    :\"ex_esdb_leader_worker_cluster_store\"","ref":"ExESDB.StoreNaming.html#genserver_name/2-examples"},{"type":"function","title":"ExESDB.StoreNaming.partition_name/2","doc":"Generate a store-specific name for partition supervisors like StreamsWriters, etc.\n\nThis function creates unique names for global resources that would otherwise conflict\nbetween multiple ExESDB instances.","ref":"ExESDB.StoreNaming.html#partition_name/2"},{"type":"function","title":"Parameters - ExESDB.StoreNaming.partition_name/2","doc":"* `base_name` - The base name atom (e.g., ExESDB.StreamsWriters)\n* `store_id` - The store identifier (string or atom)","ref":"ExESDB.StoreNaming.html#partition_name/2-parameters"},{"type":"function","title":"Examples - ExESDB.StoreNaming.partition_name/2","doc":"iex> ExESDB.StoreNaming.partition_name(ExESDB.StreamsWriters, \"my_store\")\n    :\"exesdb_streamswriters_my_store\"\n    \n    iex> ExESDB.StoreNaming.partition_name(ExESDB.StreamsWriters, nil)\n    ExESDB.StreamsWriters","ref":"ExESDB.StoreNaming.html#partition_name/2-examples"},{"type":"module","title":"ExESDB.StoreSystem","doc":"Supervisor for store-related components.\n\nThis supervisor manages the store lifecycle and clustering components.\nUses :rest_for_one strategy to ensure proper startup order.\n\nStartup order (critical for distributed coordination):\n1. Store: Core store GenServer - must be fully operational first\n2. StoreCluster: Clustering coordination - depends on Store being ready\n3. StoreRegistry: Distributed store registry - starts after Store system is stable\n\nThis order ensures StoreRegistry only announces a store that is actually ready\nto handle requests, preventing race conditions in distributed environments.","ref":"ExESDB.StoreSystem.html"},{"type":"function","title":"ExESDB.StoreSystem.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StoreSystem.html#child_spec/1"},{"type":"function","title":"ExESDB.StoreSystem.start_link/1","doc":"","ref":"ExESDB.StoreSystem.html#start_link/1"},{"type":"module","title":"ExESDB.StoreWorker","doc":"A GenServer wrapper around :khepri to act as a distributed event store.\n  Inspired by EventStoreDB's API.","ref":"ExESDB.StoreWorker.html"},{"type":"function","title":"ExESDB.StoreWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StoreWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.StoreWorker.get_state/1","doc":"Get the current state of the store.","ref":"ExESDB.StoreWorker.html#get_state/1"},{"type":"function","title":"Returns - ExESDB.StoreWorker.get_state/1","doc":"- `{:ok, state}`  if successful.\n    - `{:error, reason}` if unsuccessful.","ref":"ExESDB.StoreWorker.html#get_state/1-returns"},{"type":"function","title":"ExESDB.StoreWorker.start_link/1","doc":"","ref":"ExESDB.StoreWorker.html#start_link/1"},{"type":"module","title":"ExESDB.Streams","doc":"The ExESDB Streams SubSystem.","ref":"ExESDB.Streams.html"},{"type":"function","title":"ExESDB.Streams.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Streams.html#child_spec/1"},{"type":"function","title":"ExESDB.Streams.start_link/1","doc":"","ref":"ExESDB.Streams.html#start_link/1"},{"type":"module","title":"ExESDB.StreamsHelper","doc":"Provides helper functions for working with event store streams.","ref":"ExESDB.StreamsHelper.html"},{"type":"function","title":"ExESDB.StreamsHelper.calculate_versions/3","doc":"","ref":"ExESDB.StreamsHelper.html#calculate_versions/3"},{"type":"function","title":"ExESDB.StreamsHelper.get_version!/2","doc":"Returns the version of the stream using 0-based indexing.","ref":"ExESDB.StreamsHelper.html#get_version!/2"},{"type":"function","title":"Parameters - ExESDB.StreamsHelper.get_version!/2","doc":"- `store` is the name of the store.\n   - `stream_id` is the name of the stream.","ref":"ExESDB.StreamsHelper.html#get_version!/2-parameters"},{"type":"function","title":"Returns - ExESDB.StreamsHelper.get_version!/2","doc":"- `version` (0-based) or `-1` if the stream does not exist.\n   This means:\n   - New stream (no events): -1\n   - Stream with 1 event: 0 (version of latest event)\n   - Stream with 2 events: 1 (version of latest event)\n   - etc.","ref":"ExESDB.StreamsHelper.html#get_version!/2-returns"},{"type":"function","title":"ExESDB.StreamsHelper.pad_version/2","doc":"","ref":"ExESDB.StreamsHelper.html#pad_version/2"},{"type":"function","title":"ExESDB.StreamsHelper.stream_exists?/2","doc":"","ref":"ExESDB.StreamsHelper.html#stream_exists?/2"},{"type":"function","title":"ExESDB.StreamsHelper.to_event_record/5","doc":"","ref":"ExESDB.StreamsHelper.html#to_event_record/5"},{"type":"function","title":"ExESDB.StreamsHelper.version_to_integer/1","doc":"","ref":"ExESDB.StreamsHelper.html#version_to_integer/1"},{"type":"module","title":"ExESDB.StreamsReader","doc":"This module is responsible for reading events from a stream.","ref":"ExESDB.StreamsReader.html"},{"type":"function","title":"ExESDB.StreamsReader.get_streams/1","doc":"Returns a list of all streams in the store.","ref":"ExESDB.StreamsReader.html#get_streams/1"},{"type":"function","title":"Parameters - ExESDB.StreamsReader.get_streams/1","doc":"- `store` is the name of the store.","ref":"ExESDB.StreamsReader.html#get_streams/1-parameters"},{"type":"function","title":"Returns - ExESDB.StreamsReader.get_streams/1","doc":"- `{:ok, streams}`  if successful.","ref":"ExESDB.StreamsReader.html#get_streams/1-returns"},{"type":"function","title":"ExESDB.StreamsReader.stream_events/5","doc":"Streams events from `stream` in batches of `count` events, in a `direction`.","ref":"ExESDB.StreamsReader.html#stream_events/5"},{"type":"function","title":"ExESDB.StreamsReader.worker_id/2","doc":"","ref":"ExESDB.StreamsReader.html#worker_id/2"},{"type":"module","title":"ExESDB.StreamsReaderPool","doc":"As part of the ExESDB.System,","ref":"ExESDB.StreamsReaderPool.html"},{"type":"function","title":"ExESDB.StreamsReaderPool.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StreamsReaderPool.html#child_spec/1"},{"type":"module","title":"ExESDB.StreamsReaderWorker","doc":"Provides functions for reading and streaming events from the event store.","ref":"ExESDB.StreamsReaderWorker.html"},{"type":"function","title":"ExESDB.StreamsReaderWorker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StreamsReaderWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.StreamsReaderWorker.start_link/1","doc":"","ref":"ExESDB.StreamsReaderWorker.html#start_link/1"},{"type":"module","title":"ExESDB.StreamsWriter","doc":"This module is responsible for writing events to a stream.\n  It is actually an API style wrapper around the StreamsWriterWorker.","ref":"ExESDB.StreamsWriter.html"},{"type":"function","title":"ExESDB.StreamsWriter.append_events/4","doc":"","ref":"ExESDB.StreamsWriter.html#append_events/4"},{"type":"function","title":"ExESDB.StreamsWriter.hr_worker_id_atom/2","doc":"","ref":"ExESDB.StreamsWriter.html#hr_worker_id_atom/2"},{"type":"function","title":"ExESDB.StreamsWriter.worker_id/2","doc":"","ref":"ExESDB.StreamsWriter.html#worker_id/2"},{"type":"module","title":"ExESDB.StreamsWriterPool","doc":"As part of the ExESDB.System,","ref":"ExESDB.StreamsWriterPool.html"},{"type":"function","title":"ExESDB.StreamsWriterPool.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.StreamsWriterPool.html#child_spec/1"},{"type":"module","title":"ExESDB.StreamsWriterWorker","doc":"Provides functions for writing streams","ref":"ExESDB.StreamsWriterWorker.html"},{"type":"function","title":"ExESDB.StreamsWriterWorker.child_spec/1","doc":"Returns a child spec for a streams writer worker.\n  Please note that the restart strategy is set to `:temporary`\n  to avoid restarting the worker when the idle timeout is reached.","ref":"ExESDB.StreamsWriterWorker.html#child_spec/1"},{"type":"function","title":"ExESDB.StreamsWriterWorker.start_link/1","doc":"","ref":"ExESDB.StreamsWriterWorker.html#start_link/1"},{"type":"module","title":"ExESDB.Subscriptions","doc":"Provides functions for working with event store subscriptions.","ref":"ExESDB.Subscriptions.html"},{"type":"function","title":"ExESDB.Subscriptions.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.Subscriptions.html#child_spec/1"},{"type":"function","title":"ExESDB.Subscriptions.start_link/1","doc":"","ref":"ExESDB.Subscriptions.html#start_link/1"},{"type":"module","title":"ExESDB.SubscriptionsReader","doc":"Provides functions for working with event store subscriptions.","ref":"ExESDB.SubscriptionsReader.html"},{"type":"function","title":"ExESDB.SubscriptionsReader.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.SubscriptionsReader.html#child_spec/1"},{"type":"function","title":"ExESDB.SubscriptionsReader.get_subscriptions/1","doc":"","ref":"ExESDB.SubscriptionsReader.html#get_subscriptions/1"},{"type":"function","title":"ExESDB.SubscriptionsReader.start_link/1","doc":"","ref":"ExESDB.SubscriptionsReader.html#start_link/1"},{"type":"module","title":"ExESDB.SubscriptionsTracker","doc":"As part of the ExESDB.System, the SubscriptionsTracker is responsible for\n  observing the subscriptions that are maintained in the Store.\n\n  Since Khepri triggers are executed on the leader node, the SubscriptionsTracker\n  will be instructed to start the Emitters system on the leader node whenever a new subscription\n  is registered.\n\n  When a Subscription is deleted, the SubscriptionsTracker will instruct the Emitters system to stop \n  the associated EmitterPool.","ref":"ExESDB.SubscriptionsTracker.html"},{"type":"function","title":"ExESDB.SubscriptionsTracker.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.SubscriptionsTracker.html#child_spec/1"},{"type":"function","title":"ExESDB.SubscriptionsTracker.start_link/1","doc":"","ref":"ExESDB.SubscriptionsTracker.html#start_link/1"},{"type":"module","title":"ExESDB.SubscriptionsWriter","doc":"Provides functions for working with event store subscriptions.","ref":"ExESDB.SubscriptionsWriter.html"},{"type":"function","title":"ExESDB.SubscriptionsWriter.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.SubscriptionsWriter.html#child_spec/1"},{"type":"function","title":"ExESDB.SubscriptionsWriter.delete_subscription/4","doc":"","ref":"ExESDB.SubscriptionsWriter.html#delete_subscription/4"},{"type":"function","title":"ExESDB.SubscriptionsWriter.put_subscription/6","doc":"","ref":"ExESDB.SubscriptionsWriter.html#put_subscription/6"},{"type":"function","title":"ExESDB.SubscriptionsWriter.start_link/1","doc":"","ref":"ExESDB.SubscriptionsWriter.html#start_link/1"},{"type":"module","title":"ExESDB.System","doc":"This module is the top level supervisor for the ExESDB system.\n  \n  It uses a layered supervision architecture for better fault tolerance:\n  \n  SINGLE NODE MODE:\n  1. CoreSystem: Critical infrastructure (PersistenceSystem + NotificationSystem + StoreSystem)\n  2. GatewaySystem: External interface with pooled workers\n  \n  CLUSTER MODE:\n  1. CoreSystem: Critical infrastructure (PersistenceSystem + NotificationSystem + StoreSystem)\n  2. LibCluster: Node discovery and connection (after core is ready)\n  3. ClusterSystem: Cluster coordination and membership\n  4. GatewaySystem: External interface (LAST - only after clustering is ready)\n  \n  NotificationSystem (part of CoreSystem) includes:\n  - LeaderSystem: Leadership responsibilities and subscription management\n  - EmitterSystem: Event emission and distribution\n  \n  IMPORTANT: Core functionality (Store, Persistence) must be fully operational \n  before any clustering/membership/registration components start. This ensures \n  the server is ready to handle requests before announcing itself to the cluster.\n  \n  In cluster mode, GatewaySystem starts LAST to prevent external connections\n  until the entire distributed system is properly initialized.\n  \n  Note: Store management is now handled by the distributed ex-esdb-gater API.","ref":"ExESDB.System.html"},{"type":"function","title":"ExESDB.System.child_spec/1","doc":"Returns a specification to start this module under a supervisor.\n\nSee `Supervisor`.","ref":"ExESDB.System.html#child_spec/1"},{"type":"function","title":"ExESDB.System.start/1","doc":"","ref":"ExESDB.System.html#start/1"},{"type":"function","title":"ExESDB.System.start_link/1","doc":"Start ExESDB.System with automatic configuration discovery.\n\nThis function supports several modes:\n1. With explicit opts: `ExESDB.System.start_link(opts)` - uses provided configuration\n2. With OTP app name: `ExESDB.System.start_link(:my_app)` - discovers config from specified app\n3. With empty opts or no args: `ExESDB.System.start_link([])` or `ExESDB.System.start_link()` - discovers config from calling application","ref":"ExESDB.System.html#start_link/1"},{"type":"function","title":"Examples - ExESDB.System.start_link/1","doc":"# Explicit configuration (current approach)\n    opts = ExESDB.Options.app_env(:my_app)\n    ExESDB.System.start_link(opts)\n\n    # Auto-discovery from specific app\n    ExESDB.System.start_link(:my_app)\n\n    # Auto-discovery from calling application\n    ExESDB.System.start_link()","ref":"ExESDB.System.html#start_link/1-examples"},{"type":"function","title":"ExESDB.System.stop/1","doc":"","ref":"ExESDB.System.html#stop/1"},{"type":"function","title":"ExESDB.System.system_name/1","doc":"Generate a store-specific name for this system supervisor.\n\nThis allows multiple ExESDB systems to run on the same node with different stores.","ref":"ExESDB.System.html#system_name/1"},{"type":"function","title":"Examples - ExESDB.System.system_name/1","doc":"iex> ExESDB.System.system_name(\"my_store\")\n    :\"exesdb_system_my_store\"\n    \n    iex> ExESDB.System.system_name(nil)\n    ExESDB.System","ref":"ExESDB.System.html#system_name/1-examples"},{"type":"module","title":"ExESDB.Topics","doc":"A module to calculate topic identifiers","ref":"ExESDB.Topics.html"},{"type":"function","title":"ExESDB.Topics.selector_hash/1","doc":"","ref":"ExESDB.Topics.html#selector_hash/1"},{"type":"function","title":"ExESDB.Topics.sub_topic/3","doc":"","ref":"ExESDB.Topics.html#sub_topic/3"},{"type":"module","title":"ExESDB.UmbrellaHelper","doc":"Helper module for starting ExESDB with umbrella configuration patterns.\n\nThis module provides functions to start ExESDB with app-specific configurations\nwhen used in umbrella applications.","ref":"ExESDB.UmbrellaHelper.html"},{"type":"function","title":"ExESDB.UmbrellaHelper.child_spec/1","doc":"Start ExESDB with configuration from a specific OTP app.\n\nThis function configures ExESDB to read its configuration from the specified\nOTP app instead of the global :ex_esdb application.","ref":"ExESDB.UmbrellaHelper.html#child_spec/1"},{"type":"function","title":"Parameters - ExESDB.UmbrellaHelper.child_spec/1","doc":"* `otp_app` - The OTP application name to read configuration from","ref":"ExESDB.UmbrellaHelper.html#child_spec/1-parameters"},{"type":"function","title":"Examples - ExESDB.UmbrellaHelper.child_spec/1","doc":"# In your reckon app's application.ex:\n    def start(_type, _args) do\n      children = [\n        # Other children...\n        ExESDB.UmbrellaHelper.child_spec(:reckon_accounts)\n      ]\n      \n      Supervisor.start_link(children, strategy: :one_for_one)\n    end","ref":"ExESDB.UmbrellaHelper.html#child_spec/1-examples"},{"type":"function","title":"ExESDB.UmbrellaHelper.configure/1","doc":"Configure ExESDB to use the specified OTP app for configuration.\n\nThis function sets up ExESDB to use app-specific configuration by\nsetting the :otp_app configuration key.","ref":"ExESDB.UmbrellaHelper.html#configure/1"},{"type":"function","title":"ExESDB.UmbrellaHelper.start_link/1","doc":"Start ExESDB with configuration from a specific OTP app.\n\nThis is a convenience function that directly starts the ExESDB system\nwith the specified OTP app configuration.","ref":"ExESDB.UmbrellaHelper.html#start_link/1"},{"type":"extras","title":"Architecture Decision Records","doc":"# Architecture Decision Records","ref":"adr.html"},{"type":"extras","title":"2025.05.24 - Architecture Decision Records","doc":"","ref":"adr.html#2025-05-24"},{"type":"extras","title":"Emitter pools must run on the Ra leader. - Architecture Decision Records","doc":"Given that Khepri triggers are executed on the Ra leader, emitter pools must run on the Ra leader.\nIn order to achieve this, it is necessary that a separate process exists, that monitors the Cluster and subscribes to Ra's leadership changes, by starting relevant emitter pools on the new leader.","ref":"adr.html#emitter-pools-must-run-on-the-ra-leader"},{"type":"extras","title":"2025.05.02 - Architecture Decision Records","doc":"","ref":"adr.html#2025-05-02"},{"type":"extras","title":"Triggers will obtain Emitter Pids from :pg (Process Groups) - Architecture Decision Records","doc":"Experimentation with :khepri's trigger model has shown that it is advised to use as few dependencies as possible, when defining the trigger functions. This is because :khepri uses :horus to decompile such functions and build a separate module. For this reason, we limit the dependencies for these trigger functions to :pg (Process Groups), which is an :erlang native module that allows for inter-process communication.","ref":"adr.html#triggers-will-obtain-emitter-pids-from-pg-process-groups"},{"type":"extras","title":"2025.04.16 - Architecture Decision Records","doc":"","ref":"adr.html#2025-04-16"},{"type":"extras","title":"Subscriptions will be managed via a registry. - Architecture Decision Records","doc":"`subscribe_to` and `unsubscribe` will interact with the :subscriptions branch of the store, but instead of storing the pid of the subscriber, the subscriber will be stored in a registry.","ref":"adr.html#subscriptions-will-be-managed-via-a-registry"},{"type":"extras","title":"2025.04.13 - Architecture Decision Records","doc":"","ref":"adr.html#2025-04-13"},{"type":"extras","title":"Each Store will contain separate branches for Streams, Subscriptions, and Projections - Architecture Decision Records","doc":"- `:streams` will be used to store the events that are being read from and written to the store.\n- `:subscriptions` will be used to store the subscription information of so-called `Persistent Subscriptions`.\n- `:projections` can best be thought of as stored procedures that are used to transform the events in the `Streams` into a different format or to enrich the streams with secondary or derived events, to name a few possible use cases.\n\nThus:\n\n```mono\n:khepri\n  |\n  +-:manage_orders\n      |\n      +-:streams\n      |\n      +-:subscriptions\n      |\n      +-:projections\n```","ref":"adr.html#each-store-will-contain-separate-branches-for-streams-subscriptions-and-projections"},{"type":"extras","title":"Changelog","doc":"# Changelog","ref":"changelog.html"},{"type":"extras","title":"version 0.4.9 (2025.07.19) - Changelog","doc":"","ref":"changelog.html#version-0-4-9-2025-07-19"},{"type":"extras","title":"Comprehensive Debugging and Monitoring System - Changelog","doc":"#### New Feature: ExESDB.Debugger\n\n- **REPL-friendly debugging tool**: Added comprehensive debugging and inspection module for ExESDB systems\n- **Real-time system monitoring**: Complete visibility into processes, performance, and system health\n- **Production-safe operations**: All debugging functions are read-only and safe for live systems\n- **Auto-discovery capabilities**: Automatically detects stores and configuration without manual setup\n\n#### Core Debugging Functions\n\n- **System Overview**: `ExESDB.Debugger.overview()` - Complete system status at a glance\n- **Process Management**: `ExESDB.Debugger.processes()` - List and inspect all ExESDB processes\n- **Supervision Tree**: `ExESDB.Debugger.supervision_tree()` - Visual process hierarchy inspection\n- **Configuration Analysis**: `ExESDB.Debugger.config()` - Shows current config with sources (env vs app config)\n- **Health Monitoring**: `ExESDB.Debugger.health()` - Comprehensive system health validation\n\n#### Data Investigation Tools\n\n- **Stream Inspection**: `ExESDB.Debugger.streams()` - List all streams in the system\n- **Event Browsing**: `ExESDB.Debugger.events(stream_id, opts)` - Browse events within specific streams\n- **Subscription Monitoring**: `ExESDB.Debugger.subscriptions()` - Track active subscriptions\n- **Emitter Pool Management**: `ExESDB.Debugger.emitters()` - Monitor emitter pools and workers\n\n#### Performance and Monitoring\n\n- **Performance Metrics**: `ExESDB.Debugger.performance()` - System memory, CPU, uptime statistics\n- **Resource Analysis**: `ExESDB.Debugger.top()` - Identify resource-heavy processes\n- **Observer Integration**: `ExESDB.Debugger.observer()` - Launch Erlang Observer GUI\n- **Memory Tracking**: Detailed memory usage analysis across all components\n\n#### Advanced Debugging Tools\n\n- **Function Tracing**: `ExESDB.Debugger.trace(module, function)` - Trace specific function calls\n- **Benchmarking**: `ExESDB.Debugger.benchmark(fun)` - Measure function performance\n- **Multi-Store Support**: All functions work seamlessly with multiple store configurations\n- **Built-in Help System**: `ExESDB.Debugger.help()` - Interactive command reference\n\n#### Technical Implementation\n\n- **Production-ready**: Comprehensive error handling with graceful degradation\n- **Rich output formatting**: Human-readable output with emojis and structured information\n- **Zero configuration**: Auto-discovery of stores and system configuration\n- **Integration libraries**: Added `:recon ~> 2.5` dependency for enhanced process inspection\n\n#### Documentation and Examples\n\n- **Comprehensive guide**: New `guides/debugging.md` with complete usage examples\n- **Interactive examples**: Built-in help system with real-world usage patterns\n- **Development workflow**: Step-by-step debugging procedures for common scenarios\n- **Production troubleshooting**: Safe investigation procedures for live systems\n\n#### Benefits\n\n- **Faster development**: Quickly understand system state and identify issues\n- **Enhanced operations**: Real-time monitoring and health checking capabilities\n- **Better debugging**: Trace function calls and analyze performance bottlenecks\n- **Learning tool**: Visual representation of ExESDB architecture and process relationships\n- **Multi-environment**: Works in development, testing, and production environments","ref":"changelog.html#comprehensive-debugging-and-monitoring-system"},{"type":"extras","title":"version 0.3.5 (2025.07.18) - Changelog","doc":"","ref":"changelog.html#version-0-3-5-2025-07-18"},{"type":"extras","title":"Complete Writer Modernization - Changelog","doc":"#### Problem Resolution\n\n- **Eliminated remaining fence operations**: Completed modernization of all writer components to use asynchronous persistence\n- **Consistent performance**: All write operations now benefit from the same non-blocking persistence pattern\n- **Removed synchronous bottlenecks**: No more blocking fence operations throughout the entire system\n\n#### Updated Components\n\n- **ExESDB.SubscriptionsWriter**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n- **ExESDB.SnapshotsWriterWorker**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n- **subscriptions_store.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n- **subscriptions_procs.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n- **streams_procs.erl**: Replaced synchronous `khepri:fence(Store)` with asynchronous `'Elixir.ExESDB.PersistenceWorker':request_persistence(Store)`\n\n#### Technical Implementation\n\n- **Unified persistence pattern**: All writers now use the same asynchronous persistence mechanism\n- **Elixir-Erlang interoperability**: Erlang modules call Elixir PersistenceWorker using proper atom syntax\n- **Consistent behavior**: All write operations follow the same pattern: immediate memory write + async persistence request\n- **Complete fence removal**: No synchronous fence operations remain in the writer layer\n\n#### Benefits\n\n- **System-wide performance**: All write operations now complete in milliseconds instead of seconds\n- **Eliminated timeout risks**: No more 5-second timeout failures across any writer component\n- **Consistent user experience**: All write operations provide immediate feedback\n- **Maintained durability**: Data persistence still guaranteed through background worker\n- **Reduced complexity**: Single persistence mechanism across all writers","ref":"changelog.html#complete-writer-modernization"},{"type":"extras","title":"version 0.3.4 (2025.07.18) - Changelog","doc":"","ref":"changelog.html#version-0-3-4-2025-07-18"},{"type":"extras","title":"Asynchronous Persistence System - Changelog","doc":"#### Problem Resolution\n\n- **Eliminated timeout failures**: Resolved 5-second timeout issues in `append_events` operations that were caused by synchronous fence operations\n- **Improved performance**: Event append operations now complete in ~10-50ms instead of 5+ seconds\n- **Enhanced user experience**: System no longer appears unresponsive during data-heavy operations\n\n#### New Components\n\n- **ExESDB.PersistenceWorker**: New GenServer that handles disk persistence operations asynchronously\n  - Configurable persistence intervals (default: 5 seconds)\n  - Batching of persistence requests for efficiency\n  - Non-blocking API for event operations\n  - Graceful shutdown with final persistence\n  - Comprehensive error handling and logging\n\n#### Updated Components\n\n- **ExESDB.PersistenceSystem**: Enhanced to include PersistenceWorker as a managed component\n- **ExESDB.StreamsWriterWorker**: Replaced synchronous `:khepri.fence(store)` with asynchronous `ExESDB.PersistenceWorker.request_persistence(store)`\n\n#### New APIs\n\n- **`ExESDB.PersistenceWorker.request_persistence(store_id)`**: Non-blocking call to request persistence\n- **`ExESDB.PersistenceWorker.force_persistence(store_id)`**: Blocking call for immediate persistence (useful for testing)\n\n#### Configuration Options\n\n- **Global configuration**: `config :ex_esdb, persistence_interval: 10_000`\n- **Per-store configuration**: `opts = [store_id: :my_store, persistence_interval: 5_000]`\n\n#### Technical Implementation\n\n- **Asynchronous persistence**: Events are written to memory immediately, persistence happens in background\n- **Batching optimization**: Multiple persistence requests are deduplicated and processed together\n- **Eventual consistency**: Data is eventually persisted (within persistence interval)\n- **Fault tolerance**: System continues operating even if persistence is delayed\n\n#### Benefits\n\n- **Immediate response**: Event append operations return instantly\n- **Better throughput**: Batched disk operations are more efficient\n- **Configurable intervals**: Persistence frequency can be tuned per environment\n- **Backward compatibility**: All existing APIs continue to work unchanged\n- **No breaking changes**: Optional configuration with sensible defaults\n\n#### Documentation\n\n- **New guide**: Added comprehensive `guides/persistence_architecture.md` guide\n- **Implementation details**: Complete architecture overview and migration notes\n- **Testing guidance**: Instructions for integration tests using `force_persistence`","ref":"changelog.html#asynchronous-persistence-system"},{"type":"extras","title":"version 0.3.3 (2025.07.18) - Changelog","doc":"","ref":"changelog.html#version-0-3-3-2025-07-18"},{"type":"extras","title":"Data Persistence and Durability Enhancement - Changelog","doc":"#### Khepri Fence Operations Implementation\n\n- **Enhanced data durability**: Added `fence` operations after all Khepri write operations to ensure data persistence to disk\n- **Guaranteed write consistency**: All write operations now block until data is committed and persisted through Ra's write-ahead logging\n- **Improved reliability**: Prevents data loss in case of system crashes or power failures\n\n#### Updated Components\n\n- **ExESDB.SubscriptionsWriter**: Added fence operations after direct `:khepri.delete!` calls and indirect subscription store operations\n- **ExESDB.SnapshotsWriterWorker**: Added fence operations after `:khepri.delete!` and `:khepri.put!` operations for snapshot management\n- **ExESDB.StreamsWriterWorker**: Added fence operations after `:khepri.put!` calls in event recording operations\n- **subscriptions_store.erl**: Added fence operations after all `khepri:put`, `khepri:update`, and `khepri:delete` operations\n- **streams_procs.erl**: Added fence operations after `khepri:put` operations for event procedure registration\n- **subscriptions_procs.erl**: Added fence operations after `khepri:put` operations for subscription procedure registration\n\n#### Technical Implementation\n\n- **Write-through persistence**: All write operations now call `:khepri.fence(store)` immediately after data modification\n- **Consistency guarantees**: Fence operations ensure that subsequent reads will see the results of all previous writes\n- **Durability assurance**: Data is guaranteed to be persisted to disk before write operations complete\n- **Performance consideration**: Fence operations provide strong consistency at the cost of slightly increased write latency\n\n#### Benefits\n\n- **Data integrity**: Eliminates risk of data loss during system failures\n- **Consistency guarantees**: Ensures all write operations are properly persisted before completion\n- **Improved reliability**: Provides stronger durability guarantees for critical event store operations\n- **Operational confidence**: Reduces risk of data corruption in production environments","ref":"changelog.html#data-persistence-and-durability-enhancement"},{"type":"extras","title":"version 0.3.1 (2025.07.17) - Changelog","doc":"","ref":"changelog.html#version-0-3-1-2025-07-17"},{"type":"extras","title":"LeaderWorker Registration Fix - Changelog","doc":"#### Problem Resolution\n\n- **Fixed LeaderWorker registration warnings**: Resolved warning messages \"LeaderWorker registration issue: expected #PID<...>, got nil\"\n- **Store-specific registration verification**: LeaderWorker now properly verifies its registration using the store-specific name instead of the hardcoded module name\n- **Improved logging**: Registration confirmation logs now show the actual store-specific process name\n\n#### Technical Implementation\n\n- **Updated init/1 function**: LeaderWorker now extracts store_id from config and uses `StoreNaming.genserver_name/2` to determine the expected registration name\n- **Correct registration check**: Uses `Process.whereis(expected_name)` instead of `Process.whereis(__MODULE__)` for verification\n- **Enhanced logging**: Log messages now show the actual store-specific name used for registration\n\n#### Benefits\n\n- **Cleaner logs**: Eliminates confusing registration warning messages during startup\n- **Better debugging**: Registration verification now works correctly with store-specific naming\n- **Improved reliability**: Proper registration verification helps identify actual process registration issues","ref":"changelog.html#leaderworker-registration-fix"},{"type":"extras","title":"version 0.3.0 (2025.07.17) - Changelog","doc":"","ref":"changelog.html#version-0-3-0-2025-07-17"},{"type":"extras","title":"Multiple Stores Naming Conflicts Fix - Changelog","doc":"#### Problem Resolution\n\n- **Fixed naming conflicts**: Resolved `already started` errors when multiple umbrella applications attempted to start their own ExESDB systems\n- **Store isolation**: Each store now has its own isolated set of partition supervisors\n- **Multi-tenancy support**: Different applications can maintain completely separate event stores within the same Elixir node\n\n#### Updated Components\n\n- **ExESDB.Emitters**: Updated `start_emitter_pool/3` to use store-specific partition names\n- **ExESDB.EmitterSystem**: Updated supervisor child spec to use store-specific partition names\n- **ExESDB.GatewaySystem**: Updated supervisor child spec to use store-specific partition names\n- **ExESDB.LeaderWorker**: Updated process lookup to use store-specific partition names\n- **ExESDB.Snapshots**: Updated supervisor child specs to use store-specific partition names\n- **ExESDB.SnapshotsReader**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.SnapshotsWriter**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.Streams**: Updated supervisor child specs to use store-specific partition names\n- **ExESDB.StreamsReader**: Updated `start_child/2` to use store-specific partition names\n- **ExESDB.StreamsWriter**: Updated `start_child/2` to use store-specific partition names\n\n#### Technical Implementation\n\n- **Store-specific naming**: All modules now use `ExESDB.StoreNaming.partition_name/2` to generate unique process names\n- **Unique process names**: Each store gets its own partition supervisors (e.g., `:exesdb_streamswriters_store_one`, `:exesdb_streamswriters_store_two`)\n- **Backward compatibility**: Existing single-store deployments continue to work unchanged\n- **Consistent pattern**: All partition supervisors follow the same naming convention\n\n#### Affected Processes\n\n- **ExESDB.StreamsWriters**: Now store-specific to prevent conflicts\n- **ExESDB.StreamsReaders**: Now store-specific to prevent conflicts\n- **ExESDB.SnapshotsWriters**: Now store-specific to prevent conflicts\n- **ExESDB.SnapshotsReaders**: Now store-specific to prevent conflicts\n- **ExESDB.EmitterPools**: Now store-specific to prevent conflicts\n- **ExESDB.GatewayWorkers**: Now store-specific to prevent conflicts\n\n#### Benefits\n\n- **True multi-tenancy**: Multiple applications can run separate ExESDB systems without conflicts\n- **Better isolation**: Each store operates independently with its own resources\n- **Improved reliability**: Eliminates startup failures due to naming conflicts\n- **Enhanced scalability**: Supports complex umbrella application architectures\n- **Maintained compatibility**: Zero breaking changes for existing deployments","ref":"changelog.html#multiple-stores-naming-conflicts-fix"},{"type":"extras","title":"version 0.1.7 (2025.07.16) - Changelog","doc":"","ref":"changelog.html#version-0-1-7-2025-07-16"},{"type":"extras","title":"Configuration System Modernization - Changelog","doc":"#### Legacy Configuration Removal\n\n- **Removed khepri configuration**: Eliminated legacy `config :ex_esdb, :khepri` configuration format\n- **Modernized Options module**: Updated `ExESDB.Options` to use umbrella-aware configuration patterns\n- **Consistent configuration format**: All configurations now use `config :your_app_name, :ex_esdb, [options]` format\n- **Improved isolation**: Better configuration isolation between applications in umbrella projects\n\n#### Configuration Documentation\n\n- **New configuration guide**: Added comprehensive `guides/configuring_exesdb_apps.md` guide\n- **Standalone application examples**: Complete configuration examples for single-app deployments\n- **Umbrella application examples**: Detailed examples for multi-app umbrella projects\n- **Environment variable documentation**: Complete reference for all environment variable overrides\n- **Migration guide**: Instructions for migrating from legacy khepri configuration\n\n#### Configuration Features\n\n- **Context management**: Enhanced context switching for umbrella applications\n- **Explicit context support**: Added `app_env/3` functions for explicit context usage\n- **Context wrapper support**: Added `with_context/2` for scoped configuration access\n- **Libcluster integration**: Emphasized libcluster usage over deprecated seed_nodes mechanism\n\n#### Benefits\n\n- **Better umbrella support**: Proper configuration isolation for umbrella applications\n- **Clearer configuration patterns**: Consistent `config :app_name, :ex_esdb` format\n- **Improved developer experience**: Comprehensive documentation and examples\n- **Enhanced maintainability**: Removed legacy code paths and simplified configuration logic","ref":"changelog.html#configuration-system-modernization"},{"type":"extras","title":"version 0.1.2 (2025.07.14) - Changelog","doc":"","ref":"changelog.html#version-0-1-2-2025-07-14"},{"type":"extras","title":"Store Configuration Enhancement - Changelog","doc":"#### New Environment Variables\n\n- **`EX_ESDB_STORE_DESCRIPTION`**: Human-readable description of the store for documentation and operational purposes\n- **`EX_ESDB_STORE_TAGS`**: Comma-separated tags for store categorization and filtering (e.g., \"production,cluster,core\")\n\n#### Rich Store Configuration\n\n- **Operational Metadata**: Added `created_at`, `version`, and `status` fields to store configuration\n- **Resource Management**: Added `priority` and `auto_start` fields for resource allocation control\n- **Administrative Info**: Enhanced store registry with `tags`, `environment`, and `description` fields\n- **Enhanced Querying**: Store registry now supports filtering by tags, environment, priority, and other metadata\n\n#### Configuration Integration\n\n- **Runtime Configuration**: New environment variables integrated into `config/runtime.exs`\n- **Options System**: Added parsers in `ExESDB.Options` with comma-separated tag parsing\n- **Store Registry**: Enhanced `build_store_config/1` to include all new metadata fields\n- **Development Environment**: Updated `dev-env/*.yaml` files with new environment variables","ref":"changelog.html#store-configuration-enhancement"},{"type":"extras","title":"Architectural Refactoring - Changelog","doc":"#### NotificationSystem Introduction\n\n- **New Core Component**: Created `ExESDB.NotificationSystem` as a core supervisor for event notification\n- **Leadership Integration**: Moved `LeaderSystem` from clustering layer to core system\n- **Core System Enhancement**: Updated `CoreSystem` to include `NotificationSystem` alongside `PersistenceSystem` and `StoreSystem`\n- **Supervision Order**: `PersistenceSystem`  `NotificationSystem`  `StoreSystem` for proper dependency management\n\n#### LeaderWorker Availability Fix\n\n- **Core System Integration**: LeaderWorker now starts as part of core system, not clustering components\n- **Startup Order**: LeaderWorker is available before clustering components attempt to use it\n- **Resolved `:noproc` Error**: Fixed LeaderWorker activation failures by ensuring it's always running when needed\n- **Single and Cluster Mode**: LeaderWorker now available in both single-node and cluster modes\n\n#### System Architecture Cleanup\n\n- **Removed LeadershipSystem**: Consolidated functionality into `NotificationSystem`\n- **Cleaner Separation**: Core functionality (leadership, events) vs clustering (coordination, membership)\n- **Improved Documentation**: Updated supervision tree documentation to reflect new architecture\n- **Simplified Dependencies**: Reduced coupling between core and clustering components","ref":"changelog.html#architectural-refactoring"},{"type":"extras","title":"Process Management Enhancement - Changelog","doc":"#### Graceful Shutdown Implementation\n\n- **Universal Coverage**: All 18 GenServer processes now implement graceful shutdown\n- **Terminate Callbacks**: Added `terminate/2` callbacks to all GenServers for proper cleanup\n- **Exit Trapping**: Enabled `Process.flag(:trap_exit, true)` on all GenServers\n- **Resource Cleanup**: Proper cleanup of Swarm registrations, PubSub subscriptions, and Khepri stores\n\n#### Enhanced Process Lifecycle\n\n- **Swarm Registration Cleanup**: Worker processes properly unregister from Swarm on shutdown\n- **PubSub Subscription Cleanup**: EventProjector properly unsubscribes from topics\n- **Khepri Store Shutdown**: Store processes gracefully stop Khepri instances\n- **Network Monitoring**: NodeMonitor properly disables network monitoring on shutdown","ref":"changelog.html#process-management-enhancement"},{"type":"extras","title":"Development Environment - Changelog","doc":"#### Configuration Updates\n\n- **proc-sup Configuration**: Added description \"Process Supervisor Event Store\" and tags \"development,cluster,proc-sup,core\"\n- **reg-gh Configuration**: Added description \"Registration System Event Store\" and tags \"development,cluster,reg-gh,registration\"\n- **Environment-Specific Tags**: Different tags for development vs production environments\n- **Consistent Formatting**: Standardized environment variable layout across all configuration files","ref":"changelog.html#development-environment"},{"type":"extras","title":"Benefits - Changelog","doc":"#### Operational Improvements\n\n- **Enhanced Monitoring**: Rich store metadata enables better operational visibility\n- **Improved Debugging**: Store descriptions and tags help identify issues faster\n- **Better Resource Management**: Priority and auto-start fields enable fine-grained control\n- **Cleaner Shutdown**: All processes terminate gracefully without resource leaks\n\n#### Development Experience\n\n- **Clearer Architecture**: Separation of core vs clustering concerns\n- **Consistent Configuration**: Standardized environment variable management\n- **Better Testability**: Core components can be tested independently of clustering\n- **Simplified Debugging**: LeaderWorker availability issues resolved\n\n#### System Reliability\n\n- **Reduced Race Conditions**: Proper startup order prevents timing-related failures\n- **Resource Leak Prevention**: Graceful shutdown prevents resource accumulation\n- **Improved Fault Tolerance**: Better separation of concerns reduces cascade failures\n- **Enhanced Observability**: Rich metadata supports better monitoring and alerting","ref":"changelog.html#benefits"},{"type":"extras","title":"version 0.1.1 (2025.07.13) - Changelog","doc":"","ref":"changelog.html#version-0-1-1-2025-07-13"},{"type":"extras","title":"StoreRegistry Refactoring - Changelog","doc":"#### Enhanced Architecture\n\n- **Store Registration Centralization**: Moved store registration functionality from `StoreCluster` to dedicated `StoreRegistry` module\n- **Self-Registration**: `StoreRegistry` now automatically registers its own store during initialization when `store_id` is provided\n- **Simplified StoreCluster**: `StoreCluster` now focuses purely on cluster coordination without store registration concerns\n\n#### API Integration\n\n- **ExESDBGater.API Integration**: `list_stores()` function now directly calls `ExESDB.StoreRegistry.list_stores()` instead of maintaining local state\n- **Single Source of Truth**: Store information is now centralized in `StoreRegistry` across the entire system\n- **Improved Error Handling**: Added proper error handling for StoreRegistry calls in GaterAPI\n\n#### System Integration\n\n- **StoreSystem Supervision**: Added `StoreRegistry` to the `StoreSystem` supervisor with proper startup order\n- **Component Isolation**: Each component now has a single, well-defined responsibility\n- **Cleaner State Management**: Removed redundant store state from multiple components\n\n#### Benefits\n\n- **Separation of Concerns**: Clear boundaries between clustering and registration responsibilities\n- **Maintainability**: Easier to maintain and reason about store registration logic\n- **Testability**: Store registration can now be tested in isolation\n- **Reduced Coupling**: Components are less tightly coupled and more modular","ref":"changelog.html#storeregistry-refactoring"},{"type":"extras","title":"version 0.0.17 (2025.07.01) - Changelog","doc":"","ref":"changelog.html#version-0-0-17-2025-07-01"},{"type":"extras","title":"Auto-Clustering - Changelog","doc":"- `ExESDB` nodes now automatically join the cluster\n- \"Split-Brain\" scenarios are now mitigated","ref":"changelog.html#auto-clustering"},{"type":"extras","title":"BCUtils - Changelog","doc":"- All functionality related to styling is now transferred to the `:bc_utils` package.\n- Added a Banner after startup.\n- Logger filtering for Swarm and LibCluster noise reduction (via BCUtils.LoggerFilters)","ref":"changelog.html#bcutils"},{"type":"extras","title":"ExESDB Logger Filtering - Changelog","doc":"#### Features\n\nThe `ExESDB.LoggerFilters` module provides additional log noise reduction specifically for ExESDB's distributed systems components:\n\n- **Ra Consensus Filtering**: Reduces Ra heartbeat, append_entries, pre_vote, request_vote, and routine state transition messages while preserving all errors/warnings\n- **Khepri Database Filtering**: Filters internal Khepri operations (cluster state, store operations) at info/debug levels while maintaining error/warning visibility\n- **Enhanced Swarm Filtering**: Complements BCUtils filtering with additional ExESDB-specific Swarm noise reduction\n- **Enhanced LibCluster Filtering**: Complements BCUtils filtering with additional ExESDB-specific cluster formation noise reduction\n\n#### Benefits\n\n- Dramatically improves log readability in development and production environments\n- Intelligent filtering preserves all error and warning messages\n- Focused on ExESDB-specific distributed systems infrastructure (Ra, Khepri)\n- Works in conjunction with BCUtils.LoggerFilters for comprehensive noise reduction","ref":"changelog.html#exesdb-logger-filtering"},{"type":"extras","title":"ExESDBGater - Changelog","doc":"- The `ExESDB.GatewayAPI` is moved to the `:ex_esdb_gater` package.\n\n#### Features\n\nSnapshots Subsystem provides cluster wide support for reading and writing snapshots, using a key derived from the `source_uuid`, `stream_uuid` and `version` of the snapshot.","ref":"changelog.html#exesdbgater"},{"type":"extras","title":"version 0.0.16 (2025.06.26) - Changelog","doc":"","ref":"changelog.html#version-0-0-16-2025-06-26"},{"type":"extras","title":"Snapshots - Changelog","doc":"#### Features\n\nSnapshots Subsystem provides cluster wide support for reading and writing snapshots, using a key derived from the `source_uuid`, `stream_uuid` and `version` of the snapshot.\n\n- `record_snapshot/5` function\n- `delete_snapshot/4` function\n- `read_snapshot/4` function\n- `list_snapshots/3` function\n\n#### Supported by Gateway API\n\nIt is advised to use `ExESDB.GatewayAPI` to access the Snapshots Subsystem.","ref":"changelog.html#snapshots"},{"type":"extras","title":"version 0.0.15 (2025.06.15) - Changelog","doc":"","ref":"changelog.html#version-0-0-15-2025-06-15"},{"type":"extras","title":"Subscriptions - Changelog","doc":"#### Transient subscriptions\n\n- `:by_stream`, `:by_event_type`, `:by_event_pattern`, `:by_event_payload`\n- Events are forwarded to `Phoenix.PubSub` for now\n\n#### Persistent subscriptions\n\n- `:by_stream`, with support for replaying from a given version\n- Events are forwarded to a specific subscriber process\n- `ack_event/3` function is provided\n\n#### \"Follow-the-Leader\"\n\nEmitter processes are automatically started on the leader node,\nwhen a new leader is elected.\n\n#### Gateway API\n\n- A cluster-wide gateway API is provided\n- is an entry point for all the other modules\n- provides basic High-Availability and Load-Balancing","ref":"changelog.html#subscriptions"},{"type":"extras","title":"version 0.0.9-alpha (2025.05.04) - Changelog","doc":"","ref":"changelog.html#version-0-0-9-alpha-2025-05-04"},{"type":"extras","title":"Subscriptions - Changelog","doc":"- `ExESDB.Subscriptions` module\n- `func_registrations.exs` file\n- emitter trigger in `khepri` now only uses the `erlang`-native :pg library (process groups)\n\n#### Skeleton support for Commanded\n\n- `ExESDB.Commanded.Adapter` module\n- `ExESDB.Commanded.Mapper` module","ref":"changelog.html#subscriptions-1"},{"type":"extras","title":"version 0.0.8-alpha - Changelog","doc":"","ref":"changelog.html#version-0-0-8-alpha"},{"type":"extras","title":"2025.04.13 - Changelog","doc":"- Added `ExESDB.EventStore.stream_forward/4` function\n- Added `BeamCampus.ColorFuncs` module\n- Added `ExESDB.Commanded.Adapter` module\n- Refactored `ExESDB.EventStreamReader` and `ExESDB.EventStreamWriter` modules:\n- Streams are now read and written using the `ExESDB.Streams` module\n- Removed `ExESDB.EventStreamReader` module\n- Removed `ExESDB.EventStreamWriter` module","ref":"changelog.html#2025-04-13"},{"type":"extras","title":"version 0.0.7-alpha - Changelog","doc":"","ref":"changelog.html#version-0-0-7-alpha"},{"type":"extras","title":"version 0.0.1-alpha - Changelog","doc":"","ref":"changelog.html#version-0-0-1-alpha"},{"type":"extras","title":"2025.03.25 - Changelog","doc":"- Initial release","ref":"changelog.html#2025-03-25"},{"type":"extras","title":"Getting Started","doc":"# Getting Started with ExESDB","ref":"getting-started.html"},{"type":"extras","title":"Introduction - Getting Started","doc":"Event Sourcing with CQRS is a technique for building applications that are based on an immutable log of events, which makes it ideal for building concurrent, distributed systems.\n\nThough it is gaining popularity, the number of options for storing these events is limited and require specialized services like Kurrent (aka Greg's EventStore) or AxonIQ.\n\nOne of the strong-points of the BEAM is, that it comes 'batteries included': there are BEAM-native libraries for many common tasks, like: storage, pub/sub, caching, logging, telemetry, etc.\n\n`ExESDB` is an attempt to create a BEAM-native Event Store written in Erlang/Elixir, building further upon the [Khepri](https://github.com/rabbitmq/khepri) library, which in turn builds upon the [Ra](https://github.com/rabbitmq/ra) library.","ref":"getting-started.html#introduction"},{"type":"extras","title":"Status - Getting Started","doc":"**This is a work in progress**\n\nThe project is in an early stage of development, and is not ready for production use.\n\nSource code is available on [GitHub](https://github.com/beam-campus/ex-esdb).","ref":"getting-started.html#status"},{"type":"extras","title":"Installation - Getting Started","doc":"In your `mix.exs` file:\n\n```elixir\ndef deps do\n  [\n    {:ex_esdb, \"~> 0.0.16\"}\n  ]\nend\n```","ref":"getting-started.html#installation"},{"type":"extras","title":"Configuration - Getting Started","doc":"> **Note**: For detailed configuration examples including umbrella applications, see the [Configuring ExESDB Applications](configuring-exesdb-apps.md) guide.\n\n1. in your `config/config.exs` file:\n\n```elixir\n# Example standalone configuration\n# If your app is named :my_event_store\nconfig :my_event_store, :ex_esdb,\n  data_dir: \"/var/lib/ex_esdb\",\n  store_id: :my_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :my_pubsub,\n  reader_idle_ms: 15000,\n  writer_idle_ms: 12000,\n  store_description: \"My Event Store\",\n  store_tags: [\"production\", \"events\"]\n\n# Configure libcluster (recommended)\nconfig :libcluster,\n  topologies: [\n    example: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"my_secret\"\n      ]\n    ]\n  ]\n```\n\n2. from the ENVIRONMENT:\n\n```bash\n\nEX_ESDB_DATA_DIR=\"/var/lib/ex_esdb\"\nEX_ESDB_STORE_ID=my_store\nEX_ESDB_DB_TYPE=cluster\nEX_ESDB_TIMEOUT=5000\nEX_ESDB_PUB_SUB=my_pubsub\n\n```","ref":"getting-started.html#configuration"},{"type":"extras","title":"Usage - Getting Started","doc":"```elixir\ndefmodule MyApp.Application do\n  use Application\n\n  @impl true\n  def start(_type, _args) do\nopts = ExESDB.Options.app_env(:my_event_store)\n    children = [\n      {ExESDB.System, opts},\n    ]\n\n    opts = [strategy: :one_for_one, name: MyApp.Supervisor]\n    Supervisor.start_link(children, opts)\n  end\n\nend\n```","ref":"getting-started.html#usage"},{"type":"extras","title":"Architecture","doc":"# ExESDB Architecture Analysis","ref":"architecture.html"},{"type":"extras","title":"Overview - Architecture","doc":"ExESDB is a BEAM-native Event Store built on top of the Khepri library, which in turn is built on the Ra library. It's designed as a distributed, fault-tolerant event sourcing system that leverages the strengths of the BEAM ecosystem for handling concurrent, distributed workloads.","ref":"architecture.html#overview"},{"type":"extras","title":"High-Level Architecture - Architecture","doc":"```mermaid\ngraph TD\n    A[ExESDB.App] --> B[ExESDB.System]\n    B --> C[PubSub Layer]\n    B --> D[Store Management]\n    B --> E[Cluster Management]\n    B --> F[Event Processing]\n    B --> G[Gateway Layer]\n    \n    D --> D1[StoreManager]\n    D --> D2[Store Workers]\n    D --> D3[Khepri Backend]\n    \n    E --> E1[LibCluster]\n    E --> E2[ClusterSystem]\n    E --> E3[KhepriCluster]\n    E --> E4[LeaderSystem]\n    \n    F --> F1[Streams]\n    F --> F2[Snapshots]\n    F --> F3[Subscriptions]\n    \n    G --> G1[GatewaySupervisor]\n    G --> G2[GatewayWorker]\n```","ref":"architecture.html#high-level-architecture"},{"type":"extras","title":"Core Components - Architecture","doc":"","ref":"architecture.html#core-components"},{"type":"extras","title":"1. Application Layer - Architecture","doc":"#### ExESDB.App\n- **Purpose**: Main application entry point\n- **Responsibilities**:\n  - Application lifecycle management\n  - Initial configuration loading\n  - Starting the main supervisor tree\n  - Graceful shutdown handling\n\n#### ExESDB.System\n- **Purpose**: Top-level supervisor for the entire system\n- **Responsibilities**:\n  - Supervises all major subsystems\n  - Manages system startup sequence\n  - Handles OS signal processing\n  - Dynamically configures components based on deployment mode (single vs cluster)","ref":"architecture.html#1-application-layer"},{"type":"extras","title":"2. Storage Layer - Architecture","doc":"#### ExESDB.StoreManager\n- **Purpose**: Multi-store management and coordination\n- **Responsibilities**:\n  - Dynamic store creation and removal\n  - Store lifecycle management\n  - Configuration management per store\n  - Store status tracking\n\n#### ExESDB.Store\n- **Purpose**: Individual event store wrapper around Khepri\n- **Responsibilities**:\n  - Khepri store initialization\n  - Store state management\n  - Direct interaction with Khepri API\n\n```mermaid\ngraph LR\n    A[StoreManager] --> B[Store1]\n    A --> C[Store2]\n    A --> D[StoreN]\n    \n    B --> E[Khepri Instance 1]\n    C --> F[Khepri Instance 2]\n    D --> G[Khepri Instance N]\n    \n    E --> H[Data Directory 1]\n    F --> I[Data Directory 2]\n    G --> J[Data Directory N]\n```","ref":"architecture.html#2-storage-layer"},{"type":"extras","title":"3. Clustering Layer - Architecture","doc":"The clustering layer provides distributed coordination and fault tolerance:\n\n#### ExESDB.KhepriCluster\n- **Purpose**: Khepri-specific cluster coordination\n- **Responsibilities**:\n  - Cluster join/leave operations\n  - Leadership detection and tracking\n  - Membership monitoring\n  - Node health monitoring\n\n#### ExESDB.ClusterSystem\n- **Purpose**: High-level cluster coordination\n- **Responsibilities**:\n  - Supervises cluster coordination components\n  - Manages cluster-specific services\n  - Handles split-brain prevention\n\n#### ExESDB.LeaderSystem\n- **Purpose**: Leadership management\n- **Responsibilities**:\n  - Leader election coordination\n  - Leader-specific functionality activation\n  - Leader state tracking\n\n```mermaid\ngraph TD\n    A[LibCluster] --> B[Node Discovery]\n    B --> C[KhepriCluster]\n    C --> D[Cluster Join/Leave]\n    C --> E[Leadership Detection]\n    C --> F[Membership Monitoring]\n    \n    G[ClusterSystem] --> H[ClusterCoordinator]\n    G --> I[NodeMonitor]\n    \n    J[LeaderSystem] --> K[LeaderWorker]\n    J --> L[LeaderTracker]\n    \n    style A fill:#e1f5fe\n    style G fill:#f3e5f5\n    style J fill:#e8f5e8\n```","ref":"architecture.html#3-clustering-layer"},{"type":"extras","title":"4. Event Processing Layer - Architecture","doc":"#### ExESDB.Streams\n- **Purpose**: Event stream management\n- **Responsibilities**:\n  - Stream read/write operations\n  - Stream partitioning via PartitionSupervisor\n  - Worker pool management for stream operations\n\n#### ExESDB.Snapshots\n- **Purpose**: Snapshot management for event sourcing\n- **Responsibilities**:\n  - Snapshot creation and retrieval\n  - Snapshot versioning\n  - Snapshot storage path management\n\n#### ExESDB.Subscriptions\n- **Purpose**: Event subscription management\n- **Responsibilities**:\n  - Subscription lifecycle management\n  - Event delivery to subscribers\n  - Subscription persistence\n\n```mermaid\ngraph TD\n    A[Streams] --> B[StreamsWriters Pool]\n    A --> C[StreamsReaders Pool]\n    \n    D[Snapshots] --> E[SnapshotsWriters Pool]\n    D --> F[SnapshotsReaders Pool]\n    \n    G[Subscriptions] --> H[SubscriptionsReader]\n    G --> I[SubscriptionsWriter]\n    \n    B --> J[DynamicSupervisor]\n    C --> K[DynamicSupervisor]\n    E --> L[DynamicSupervisor]\n    F --> M[DynamicSupervisor]\n    \n    J --> N[StreamWriterWorker1]\n    J --> O[StreamWriterWorker2]\n    K --> P[StreamReaderWorker1]\n    K --> Q[StreamReaderWorker2]\n```","ref":"architecture.html#4-event-processing-layer"},{"type":"extras","title":"5. Communication Layer - Architecture","doc":"#### PubSub Integration\n- **Purpose**: Inter-process and inter-node communication\n- **Responsibilities**:\n  - Event broadcasting\n  - Subscription management\n  - Message routing\n\n#### ExESDB.GatewaySupervisor & GatewayWorker\n- **Purpose**: External API gateway\n- **Responsibilities**:\n  - External client request handling\n  - API endpoint management\n  - Request routing to appropriate subsystems","ref":"architecture.html#5-communication-layer"},{"type":"extras","title":"Architecture Patterns - Architecture","doc":"","ref":"architecture.html#architecture-patterns"},{"type":"extras","title":"1. Supervision Tree Pattern - Architecture","doc":"```mermaid\ngraph TD\n    A[ExESDB.App] --> B[ExESDB.System]\n    B --> C[PubSub]\n    B --> D[StoreManager]\n    B --> E[Streams]\n    B --> F[Snapshots]\n    B --> G[Subscriptions]\n    B --> H[LeaderSystem]\n    B --> I[KhepriCluster]\n    B --> J[GatewaySupervisor]\n    B --> K[ClusterSystem]\n    B --> L[EmitterPools]\n    \n    E --> M[PartitionSupervisor - Writers]\n    E --> N[PartitionSupervisor - Readers]\n    \n    F --> O[PartitionSupervisor - Writers]\n    F --> P[PartitionSupervisor - Readers]\n    \n    H --> Q[LeaderWorker]\n    H --> R[LeaderTracker]\n    \n    K --> S[ClusterCoordinator]\n    K --> T[NodeMonitor]\n    \n    J --> U[GatewayWorker]\n```","ref":"architecture.html#1-supervision-tree-pattern"},{"type":"extras","title":"2. Worker Pool Pattern - Architecture","doc":"ExESDB extensively uses worker pools for different types of operations:\n\n```mermaid\ngraph LR\n    A[Client Request] --> B[PartitionSupervisor]\n    B --> C[DynamicSupervisor]\n    C --> D[Worker1]\n    C --> E[Worker2]\n    C --> F[WorkerN]\n    \n    G[Hash Ring] --> B\n    H[Load Balancing] --> B\n```","ref":"architecture.html#2-worker-pool-pattern"},{"type":"extras","title":"3. Distributed State Management - Architecture","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant G as Gateway\n    participant L as Leader\n    participant F as Follower\n    participant K as Khepri\n    \n    C->>G: Write Request\n    G->>L: Route to Leader\n    L->>K: Write to Khepri\n    K->>F: Replicate to Followers\n    F->>K: Acknowledge\n    K->>L: Confirm Write\n    L->>G: Success Response\n    G->>C: Return Result\n```","ref":"architecture.html#3-distributed-state-management"},{"type":"extras","title":"Deployment Modes - Architecture","doc":"","ref":"architecture.html#deployment-modes"},{"type":"extras","title":"Single Node Mode - Architecture","doc":"- **Configuration**: `db_type: :single`\n- **Characteristics**:\n  - No clustering components\n  - Local-only operations\n  - Simplified architecture\n  - Development/testing focused","ref":"architecture.html#single-node-mode"},{"type":"extras","title":"Cluster Mode - Architecture","doc":"- **Configuration**: `db_type: :cluster`\n- **Characteristics**:\n  - Full clustering capabilities\n  - Distributed consensus via Ra\n  - Fault tolerance\n  - Production-ready\n\n```mermaid\ngraph TD\n    A[Configuration] --> B{db_type}\n    B -->|:single| C[Single Node Components]\n    B -->|:cluster| D[Cluster Components]\n    \n    C --> E[StoreManager]\n    C --> F[Local Streams]\n    C --> G[Local Snapshots]\n    \n    D --> H[LibCluster]\n    D --> I[ClusterSystem]\n    D --> J[KhepriCluster]\n    D --> K[Distributed Components]\n```","ref":"architecture.html#cluster-mode"},{"type":"extras","title":"Data Flow - Architecture","doc":"","ref":"architecture.html#data-flow"},{"type":"extras","title":"Event Write Flow - Architecture","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant GW as Gateway\n    participant SM as StoreManager\n    participant S as Store\n    participant K as Khepri\n    participant PS as PubSub\n    \n    C->>GW: Write Event\n    GW->>SM: Route to Store\n    SM->>S: Write Request\n    S->>K: Store Event\n    K->>S: Confirm Write\n    S->>PS: Publish Event\n    PS->>C: Event Notification\n    S->>GW: Success Response\n    GW->>C: Return Result\n```","ref":"architecture.html#event-write-flow"},{"type":"extras","title":"Event Read Flow - Architecture","doc":"```mermaid\nsequenceDiagram\n    participant C as Client\n    participant GW as Gateway\n    participant SR as StreamReader\n    participant S as Store\n    participant K as Khepri\n    \n    C->>GW: Read Stream\n    GW->>SR: Route to Reader\n    SR->>S: Read Request\n    S->>K: Query Events\n    K->>S: Return Events\n    S->>SR: Event Data\n    SR->>GW: Stream Response\n    GW->>C: Return Events\n```","ref":"architecture.html#event-read-flow"},{"type":"extras","title":"Key Design Decisions - Architecture","doc":"","ref":"architecture.html#key-design-decisions"},{"type":"extras","title":"1. Khepri as Backend - Architecture","doc":"- **Rationale**: BEAM-native, Ra-based distributed database\n- **Benefits**: \n  - Native Erlang integration\n  - Built-in clustering\n  - Strong consistency guarantees\n  - Fault tolerance","ref":"architecture.html#1-khepri-as-backend"},{"type":"extras","title":"2. Supervisor Tree Architecture - Architecture","doc":"- **Rationale**: Leverages OTP supervision principles\n- **Benefits**:\n  - Fault isolation\n  - Automatic restart strategies\n  - System resilience\n  - Clear responsibility boundaries","ref":"architecture.html#2-supervisor-tree-architecture"},{"type":"extras","title":"3. Worker Pool Pattern - Architecture","doc":"- **Rationale**: Efficient concurrent processing\n- **Benefits**:\n  - Load distribution\n  - Resource management\n  - Scalability\n  - Fault tolerance","ref":"architecture.html#3-worker-pool-pattern"},{"type":"extras","title":"4. Multi-Store Architecture - Architecture","doc":"- **Rationale**: Support for multiple event stores in single cluster\n- **Benefits**:\n  - Tenant isolation\n  - Resource optimization\n  - Flexible deployment\n  - Gradual migration support","ref":"architecture.html#4-multi-store-architecture"},{"type":"extras","title":"Performance Considerations - Architecture","doc":"","ref":"architecture.html#performance-considerations"},{"type":"extras","title":"Partitioning Strategy - Architecture","doc":"- Uses PartitionSupervisor for distributing workload\n- Hash-based routing for even distribution\n- Separate pools for read/write operations","ref":"architecture.html#partitioning-strategy"},{"type":"extras","title":"Clustering Optimization - Architecture","doc":"- Configurable probe intervals for node monitoring\n- Failure thresholds to prevent cascade failures\n- Efficient membership change detection","ref":"architecture.html#clustering-optimization"},{"type":"extras","title":"Resource Management - Architecture","doc":"- Dynamic worker creation/destruction\n- Configurable timeout values\n- Memory-efficient event storage via Khepri","ref":"architecture.html#resource-management"},{"type":"extras","title":"Security Considerations - Architecture","doc":"","ref":"architecture.html#security-considerations"},{"type":"extras","title":"Network Security - Architecture","doc":"- Node-to-node communication via Erlang distribution\n- Cluster authentication via shared secrets\n- Network partitioning detection and handling","ref":"architecture.html#network-security"},{"type":"extras","title":"Access Control - Architecture","doc":"- Gateway-based request filtering\n- Store-level access control\n- Subscription-based permissions","ref":"architecture.html#access-control"},{"type":"extras","title":"Monitoring and Observability - Architecture","doc":"","ref":"architecture.html#monitoring-and-observability"},{"type":"extras","title":"Metrics Collection - Architecture","doc":"- Built-in metrics module\n- Performance monitoring\n- Cluster health tracking","ref":"architecture.html#metrics-collection"},{"type":"extras","title":"Logging Strategy - Architecture","doc":"- Structured logging throughout\n- Configurable log levels\n- Cluster-aware log correlation","ref":"architecture.html#logging-strategy"},{"type":"extras","title":"Health Checks - Architecture","doc":"- Node health monitoring\n- Store availability checks\n- Leadership status tracking","ref":"architecture.html#health-checks"},{"type":"extras","title":"Scalability Patterns - Architecture","doc":"","ref":"architecture.html#scalability-patterns"},{"type":"extras","title":"Horizontal Scaling - Architecture","doc":"- Add nodes to existing cluster\n- Automatic workload redistribution\n- Leader election for coordination","ref":"architecture.html#horizontal-scaling"},{"type":"extras","title":"Vertical Scaling - Architecture","doc":"- Worker pool sizing\n- Memory allocation tuning\n- Timeout configuration\n\nThis architecture provides a solid foundation for building distributed, fault-tolerant event sourcing systems while leveraging the unique strengths of the BEAM ecosystem.","ref":"architecture.html#vertical-scaling"},{"type":"extras","title":"Testing","doc":"","ref":"testing.html"},{"type":"extras","title":"Failure Handling","doc":"# Failure Handling in ExESDB\n\nThis guide provides a comprehensive overview of all failure handling strategies implemented in ExESDB, from individual process failures to cluster-wide outages. ExESDB is built on the BEAM's \"let it crash\" philosophy while providing robust recovery mechanisms at every level.","ref":"failure-handling.html"},{"type":"extras","title":"Table of Contents - Failure Handling","doc":"1. [Failure Categories](#failure-categories)\n2. [Supervision Strategies](#supervision-strategies)\n3. [Node-Level Failures](#node-level-failures)\n4. [Network Partitions](#network-partitions)\n5. [Data Consistency](#data-consistency)\n6. [Worker Process Failures](#worker-process-failures)\n7. [Storage Failures](#storage-failures)\n8. [Configuration and Monitoring](#configuration-and-monitoring)\n9. [Recovery Procedures](#recovery-procedures)\n10. [Testing Failure Scenarios](#testing-failure-scenarios)","ref":"failure-handling.html#table-of-contents"},{"type":"extras","title":"Failure Categories - Failure Handling","doc":"ExESDB handles failures across multiple dimensions:","ref":"failure-handling.html#failure-categories"},{"type":"extras","title":"1. Process-Level Failures - Failure Handling","doc":"- **Worker crashes**: Individual GenServer processes failing\n- **Supervisor crashes**: Supervisor trees failing\n- **Application crashes**: Entire OTP applications going down","ref":"failure-handling.html#1-process-level-failures"},{"type":"extras","title":"2. Node-Level Failures - Failure Handling","doc":"- **Hard crashes**: Sudden node termination (power loss, kill -9)\n- **Soft crashes**: Graceful shutdowns and restarts\n- **Network isolation**: Node becomes unreachable but continues running","ref":"failure-handling.html#2-node-level-failures"},{"type":"extras","title":"3. Cluster-Level Failures - Failure Handling","doc":"- **Split-brain scenarios**: Network partitions causing multiple leaders\n- **Quorum loss**: Insufficient nodes for consensus\n- **Data corruption**: Storage-level integrity issues","ref":"failure-handling.html#3-cluster-level-failures"},{"type":"extras","title":"4. Storage-Level Failures - Failure Handling","doc":"- **Disk failures**: Storage becoming unavailable\n- **Corruption**: Data integrity violations\n- **Performance degradation**: Slow storage affecting operations","ref":"failure-handling.html#4-storage-level-failures"},{"type":"extras","title":"Supervision Strategies - Failure Handling","doc":"ExESDB uses a hierarchical supervision tree with different restart strategies for different components:","ref":"failure-handling.html#supervision-strategies"},{"type":"extras","title":"Root Supervision Tree - Failure Handling","doc":"```\nExESDB.App (one_for_one)\n ExESDB.System (one_for_one)\n     Phoenix.PubSub\n     Cluster.Supervisor (LibCluster)\n     PartitionSupervisor (EmitterPools)\n     ExESDB.Store\n     ExESDB.ClusterSystem (one_for_one)\n     ExESDB.Streams (one_for_one)\n     ExESDB.Snapshots\n     ExESDB.Subscriptions (one_for_one)\n     ExESDB.GatewaySupervisor (one_for_one)\n     ExESDB.LeaderSystem (one_for_one)\n```","ref":"failure-handling.html#root-supervision-tree"},{"type":"extras","title":"Restart Strategies by Component - Failure Handling","doc":"| Component | Strategy | Restart | Reason |\n|-----------|----------|---------|--------|\n| **System** | `:one_for_one` | `:permanent` | Core system components |\n| **ClusterSystem** | `:one_for_one` | `:permanent` | Independent cluster services |\n| **Streams** | `:one_for_one` | `:permanent` | Stream readers/writers are independent |\n| **GatewaySupervisor** | `:one_for_one` | `:permanent` | Gateway API and workers |\n| **StreamsWriterWorker** | `:temporary` | `:temporary` | TTL-based lifecycle |\n| **StreamsReaderWorker** | `:temporary` | `:temporary` | On-demand workers |\n| **GatewayWorker** | `:permanent` | `:permanent` | Core gateway functionality |","ref":"failure-handling.html#restart-strategies-by-component"},{"type":"extras","title":"Worker Lifecycle Management - Failure Handling","doc":"**Stream Workers**:\n- Use `:temporary` restart to prevent infinite restart loops\n- Implement TTL-based shutdown for resource management\n- Automatically clean up Swarm registrations on exit\n\n**Gateway Workers**:\n- Use `:permanent` restart for high availability\n- Register with Swarm for distributed load balancing\n- Handle graceful shutdown with cleanup","ref":"failure-handling.html#worker-lifecycle-management"},{"type":"extras","title":"Node-Level Failures - Failure Handling","doc":"","ref":"failure-handling.html#node-level-failures"},{"type":"extras","title":"Hard Crash Detection and Recovery - Failure Handling","doc":"When a node crashes unexpectedly, multiple systems work together to detect and recover:\n\n#### 1. NodeMonitor Service (Fast Detection)\n\n**Problem**: Traditional Raft consensus timeouts can take 10-30 seconds\n**Solution**: Proactive health monitoring with 6-second detection\n\nThis solution implements a comprehensive approach to quickly detect and handle node failures:","ref":"failure-handling.html#hard-crash-detection-and-recovery"},{"type":"extras","title":"1. NodeMonitor Service - Failure Handling","doc":"**Location**: `lib/ex_esdb/node_monitor.ex`\n\n**Features**:\n- **Active Health Probing**: Probes cluster nodes every 2 seconds\n- **Multi-Layer Health Checks**: Verifies node connectivity + application health\n- **Fast Failure Detection**: Marks nodes as failed after 3 consecutive probe failures (6 seconds total)\n- **Automatic Cleanup**: Removes stale Swarm registrations from failed nodes\n- **Event-Driven Updates**: Responds to `:nodeup`/`:nodedown` events\n\n**Configuration**:\n```elixir\n# Default settings (configurable)\nprobe_interval: 2_000,     # 2 seconds between probes\nfailure_threshold: 3,      # 3 failures = node considered down\nprobe_timeout: 1_000       # 1 second timeout per probe\n```","ref":"failure-handling.html#1-nodemonitor-service"},{"type":"extras","title":"2. Integration with Existing Architecture - Failure Handling","doc":"The NodeMonitor integrates seamlessly with your current LibCluster setup:\n\n**Modified Files**:\n- `lib/ex_esdb/cluster_system.ex` - Added NodeMonitor to supervision tree\n- Uses existing `ClusterCoordinator` for split-brain prevention\n- Leverages current `Swarm` registrations for worker cleanup","ref":"failure-handling.html#2-integration-with-existing-architecture"},{"type":"extras","title":"3. How It Works - Failure Handling","doc":"#### Health Probe Cycle (Every 2 seconds):\n1. **Discover Nodes**: Get cluster members from Khepri\n2. **Probe Health**: Test each node with RPC calls\n3. **Track Failures**: Increment failure count for unresponsive nodes\n4. **Trigger Cleanup**: Handle nodes that exceed failure threshold\n5. **Update State**: Maintain monitoring state for next cycle\n\n#### Failure Detection:\n```elixir\n# Multi-layer health check\n1. Node connectivity: :rpc.call(node, :erlang, :node, [])\n2. Application health: Check if :ex_esdb is running\n3. Failure tracking: 3 consecutive failures = node down\n```\n\n#### Automatic Cleanup:\n```elixir\n# When a node is detected as failed:\n1. Remove Swarm worker registrations from failed node\n2. Update cluster state (notify other components)\n3. Clean up subscriptions tied to failed node\n4. Log failure for monitoring/alerting\n```","ref":"failure-handling.html#3-how-it-works"},{"type":"extras","title":"4. Benefits - Failure Handling","doc":"**Fast Detection**: \n- Traditional Raft consensus timeout: 10-30 seconds\n- This solution: 6 seconds (3 failures  2 second intervals)\n\n**Proactive Cleanup**:\n- Prevents requests to unavailable workers\n- Maintains cluster integrity\n- Enables faster recovery\n\n**Graceful Degradation**:\n- System continues operating with remaining nodes\n- Workers redistribute automatically via Swarm\n- No single point of failure","ref":"failure-handling.html#4-benefits"},{"type":"extras","title":"5. Usage - Failure Handling","doc":"#### Check Cluster Health:\n```elixir\n# Get current health status\nExESDB.NodeMonitor.health_status()\n# Returns: %{\n#   monitored_nodes: [:node1@host, :node2@host],\n#   node_failures: %{},\n#   last_seen: %{:node1@host => 1625567890123},\n#   threshold: 3\n# }\n```\n\n#### Manual Node Probe:\n```elixir\n# Force probe a specific node\nExESDB.NodeMonitor.probe_node(:node1@host)\n# Returns: :healthy or :unhealthy\n```","ref":"failure-handling.html#5-usage"},{"type":"extras","title":"6. Configuration Options - Failure Handling","doc":"**Environment Variables** (can be added to your config):\n```elixir\nconfig :ex_esdb, :node_monitor,\n  probe_interval: 2_000,      # How often to probe (ms)\n  failure_threshold: 3,       # Failures before marking as down\n  probe_timeout: 1_000,       # Timeout per probe (ms)\n  cleanup_stale_workers: true # Auto-cleanup Swarm registrations\n```","ref":"failure-handling.html#6-configuration-options"},{"type":"extras","title":"7. Monitoring and Observability - Failure Handling","doc":"**Log Messages**:\n- `NodeMonitor started with 2000ms intervals`\n- `Health probe failed for node1@host (2/3)`\n- `Node node1@host detected as failed, initiating cleanup`\n- `Cleaning up Swarm registration: {:gateway_worker, node1@host, 1234}`\n\n**Integration Points**:\n- Logs can be forwarded to your monitoring system\n- Health status can be exposed via HTTP endpoints\n- Alerts can be triggered on node failures","ref":"failure-handling.html#7-monitoring-and-observability"},{"type":"extras","title":"8. Advanced Features (Future Extensions) - Failure Handling","doc":"The solution is designed to be extensible:\n\n**Planned Enhancements**:\n- **Forced Khepri Node Removal**: Actively remove failed nodes from consensus\n- **Worker Redistribution**: Trigger immediate rebalancing of Swarm workers\n- **Leader Election**: Handle leader failures more aggressively\n- **Custom Health Checks**: Application-specific health validation","ref":"failure-handling.html#8-advanced-features-future-extensions"},{"type":"extras","title":"9. Deployment - Failure Handling","doc":"**No Breaking Changes**: \n- Fully backward compatible with existing cluster\n- Can be deployed incrementally (node by node)\n- Falls back gracefully if monitoring fails\n\n**Resource Usage**:\n- Minimal CPU overhead (RPC calls every 2 seconds)\n- Low memory footprint (tracks failure state only)\n- Network traffic: ~1KB per node per probe","ref":"failure-handling.html#9-deployment"},{"type":"extras","title":"10. Testing the Solution - Failure Handling","doc":"**Chaos Engineering**:\n```bash\n# Simulate hard crash\ndocker kill ex-esdb-node1\n\n# Monitor logs for detection\ndocker logs ex-esdb-node2 | grep NodeMonitor\n\n# Verify cleanup\n# Should see Swarm registrations removed within 6 seconds\n```\n\n**Expected Timeline**:\n- T+0: Node crashes\n- T+2s: First probe failure detected\n- T+4s: Second probe failure  \n- T+6s: Third probe failure, node marked as failed\n- T+6s: Cleanup initiated (Swarm registrations removed)\n- T+8s: Next probe cycle (failed node no longer monitored)\n\n#### 2. LibCluster Integration\n\n**Built-in Node Detection**:\n- Uses `:net_kernel.monitor_nodes(true, [:nodedown_reason])` for immediate notification\n- Gossip-based discovery helps detect network issues\n- Automatic cluster formation and healing\n\n#### 3. ClusterCoordinator\n\n**Split-Brain Prevention**:\n- Deterministic leader election (lowest node name)\n- Prevents multiple clusters from forming\n- Coordinates safe cluster joining\n\n**Features**:\n```elixir\n# Prevent split-brain during network partitions\ndef should_be_cluster_coordinator(connected_nodes) do\n  all_nodes = [node() | connected_nodes] |> Enum.sort()\n  node() == List.first(all_nodes)\nend\n```","ref":"failure-handling.html#10-testing-the-solution"},{"type":"extras","title":"Graceful Shutdown Handling - Failure Handling","doc":"ExESDB handles graceful shutdowns through multiple mechanisms:\n\n**Signal Handling**:\n```elixir\n# SIGTERM and SIGQUIT handling\n:os.set_signal(:sigterm, :handle)\n:os.set_signal(:sigquit, :handle)\n```\n\n**Process Cleanup**:\n- Workers unregister from Swarm before termination\n- Subscription state is persisted\n- Transactions are completed or rolled back","ref":"failure-handling.html#graceful-shutdown-handling"},{"type":"extras","title":"Network Partitions - Failure Handling","doc":"","ref":"failure-handling.html#network-partitions"},{"type":"extras","title":"Split-Brain Prevention - Failure Handling","doc":"ExESDB implements multiple layers to prevent split-brain scenarios:\n\n#### 1. ClusterCoordinator Logic\n- **Deterministic Election**: Uses sorted node names for consistent leader selection\n- **Existing Cluster Detection**: Searches for active clusters before forming new ones\n- **Coordinated Joining**: Prevents multiple simultaneous cluster formations\n\n#### 2. Raft Consensus (Ra/Khepri)\n- **Majority Quorum**: Requires majority of nodes for write operations\n- **Leader Election**: Automatic failover when leader becomes unavailable\n- **Log Replication**: Ensures consistency across cluster members\n\n#### 3. Partition Tolerance\n**Minority Partition Behavior**:\n- Nodes in minority partition become read-only\n- No new events can be written without quorum\n- Automatic healing when partition resolves\n\n**Majority Partition Behavior**:\n- Continues normal operations\n- Elects new leader if needed\n- Accepts new writes and maintains consistency","ref":"failure-handling.html#split-brain-prevention"},{"type":"extras","title":"Network Partition Recovery - Failure Handling","doc":"**Automatic Healing Process**:\n1. **Detection**: Nodes detect network connectivity restoration\n2. **State Synchronization**: Minority nodes sync with majority\n3. **Conflict Resolution**: Raft log reconciliation\n4. **Service Restoration**: Workers redistribute across all nodes","ref":"failure-handling.html#network-partition-recovery"},{"type":"extras","title":"Data Consistency - Failure Handling","doc":"","ref":"failure-handling.html#data-consistency"},{"type":"extras","title":"Transaction Handling - Failure Handling","doc":"ExESDB provides ACID guarantees through Khepri transactions:\n\n**Optimistic Concurrency Control**:\n```elixir\ndef try_append_events(store, stream_id, expected_version, events) do\n  current_version = get_current_version(store, stream_id)\n  \n  if current_version == expected_version do\n    # Proceed with append\n    append_events_atomically(store, stream_id, events, current_version)\n  else\n    {:error, :wrong_expected_version}\n  end\nend\n```\n\n**Transaction Isolation**:\n- Uses Khepri's MVCC for concurrent access\n- Transactions are atomic and isolated\n- Automatic rollback on failures","ref":"failure-handling.html#transaction-handling"},{"type":"extras","title":"Conflict Resolution - Failure Handling","doc":"**Version Conflicts**:\n- Expected version mismatches return `:wrong_expected_version`\n- Clients must retry with updated expected version\n- No automatic conflict resolution (explicit client handling)\n\n**Concurrent Writes**:\n- Only one writer per stream at a time\n- Serialized access through stream-specific workers\n- Queue management for high-throughput scenarios","ref":"failure-handling.html#conflict-resolution"},{"type":"extras","title":"Worker Process Failures - Failure Handling","doc":"","ref":"failure-handling.html#worker-process-failures"},{"type":"extras","title":"Stream Worker Failure Handling - Failure Handling","doc":"#### Writers (StreamsWriterWorker)\n**Lifecycle Management**:\n- `:temporary` restart strategy prevents restart loops\n- TTL-based shutdown for resource management\n- Automatic Swarm cleanup on termination\n\n**Failure Scenarios**:\n```elixir\n# TTL-based shutdown\ndef handle_info(:check_idle, %{idle_since: idle_since} = state) do\n  writer_ttl = Options.writer_idle_ms()\n  \n  if idle_since + writer_ttl < epoch_time_ms() do\n    Process.exit(self(), :ttl_reached)\n  end\n  \n  {:noreply, state}\nend\n\n# Graceful cleanup on exit\ndef handle_info({:EXIT, _pid, reason}, %{worker_name: name} = state) do\n  Swarm.unregister_name(name)\n  {:noreply, state}\nend\n```\n\n#### Readers (StreamsReaderWorker)\n- Similar TTL-based lifecycle\n- On-demand creation for read operations\n- Automatic cleanup when no longer needed","ref":"failure-handling.html#stream-worker-failure-handling"},{"type":"extras","title":"Gateway Worker Failures - Failure Handling","doc":"**High Availability Design**:\n- `:permanent` restart strategy for critical gateway functions\n- Load balancing through random worker selection\n- Graceful failover to other gateway workers\n\n**Worker Distribution**:\n```elixir\n# Random load balancing with fallback\ndefp random_gateway_worker do\n  case Swarm.members(:gateway_workers) do\n    [] -> \n      # Fallback to local gateway if no distributed workers\n      ExESDB.GatewayWorker\n    workers -> \n      workers |> Enum.random() |> elem(1)\n  end\nend\n```","ref":"failure-handling.html#gateway-worker-failures"},{"type":"extras","title":"Subscription Worker Failures - Failure Handling","doc":"**\"Follow-the-Leader\" Pattern**:\n- Subscription workers automatically migrate to leader node\n- Persistent subscription state survives worker failures\n- Automatic restart and state recovery","ref":"failure-handling.html#subscription-worker-failures"},{"type":"extras","title":"Storage Failures - Failure Handling","doc":"","ref":"failure-handling.html#storage-failures"},{"type":"extras","title":"Khepri/Ra Storage Resilience - Failure Handling","doc":"**Data Directory Management**:\n```elixir\n# Configurable data directory\nconfig :ex_esdb, :khepri,\n  data_dir: \"/data\",\n  store_id: :reg_gh,\n  timeout: 2_000\n```\n\n**Failure Scenarios**:\n\n#### Disk Space Exhaustion\n- **Detection**: Monitor disk usage in production\n- **Mitigation**: Implement log compaction and cleanup\n- **Recovery**: Restore from snapshots if available\n\n#### Corruption Detection\n- **Checksums**: Ra maintains data integrity checks\n- **Verification**: Periodic consistency checks\n- **Recovery**: Restore from cluster peers or backups\n\n#### Performance Degradation\n- **Monitoring**: Track operation latencies\n- **Alerting**: Set thresholds for response times\n- **Mitigation**: Scale storage or redistribute load","ref":"failure-handling.html#khepri-ra-storage-resilience"},{"type":"extras","title":"Backup and Recovery - Failure Handling","doc":"**Snapshot Management**:\n- Regular snapshots of aggregate state\n- Version-based snapshot storage\n- Distributed snapshot replication\n\n**Disaster Recovery**:\n1. **Data Loss Prevention**: Multi-node replication\n2. **Point-in-Time Recovery**: Event replay from snapshots\n3. **Cross-Region Backup**: External backup strategies","ref":"failure-handling.html#backup-and-recovery"},{"type":"extras","title":"Configuration and Monitoring - Failure Handling","doc":"","ref":"failure-handling.html#configuration-and-monitoring"},{"type":"extras","title":"Health Check Endpoints - Failure Handling","doc":"**Cluster Health**:\n```elixir\n# Check overall cluster status\nExESDB.NodeMonitor.health_status()\n\n# Check specific node\nExESDB.NodeMonitor.probe_node(:node1@host)\n\n# Get cluster members\nExESDB.Cluster.members(store_id)\n```\n\n**Performance Metrics**:\n- Operation latencies\n- Throughput measurements\n- Resource utilization\n- Error rates","ref":"failure-handling.html#health-check-endpoints"},{"type":"extras","title":"Logging and Alerting - Failure Handling","doc":"**Structured Logging**:\n```elixir\nconfig :logger, :console,\n  format: \"$time $metadata[$level] $message\\n\",\n  metadata: [:mfa]\n```\n\n**Alert Categories**:\n- **Critical**: Node failures, data corruption\n- **Warning**: Performance degradation, high error rates\n- **Info**: Normal operations, state changes","ref":"failure-handling.html#logging-and-alerting"},{"type":"extras","title":"Configuration Best Practices - Failure Handling","doc":"**Production Settings**:\n```elixir\n# Timeouts\nconfig :ex_esdb, :khepri,\n  timeout: 5_000  # Increase for production\n\n# Node monitoring\nconfig :ex_esdb, :node_monitor,\n  probe_interval: 2_000,\n  failure_threshold: 3,\n  probe_timeout: 1_000\n\n# Worker TTL\nconfig :ex_esdb, :worker_idle_ms, 300_000  # 5 minutes\n```\n\n**Development Settings**:\n```elixir\n# Faster timeouts for development\nconfig :ex_esdb, :khepri,\n  timeout: 10_000\n\n# Shorter TTL for resource management\nconfig :ex_esdb, :worker_idle_ms, 60_000  # 1 minute\n```","ref":"failure-handling.html#configuration-best-practices"},{"type":"extras","title":"Recovery Procedures - Failure Handling","doc":"","ref":"failure-handling.html#recovery-procedures"},{"type":"extras","title":"Manual Recovery Steps - Failure Handling","doc":"#### Single Node Recovery\n1. **Identify Issue**: Check logs and monitoring\n2. **Isolate Node**: Remove from load balancer if needed\n3. **Restart Service**: Use graceful restart procedures\n4. **Verify Health**: Confirm cluster membership\n5. **Restore Traffic**: Gradually return to service\n\n#### Cluster Recovery\n1. **Assess Damage**: Determine scope of failure\n2. **Quorum Check**: Ensure majority of nodes available\n3. **Leader Election**: Verify or trigger new leader election\n4. **Data Integrity**: Check for any corruption\n5. **Service Validation**: Test critical operations","ref":"failure-handling.html#manual-recovery-steps"},{"type":"extras","title":"Automated Recovery - Failure Handling","doc":"**Self-Healing Mechanisms**:\n- Automatic process restarts via supervision\n- Worker redistribution through Swarm\n- Cluster reformation after partitions\n- Leader election on failures\n\n**Monitoring Integration**:\n- Health check failures trigger alerts\n- Automatic scaling based on load\n- Proactive maintenance scheduling","ref":"failure-handling.html#automated-recovery"},{"type":"extras","title":"Testing Failure Scenarios - Failure Handling","doc":"","ref":"failure-handling.html#testing-failure-scenarios"},{"type":"extras","title":"Chaos Engineering - Failure Handling","doc":"**Node Failures**:\n```bash\n# Hard crash simulation\ndocker kill ex-esdb-node1\n\n# Network partition simulation\niptables -A INPUT -s   -j DROP\niptables -A OUTPUT -d   -j DROP\n\n# Resource exhaustion\nstress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 60s\n```\n\n**Service Failures**:\n```bash\n# Stop specific services\nsystemctl stop ex_esdb\n\n# Simulate disk failures\nfill-disk.sh /data 95%\n\n# Network latency injection\ntc qdisc add dev eth0 root netem delay 500ms\n```","ref":"failure-handling.html#chaos-engineering"},{"type":"extras","title":"Test Scenarios - Failure Handling","doc":"#### Scenario 1: Single Node Failure\n**Steps**:\n1. Start 3-node cluster\n2. Kill one node abruptly\n3. Verify cluster continues operating\n4. Check client request success rates\n5. Restart failed node\n6. Verify automatic rejoin\n\n**Expected Results**:\n- Cluster maintains quorum (2/3 nodes)\n- No data loss\n- Client operations continue\n- Failed node rejoins automatically\n\n#### Scenario 2: Network Partition\n**Steps**:\n1. Start 3-node cluster\n2. Create network partition (2 vs 1 node)\n3. Verify majority partition continues\n4. Check minority partition becomes read-only\n5. Heal partition\n6. Verify automatic reconciliation\n\n**Expected Results**:\n- Majority partition elects new leader\n- Minority partition rejects writes\n- Healing triggers state synchronization\n- No data inconsistencies\n\n#### Scenario 3: Leader Failure\n**Steps**:\n1. Identify current leader\n2. Kill leader node\n3. Verify new leader election\n4. Check subscription migration\n5. Validate continued operations\n\n**Expected Results**:\n- New leader elected within timeout\n- Subscriptions migrate to new leader\n- Client operations resume\n- Worker redistribution occurs","ref":"failure-handling.html#test-scenarios"},{"type":"extras","title":"Monitoring During Tests - Failure Handling","doc":"**Key Metrics**:\n- Response times\n- Error rates\n- Memory usage\n- CPU utilization\n- Network connectivity\n- Disk I/O\n\n**Log Analysis**:\n```bash\n# Monitor cluster events\ndocker logs -f ex-esdb-node1 | grep -E \"(NodeMonitor|Cluster|Leader)\"\n\n# Check for errors\ndocker logs ex-esdb-node1 | grep -i error\n\n# Monitor worker redistribution\ndocker logs ex-esdb-node1 | grep -i swarm\n```","ref":"failure-handling.html#monitoring-during-tests"},{"type":"extras","title":"Conclusion - Failure Handling","doc":"ExESDB provides comprehensive failure handling across all system levels, from individual process failures to cluster-wide outages. The system is designed with the BEAM's \"let it crash\" philosophy while ensuring data consistency and high availability through:\n\n**Key Strengths**:\n- **Fast Failure Detection**: 6-second node failure detection vs 10-30 second consensus timeouts\n- **Automatic Recovery**: Self-healing mechanisms at every level\n- **Data Consistency**: ACID guarantees through Raft consensus\n- **High Availability**: No single points of failure\n- **Graceful Degradation**: System continues operating with reduced capacity\n\n**Operational Benefits**:\n- **Reduced Downtime**: Automatic failover and recovery\n- **Operational Simplicity**: Minimal manual intervention required\n- **Predictable Behavior**: Well-defined failure modes and recovery procedures\n- **Monitoring Integration**: Comprehensive observability and alerting\n\n**Production Readiness**:\n- Battle-tested BEAM supervision principles\n- Proven Raft consensus implementation (Ra)\n- Comprehensive testing scenarios\n- Clear operational procedures\n\nThis failure handling strategy ensures that ExESDB can operate reliably in production environments while maintaining the flexibility and resilience that makes BEAM-based systems ideal for distributed, fault-tolerant applications.","ref":"failure-handling.html#conclusion"},{"type":"extras","title":"Logger Filtering","doc":"# Logger Filtering in ExESDB Event Store\n\nExESDB is built on top of several distributed systems components that can generate significant amounts of log noise during normal operation. To provide a better developer experience and cleaner production logs, ExESDB implements a comprehensive logger filtering system.","ref":"logger-filtering.html"},{"type":"extras","title":"Overview - Logger Filtering","doc":"ExESDB uses a two-tier approach to logger filtering:\n\n1. **BCUtils.LoggerFilters** - Provides general-purpose filtering for common BEAM distributed systems\n2. **ExESDB.LoggerFilters** - Provides specialized filtering for ExESDB's core infrastructure components\n\nThis layered approach ensures comprehensive noise reduction while maintaining full visibility into errors and warnings.","ref":"logger-filtering.html#overview"},{"type":"extras","title":"BCUtils Logger Filtering - Logger Filtering","doc":"BCUtils provides foundational logger filtering for commonly used distributed systems libraries:","ref":"logger-filtering.html#bcutils-logger-filtering"},{"type":"extras","title":"Swarm Process Registry - Logger Filtering","doc":"- Filters routine process registration/unregistration messages\n- Reduces noise from node up/down events\n- Maintains visibility into registry errors and conflicts","ref":"logger-filtering.html#swarm-process-registry"},{"type":"extras","title":"LibCluster Auto-Clustering - Logger Filtering","doc":"- Filters cluster formation and gossip protocol messages\n- Reduces connection/disconnection event noise\n- Preserves cluster formation failures and split-brain warnings","ref":"logger-filtering.html#libcluster-auto-clustering"},{"type":"extras","title":"ExESDB Logger Filtering - Logger Filtering","doc":"ExESDB extends the filtering capabilities with specialized filters for its core infrastructure:","ref":"logger-filtering.html#exesdb-logger-filtering"},{"type":"extras","title":"Ra Consensus Library Filtering - Logger Filtering","doc":"The Ra consensus library is fundamental to ExESDB's distributed operation but generates substantial log noise during normal consensus operations.\n\n**Filtered Messages:**\n- Heartbeat messages between Ra nodes\n- `append_entries` consensus protocol messages\n- `pre_vote` and `request_vote` election messages\n- Routine state transitions (follower  candidate  leader)\n- Internal Ra module operations at info/debug levels\n\n**Preserved Messages:**\n- All error and warning level messages\n- Election failures and split-brain scenarios\n- Consensus failures and recovery operations","ref":"logger-filtering.html#ra-consensus-library-filtering"},{"type":"extras","title":"Khepri Database Filtering - Logger Filtering","doc":"Khepri serves as ExESDB's distributed database backend and can be verbose about internal operations.\n\n**Filtered Messages:**\n- Cluster state synchronization messages\n- Store operation confirmations\n- Routine cluster member coordination\n- Internal Khepri module operations at info/debug levels\n\n**Preserved Messages:**\n- Database errors and transaction failures\n- Cluster coordination problems\n- Data consistency warnings","ref":"logger-filtering.html#khepri-database-filtering"},{"type":"extras","title":"Enhanced Swarm & LibCluster Filtering - Logger Filtering","doc":"ExESDB provides additional filtering beyond BCUtils for ExESDB-specific use cases:\n\n**Enhanced Swarm Filtering:**\n- ExESDB-specific process registry patterns\n- Stream coordinator registration/unregistration\n- Event emitter lifecycle messages\n\n**Enhanced LibCluster Filtering:**\n- ExESDB cluster topology changes\n- Node role transitions (leader/follower)\n- Gateway API cluster coordination","ref":"logger-filtering.html#enhanced-swarm-libcluster-filtering"},{"type":"extras","title":"Configuration - Logger Filtering","doc":"","ref":"logger-filtering.html#configuration"},{"type":"extras","title":"Development Environment - Logger Filtering","doc":"In development, you may want to see more detailed logs. Configure your logger in `config/dev.exs`:\n\n```elixir\n# Minimal filtering - see more activity\nconfig :logger, :console,\n  level: :debug,\n  format: \"[$level] $message\\n\"\n\n# Apply only critical noise reduction\nconfig :logger, \n  backends: [:console],\n  compile_time_purge_matching: [\n    [level_lower_than: :info]\n  ]\n```","ref":"logger-filtering.html#development-environment"},{"type":"extras","title":"Production Environment - Logger Filtering","doc":"For production, apply comprehensive filtering in `config/prod.exs`:\n\n```elixir\n# Apply all ExESDB logger filters\nconfig :logger,\n  backends: [:console],\n  compile_time_purge_matching: [\n    [level_lower_than: :info]\n  ]\n\n# Add custom filters\nconfig :logger, :console,\n  level: :info,\n  format: \"[$level] $message\\n\",\n  metadata_filter: [\n    {ExESDB.LoggerFilters, :filter_ra},\n    {ExESDB.LoggerFilters, :filter_khepri},\n    {ExESDB.LoggerFilters, :filter_swarm},\n    {ExESDB.LoggerFilters, :filter_libcluster}\n  ]\n```","ref":"logger-filtering.html#production-environment"},{"type":"extras","title":"Custom Filtering - Logger Filtering","doc":"You can extend the filtering system for your specific needs:\n\n```elixir\ndefmodule MyApp.CustomLoggerFilters do\n  def filter_my_component(log_event) do\n    case log_event do\n      {level, _gl, {Logger, msg, _ts, metadata}} ->\n        if should_filter_my_component?(level, msg, metadata) do\n          :stop\n        else\n          :ignore\n        end\n      _ ->\n        :ignore\n    end\n  end\n\n  defp should_filter_my_component?(level, msg, metadata) do\n    # Your custom filtering logic\n    level in [:info, :debug] and routine_operation?(msg)\n  end\nend\n```","ref":"logger-filtering.html#custom-filtering"},{"type":"extras","title":"Best Practices - Logger Filtering","doc":"","ref":"logger-filtering.html#best-practices"},{"type":"extras","title":"1. Preserve Error Visibility - Logger Filtering","doc":"Always ensure that error and warning messages are never filtered. The ExESDB filters follow this principle religiously.","ref":"logger-filtering.html#1-preserve-error-visibility"},{"type":"extras","title":"2. Filter by Message Content and Metadata - Logger Filtering","doc":"Use both message content and logger metadata to make intelligent filtering decisions:\n\n```elixir\ndefp should_filter?(level, msg, metadata) do\n  cond do\n    level in [:error, :warning] -> false\n    routine_message?(msg) and routine_module?(metadata) -> true\n    true -> false\n  end\nend\n```","ref":"logger-filtering.html#2-filter-by-message-content-and-metadata"},{"type":"extras","title":"3. Environment-Specific Configuration - Logger Filtering","doc":"Apply different filtering levels based on your environment:\n- **Development**: Minimal filtering for debugging\n- **Testing**: Aggressive filtering for clean test output\n- **Production**: Balanced filtering for operational visibility","ref":"logger-filtering.html#3-environment-specific-configuration"},{"type":"extras","title":"4. Monitor Filter Effectiveness - Logger Filtering","doc":"Regularly review your logs to ensure:\n- Important messages aren't being filtered\n- Noise levels remain manageable\n- New components don't introduce new noise patterns","ref":"logger-filtering.html#4-monitor-filter-effectiveness"},{"type":"extras","title":"Filter Performance - Logger Filtering","doc":"Logger filters are applied during log message processing and should be efficient:\n\n- Use pattern matching for quick message classification\n- Cache expensive computations where possible\n- Prefer string contains checks over regex for performance\n- Exit early from filter functions when possible","ref":"logger-filtering.html#filter-performance"},{"type":"extras","title":"Troubleshooting - Logger Filtering","doc":"","ref":"logger-filtering.html#troubleshooting"},{"type":"extras","title":"Too Much Noise - Logger Filtering","doc":"If you're still seeing too much noise:\n1. Check that filters are properly configured\n2. Identify new noise sources and extend filters\n3. Consider adjusting log levels for specific modules","ref":"logger-filtering.html#too-much-noise"},{"type":"extras","title":"Missing Important Messages - Logger Filtering","doc":"If important messages are being filtered:\n1. Review filter logic for overly broad patterns\n2. Add explicit exceptions for critical message types\n3. Test filters in development before production deployment","ref":"logger-filtering.html#missing-important-messages"},{"type":"extras","title":"Performance Issues - Logger Filtering","doc":"If logging performance is impacted:\n1. Profile filter functions for bottlenecks\n2. Optimize message matching patterns\n3. Consider compile-time filtering for high-volume noise","ref":"logger-filtering.html#performance-issues"},{"type":"extras","title":"Integration with Telemetry - Logger Filtering","doc":"ExESDB's logger filtering integrates well with Phoenix telemetry and observability tools:\n\n```elixir\n# Telemetry events are not affected by logger filtering\n:telemetry.execute([:exesdb, :stream, :read], %{duration: duration}, %{\n  stream_id: stream_id,\n  event_count: length(events)\n})\n```\n\nThis ensures that your observability and monitoring systems continue to receive all necessary operational data while keeping logs clean and readable.","ref":"logger-filtering.html#integration-with-telemetry"},{"type":"extras","title":"Conclusion - Logger Filtering","doc":"ExESDB's comprehensive logger filtering system provides a clean, production-ready logging experience while maintaining full visibility into system health and errors. By layering BCUtils general-purpose filtering with ExESDB-specific filters, developers get the best of both worlds: quiet logs during normal operation and detailed diagnostics when things go wrong.\n\nThe filtering system is designed to be:\n- **Intelligent**: Preserves all errors and warnings\n- **Comprehensive**: Covers all major noise sources\n- **Configurable**: Adaptable to different environments and needs\n- **Extensible**: Easy to add custom filters for specific requirements\n- **Performant**: Minimal impact on logging performance","ref":"logger-filtering.html#conclusion"},{"type":"extras","title":"Multiple Stores","doc":"# How ExESDB Handles Multiple Stores\n\nThis document describes the dynamic store creation and management capabilities added to ExESDB, allowing users to create multiple event stores on-demand within a single cluster.","ref":"multiple-stores.html"},{"type":"extras","title":"Overview - Multiple Stores","doc":"Previously, ExESDB was configured with a single store ID at startup. Now, with the `ExESDB.StoreManager`, users can:\n\n- Create new stores dynamically at runtime\n- Remove stores when no longer needed\n- List and query store status and configuration\n- Use multiple stores simultaneously in the same cluster","ref":"multiple-stores.html#overview"},{"type":"extras","title":"Architecture - Multiple Stores","doc":"","ref":"multiple-stores.html#architecture"},{"type":"extras","title":"Components - Multiple Stores","doc":"1. **ExESDB.StoreManager**: The core GenServer that manages multiple Khepri stores\n2. **ExESDBGater.API**: Updated API with store management functions\n3. **ExESDB.GatewayWorker**: Updated to handle store management operations","ref":"multiple-stores.html#components"},{"type":"extras","title":"How It Works - Multiple Stores","doc":"1. The `StoreManager` replaces the single `Store` process in the supervision tree\n2. Each store gets its own unique data directory under the base data directory\n3. Stores are managed independently but share the same cluster infrastructure\n4. All existing stream, subscription, and snapshot operations work with any managed store","ref":"multiple-stores.html#how-it-works"},{"type":"extras","title":"API Reference - Multiple Stores","doc":"","ref":"multiple-stores.html#api-reference"},{"type":"extras","title":"Creating a Store - Multiple Stores","doc":"```elixir\n# Fire-and-forget operation\n:ok = ExESDBGater.API.create_store(:my_new_store, [timeout: 15_000])\n```","ref":"multiple-stores.html#creating-a-store"},{"type":"extras","title":"Removing a Store - Multiple Stores","doc":"```elixir\n# Fire-and-forget operation\n:ok = ExESDBGater.API.remove_store(:my_store)\n```","ref":"multiple-stores.html#removing-a-store"},{"type":"extras","title":"Listing Stores - Multiple Stores","doc":"```elixir\n{:ok, stores} = ExESDBGater.API.list_stores()\n# Returns: %{store_id => %{status: :running, config: [...]}}\n```","ref":"multiple-stores.html#listing-stores"},{"type":"extras","title":"Getting Store Status - Multiple Stores","doc":"```elixir\n{:ok, :running} = ExESDBGater.API.get_store_status(:my_store)\n```","ref":"multiple-stores.html#getting-store-status"},{"type":"extras","title":"Getting Store Configuration - Multiple Stores","doc":"```elixir\n{:ok, config} = ExESDBGater.API.get_store_config(:my_store)\n```","ref":"multiple-stores.html#getting-store-configuration"},{"type":"extras","title":"Store Operations - Multiple Stores","doc":"Once a store is created, you can use it with all existing operations:\n\n```elixir\n# Append events to a specific store\n{:ok, version} = ExESDBGater.API.append_events(:my_store, \"stream-1\", events)\n\n# Read events from a specific store\n{:ok, events} = ExESDBGater.API.get_events(:my_store, \"stream-1\", 0, 10)\n\n# List streams in a specific store\n{:ok, streams} = ExESDBGater.API.get_streams(:my_store)\n\n# Create subscriptions for a specific store\n:ok = ExESDBGater.API.save_subscription(:my_store, :by_stream, \"$all\", \"my_sub\")\n```","ref":"multiple-stores.html#store-operations"},{"type":"extras","title":"Configuration - Multiple Stores","doc":"","ref":"multiple-stores.html#configuration"},{"type":"extras","title":"Default Store - Multiple Stores","doc":"The system still creates a default store on startup using the existing configuration:\n\n```elixir\n# In runtime.exs\nconfig :ex_esdb, :khepri,\n  data_dir: data_dir(),\n  store_id: store_id(),  # This becomes the default store\n  timeout: timeout(),\n  db_type: db_type(),\n  pub_sub: pub_sub()\n```","ref":"multiple-stores.html#default-store"},{"type":"extras","title":"Dynamic Store Configuration - Multiple Stores","doc":"New stores inherit the default configuration but can override specific settings:\n\n```elixir\nExESDBGater.API.create_store(:custom_store, [\n  timeout: 20_000,        # Custom timeout\n  # data_dir is automatically set to base_dir/custom_store\n])\n```","ref":"multiple-stores.html#dynamic-store-configuration"},{"type":"extras","title":"Data Storage - Multiple Stores","doc":"Each store gets its own data directory:\n\n```\n/data/\n ex_esdb_store/          # Default store\n user_data_store/        # Custom store 1\n analytics_store/        # Custom store 2\n audit_logs_store/       # Custom store 3\n```","ref":"multiple-stores.html#data-storage"},{"type":"extras","title":"Use Cases - Multiple Stores","doc":"","ref":"multiple-stores.html#use-cases"},{"type":"extras","title":"Multi-Tenant Applications - Multiple Stores","doc":"Create separate stores for each tenant:\n\n```elixir\n# Create tenant-specific stores\nExESDBGater.API.create_store(:tenant_123_store)\nExESDBGater.API.create_store(:tenant_456_store)\n\n# Use tenant-specific store for operations\nExESDBGater.API.append_events(:tenant_123_store, \"orders\", events)\n```","ref":"multiple-stores.html#multi-tenant-applications"},{"type":"extras","title":"Domain Separation - Multiple Stores","doc":"Create stores for different business domains:\n\n```elixir\n# Separate stores by domain\nExESDBGater.API.create_store(:user_management_store)\nExESDBGater.API.create_store(:order_processing_store)\nExESDBGater.API.create_store(:analytics_store)\n```","ref":"multiple-stores.html#domain-separation"},{"type":"extras","title":"Environment-Specific Stores - Multiple Stores","doc":"Create stores for different purposes:\n\n```elixir\n# Development/testing stores\nExESDBGater.API.create_store(:test_store)\nExESDBGater.API.create_store(:staging_store)\n```","ref":"multiple-stores.html#environment-specific-stores"},{"type":"extras","title":"Cluster Behavior - Multiple Stores","doc":"- Stores are created on the node that receives the request\n- Khepri handles replication across the cluster automatically\n- Each store maintains its own Raft consensus group\n- Store operations are distributed across cluster nodes via Swarm","ref":"multiple-stores.html#cluster-behavior"},{"type":"extras","title":"Backward Compatibility - Multiple Stores","doc":"The changes are fully backward compatible:\n\n- Existing single-store configurations continue to work\n- All existing APIs work with the default store\n- No migration is required for existing deployments","ref":"multiple-stores.html#backward-compatibility"},{"type":"extras","title":"Best Practices - Multiple Stores","doc":"","ref":"multiple-stores.html#best-practices"},{"type":"extras","title":"Store Naming - Multiple Stores","doc":"Use descriptive, unique atom names:\n\n```elixir\n# Good\n:user_events_store\n:order_processing_store\n:analytics_events_store\n\n# Avoid\n:store1\n:store\n:temp\n```","ref":"multiple-stores.html#store-naming"},{"type":"extras","title":"Resource Management - Multiple Stores","doc":"- Monitor store count to avoid resource exhaustion\n- Remove unused stores to free up resources\n- Consider store lifecycle in your application design","ref":"multiple-stores.html#resource-management"},{"type":"extras","title":"Configuration - Multiple Stores","doc":"- Use consistent timeout values for related stores\n- Plan data directory structure for backup/restore operations\n- Consider store-specific configuration needs","ref":"multiple-stores.html#configuration-1"},{"type":"extras","title":"Monitoring - Multiple Stores","doc":"To monitor store health:\n\n```elixir\n# Get all stores and their status\n{:ok, stores} = ExESDBGater.API.list_stores()\n\nfor {store_id, info} <- stores do\n  IO.puts(\"Store #{store_id}: #{info.status}\")\nend\n```","ref":"multiple-stores.html#monitoring"},{"type":"extras","title":"Error Handling - Multiple Stores","doc":"Common error scenarios:\n\n```elixir\n# Store already exists\n{:error, :already_exists} = ExESDBGater.API.create_store(:existing_store)\n\n# Store not found\n{:error, :not_found} = ExESDBGater.API.get_store_status(:nonexistent_store)\n{:error, :not_found} = ExESDBGater.API.remove_store(:nonexistent_store)\n```","ref":"multiple-stores.html#error-handling"},{"type":"extras","title":"Migration Guide - Multiple Stores","doc":"If you're currently using a single store and want to adopt multiple stores:\n\n1. **No immediate action required** - your existing setup continues to work\n2. **Gradual migration** - start creating new stores for new features\n3. **Optional consolidation** - consider reorganizing existing data into domain-specific stores","ref":"multiple-stores.html#migration-guide"},{"type":"extras","title":"Performance Considerations - Multiple Stores","doc":"- Each store has its own Khepri cluster member\n- Memory usage scales with the number of stores\n- Network traffic increases with store count due to more Raft groups\n- Consider store count limits based on cluster capacity","ref":"multiple-stores.html#performance-considerations"},{"type":"extras","title":"Security Considerations - Multiple Stores","doc":"- Store creation/removal should be restricted to authorized operations\n- Consider implementing store-level access controls in your application\n- Monitor store creation for unauthorized usage","ref":"multiple-stores.html#security-considerations"},{"type":"extras","title":"Limitations - Multiple Stores","doc":"- Store IDs must be valid Elixir atoms\n- Each store requires cluster resources (memory, network)\n- Maximum practical store count depends on cluster capacity\n- Store removal is immediate and irreversible","ref":"multiple-stores.html#limitations"},{"type":"extras","title":"Future Enhancements - Multiple Stores","doc":"Potential future improvements:\n\n- Store templates for consistent configuration\n- Store migration utilities\n- Store-level metrics and monitoring\n- Automatic store cleanup policies\n- Store backup/restore functionality","ref":"multiple-stores.html#future-enhancements"},{"type":"extras","title":"Configuring ExESDB Applications","doc":"# Configuring ExESDB Applications\n\nThis guide explains how to configure ExESDB in both standalone and umbrella applications.","ref":"configuring-exesdb-apps.html"},{"type":"extras","title":"Configuration Overview - Configuring ExESDB Applications","doc":"ExESDB supports two deployment patterns:\n\n1. **Standalone Applications**: A single OTP application that uses ExESDB\n2. **Umbrella Applications**: Multiple child applications within an umbrella project, each with their own ExESDB configuration\n\nThe configuration format is always: `config :your_app_name, :ex_esdb, [options]` where:\n- `:your_app_name` is the name of your OTP application\n- `:ex_esdb` is the configuration namespace that ExESDB looks for\n- `[options]` are the ExESDB configuration options","ref":"configuring-exesdb-apps.html#configuration-overview"},{"type":"extras","title":"Standalone Application Configuration - Configuring ExESDB Applications","doc":"For a standalone application, configure ExESDB under your application's name:\n\n```elixir\n# config/config.exs\n# If your app is named :my_event_store\nconfig :my_event_store, :ex_esdb,\n  data_dir: \"/var/lib/ex_esdb\",\n  store_id: :my_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :my_pubsub,\n  reader_idle_ms: 15000,\n  writer_idle_ms: 12000,\n  store_description: \"My Event Store\",\n  store_tags: [\"production\", \"events\"]\n\n# Libcluster configuration (recommended over seed_nodes)\nconfig :libcluster,\n  topologies: [\n    example: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"my_secret\"\n      ]\n    ]\n  ]\n```","ref":"configuring-exesdb-apps.html#standalone-application-configuration"},{"type":"extras","title":"Usage in Standalone Apps - Configuring ExESDB Applications","doc":"```elixir\n# Uses the default context (your app name)\nExESDB.Options.data_dir()          # \"/var/lib/ex_esdb\"\nExESDB.Options.store_id()          # :my_store\nExESDB.Options.db_type()           # :cluster\n```","ref":"configuring-exesdb-apps.html#usage-in-standalone-apps"},{"type":"extras","title":"Umbrella Application Configuration - Configuring ExESDB Applications","doc":"For umbrella applications, each child application can have its own ExESDB configuration:\n\n```elixir\n# config/config.exs (umbrella root)\n\n# Child app 1: orders service\nconfig :orders_service, :ex_esdb,\n  data_dir: \"/var/lib/orders_events\",\n  store_id: :orders_store,\n  timeout: 5000,\n  db_type: :cluster,\n  pub_sub: :orders_pubsub,\n  reader_idle_ms: 10000,\n  writer_idle_ms: 8000,\n  store_description: \"Orders Event Store\",\n  store_tags: [\"orders\", \"production\"]\n\n# Child app 2: inventory service\nconfig :inventory_service, :ex_esdb,\n  data_dir: \"/var/lib/inventory_events\",\n  store_id: :inventory_store,\n  timeout: 3000,\n  db_type: :single,\n  pub_sub: :inventory_pubsub,\n  reader_idle_ms: 12000,\n  writer_idle_ms: 10000,\n  store_description: \"Inventory Event Store\",\n  store_tags: [\"inventory\", \"production\"]\n\n# Child app 3: user service\nconfig :user_service, :ex_esdb,\n  data_dir: \"/var/lib/user_events\",\n  store_id: :user_store,\n  timeout: 4000,\n  db_type: :cluster,\n  pub_sub: :user_pubsub,\n  reader_idle_ms: 8000,\n  writer_idle_ms: 6000,\n  store_description: \"User Event Store\",\n  store_tags: [\"users\", \"auth\", \"production\"]\n\n# Shared libcluster configuration\nconfig :libcluster,\n  topologies: [\n    umbrella_cluster: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"230.1.1.251\",\n        multicast_ttl: 1,\n        secret: \"umbrella_secret\"\n      ]\n    ]\n  ]\n```","ref":"configuring-exesdb-apps.html#umbrella-application-configuration"},{"type":"extras","title":"Usage in Umbrella Apps - Configuring ExESDB Applications","doc":"ExESDB provides several ways to work with different contexts in umbrella applications:\n\n#### Setting Context\n\n```elixir\n# Set context for orders service\nExESDB.Options.set_context(:orders_service)\nExESDB.Options.data_dir()          # \"/var/lib/orders_events\"\nExESDB.Options.store_id()          # :orders_store\n```\n\n#### Explicit Context\n\n```elixir\n# Use with explicit context\nExESDB.Options.data_dir(:inventory_service)     # \"/var/lib/inventory_events\"\nExESDB.Options.store_id(:user_service)          # :user_store\n```\n\n#### Context Wrapper\n\n```elixir\n# Use with context wrapper\nExESDB.Options.with_context(:orders_service, fn ->\n  ExESDB.Options.db_type()         # :cluster\n  ExESDB.Options.timeout()         # 5000\nend)\n```","ref":"configuring-exesdb-apps.html#usage-in-umbrella-apps"},{"type":"extras","title":"Configuration Options - Configuring ExESDB Applications","doc":"| Option | Type | Default | Description |\n|--------|------|---------|-------------|\n| `data_dir` | String | `\"/data\"` | Directory for storing event data |\n| `store_id` | Atom | `:undefined` | Unique identifier for the store |\n| `timeout` | Integer | `10_000` | Timeout in milliseconds |\n| `db_type` | Atom | `:single` | Database type (`:single` or `:cluster`) |\n| `pub_sub` | Atom | `:ex_esdb_pubsub` | PubSub module name |\n| `reader_idle_ms` | Integer | `10_000` | Reader idle timeout in milliseconds |\n| `writer_idle_ms` | Integer | `10_000` | Writer idle timeout in milliseconds |\n| `store_description` | String | `\"undefined!\"` | Human-readable store description |\n| `store_tags` | List | `[]` | List of tags for the store |\n| `persistence_interval` | Integer | `5_000` | Persistence interval in milliseconds |\n| `persistence_enabled` | Boolean | `true` | Whether persistence is enabled |","ref":"configuring-exesdb-apps.html#configuration-options"},{"type":"extras","title":"Environment Variable Overrides - Configuring ExESDB Applications","doc":"All configuration options can be overridden using environment variables, which take precedence over application configuration:\n\n```bash\n# These will override any app configuration\nexport EX_ESDB_DATA_DIR=\"/tmp/events\"\nexport EX_ESDB_STORE_ID=\"temp_store\"\nexport EX_ESDB_DB_TYPE=\"single\"\nexport EX_ESDB_TIMEOUT=\"2000\"\nexport EX_ESDB_PUB_SUB=\"temp_pubsub\"\nexport EX_ESDB_READER_IDLE_MS=\"5000\"\nexport EX_ESDB_WRITER_IDLE_MS=\"4000\"\nexport EX_ESDB_STORE_DESCRIPTION=\"Temporary Store\"\nexport EX_ESDB_STORE_TAGS=\"temp,testing\"\nexport EX_ESDB_PERSISTENCE_INTERVAL=\"10000\"\nexport EX_ESDB_PERSISTENCE_ENABLED=\"true\"\n```\n\nEnvironment variables are especially useful for:\n- Development vs production configurations\n- Container deployments\n- Testing scenarios\n- Runtime configuration changes","ref":"configuring-exesdb-apps.html#environment-variable-overrides"},{"type":"extras","title":"Clustering Configuration - Configuring ExESDB Applications","doc":"ExESDB is designed to work with `libcluster` for node discovery and clustering. The old `seed_nodes` mechanism has been deprecated in favor of `libcluster`'s more robust topology strategies.","ref":"configuring-exesdb-apps.html#clustering-configuration"},{"type":"extras","title":"Libcluster Strategies - Configuring ExESDB Applications","doc":"Common strategies include:\n\n- **Gossip**: For local network discovery\n- **Kubernetes**: For Kubernetes deployments\n- **ECS**: For AWS ECS deployments\n- **EpMD**: For Erlang Port Mapper Daemon\n- **DNS**: For DNS-based discovery\n\nRefer to the [libcluster documentation](https://hexdocs.pm/libcluster/) for detailed configuration options.","ref":"configuring-exesdb-apps.html#libcluster-strategies"},{"type":"extras","title":"Best Practices - Configuring ExESDB Applications","doc":"1. **Use libcluster**: Always prefer `libcluster` over manual seed nodes configuration\n2. **Isolate configurations**: In umbrella apps, keep each service's configuration separate\n3. **Environment-specific configs**: Use environment variables for deployment-specific settings\n4. **Meaningful names**: Use descriptive `store_id` and `store_description` values\n5. **Tagging**: Use `store_tags` for operational visibility and monitoring\n6. **Resource sizing**: Adjust timeout and idle settings based on your workload characteristics","ref":"configuring-exesdb-apps.html#best-practices"},{"type":"extras","title":"Migration from Legacy Configuration - Configuring ExESDB Applications","doc":"If you're migrating from the legacy `khepri` configuration format, update your configuration from:\n\n```elixir\n# Old format (deprecated)\nconfig :ex_esdb, :khepri, [options]\n```\n\nTo:\n\n```elixir\n# New format\nconfig :your_app_name, :ex_esdb, [options]\n```\n\nThe new format provides better isolation and supports umbrella applications more effectively.","ref":"configuring-exesdb-apps.html#migration-from-legacy-configuration"},{"type":"extras","title":"Persistence Architecture","doc":"# ExESDB Asynchronous Persistence Architecture","ref":"persistence-architecture.html"},{"type":"extras","title":"Overview - Persistence Architecture","doc":"The ExESDB persistence system has been redesigned to handle disk persistence operations asynchronously, eliminating timeout issues that were occurring during event append operations.","ref":"persistence-architecture.html#overview"},{"type":"extras","title":"Problem Statement - Persistence Architecture","doc":"In version 0.3.3, synchronous fence operations were introduced to ensure data persistence to disk. However, these operations caused significant performance issues:\n\n1. **Blocking Operations**: Synchronous `khepri.fence()` calls blocked event append operations\n2. **Timeout Failures**: Operations would timeout after 5 seconds, causing command failures\n3. **Poor User Experience**: System appeared unresponsive during data-heavy operations","ref":"persistence-architecture.html#problem-statement"},{"type":"extras","title":"Solution: Asynchronous Persistence System - Persistence Architecture","doc":"","ref":"persistence-architecture.html#solution-asynchronous-persistence-system"},{"type":"extras","title":"Architecture Components - Persistence Architecture","doc":"#### 1. PersistenceWorker (`ex_esdb/persistence_worker.ex`)\n\nA dedicated GenServer that handles all disk persistence operations asynchronously:\n\n- **Batching**: Collects multiple persistence requests and processes them together\n- **Periodic Execution**: Runs every 5 seconds (configurable) to persist pending data\n- **Non-blocking**: Event append operations return immediately without waiting for disk writes\n- **Graceful Shutdown**: Ensures all pending data is persisted before termination\n\n#### 2. Modified StreamsWriterWorker (`ex_esdb/streams_writer_worker.ex`)\n\nThe event append process now:\n- Stores events in memory immediately\n- Queues a persistence request to the PersistenceWorker\n- Returns success without waiting for disk persistence\n\n#### 3. Enhanced PersistenceSystem (`ex_esdb/persistence_system.ex`)\n\nThe supervisor now includes the PersistenceWorker as a managed component.","ref":"persistence-architecture.html#architecture-components"},{"type":"extras","title":"Benefits - Persistence Architecture","doc":"1. **Immediate Response**: Event append operations return instantly\n2. **Better Throughput**: Batched disk operations are more efficient\n3. **Configurable Intervals**: Persistence frequency can be tuned per environment\n4. **Fault Tolerance**: Failed persistence operations are retried automatically\n5. **Graceful Degradation**: System continues operating even if persistence is delayed","ref":"persistence-architecture.html#benefits"},{"type":"extras","title":"Configuration - Persistence Architecture","doc":"The persistence system can be configured in several ways:\n\n#### Application Configuration\n```elixir\n# Global configuration\nconfig :ex_esdb, \n  persistence_interval: 10_000,  # 10 seconds\n  persistence_enabled: true       # Enable/disable persistence\n\n# Per-application configuration (umbrella apps)\nconfig :my_app, :ex_esdb,\n  persistence_interval: 5_000,    # 5 seconds\n  persistence_enabled: true\n```\n\n#### Environment Variables\n```bash\n# Persistence interval in milliseconds\nexport EX_ESDB_PERSISTENCE_INTERVAL=10000\n\n# Enable or disable persistence\nexport EX_ESDB_PERSISTENCE_ENABLED=true\n```\n\n#### Runtime Configuration\n```elixir\n# Per-store configuration at startup\nopts = [\n  store_id: :my_store, \n  persistence_interval: 5_000,\n  otp_app: :my_app\n]\n```\n\n#### Configuration Options\n\n- **`persistence_interval`**: Time in milliseconds between persistence cycles (default: 5000)\n- **`persistence_enabled`**: Whether persistence is enabled (default: true)\n- **`otp_app`**: OTP application name for umbrella app configurations","ref":"persistence-architecture.html#configuration"},{"type":"extras","title":"API Usage - Persistence Architecture","doc":"#### Request Asynchronous Persistence\n```elixir\n# Non-blocking call to request persistence\nExESDB.PersistenceWorker.request_persistence(:my_store)\n```\n\n#### Force Immediate Persistence\n```elixir\n# Blocking call for immediate persistence (useful for testing/shutdown)\nExESDB.PersistenceWorker.force_persistence(:my_store)\n```","ref":"persistence-architecture.html#api-usage"},{"type":"extras","title":"Implementation Details - Persistence Architecture","doc":"#### Event Flow\n1. Client calls `append_events`\n2. Event is written to Khepri in-memory store\n3. Persistence request is queued with PersistenceWorker\n4. Success is returned immediately to client\n5. PersistenceWorker processes persistence in background\n\n#### Persistence Batching\n- Multiple persistence requests for the same store are deduplicated\n- Periodic timer processes all pending stores together\n- Failed persistence operations are logged but don't affect event storage\n\n#### Error Handling\n- Persistence failures are logged but don't interrupt event processing\n- System continues to operate with in-memory data\n- Persistence is retried on next interval","ref":"persistence-architecture.html#implementation-details"},{"type":"extras","title":"Testing - Persistence Architecture","doc":"The system includes comprehensive testing support:\n\n```elixir\n# For integration tests, force persistence before assertions\nExESDB.PersistenceWorker.force_persistence(:test_store)\n\n# Verify events are persisted\nassert {:ok, events} = ExESDB.get_events(:test_store, stream_id)\n```","ref":"persistence-architecture.html#testing"},{"type":"extras","title":"Migration Notes - Persistence Architecture","doc":"#### From Synchronous to Asynchronous Persistence\n\n- **Immediate Effect**: Event append operations will be significantly faster\n- **Eventual Consistency**: Data is eventually persisted (within persistence interval)\n- **Testing Impact**: Tests may need to call `force_persistence` before assertions\n- **Configuration**: Default 5-second interval can be adjusted per requirements\n\n#### Backward Compatibility\n\nThe changes are fully backward compatible:\n- Existing APIs continue to work unchanged\n- No changes required to client code\n- Configuration is optional (sensible defaults provided)","ref":"persistence-architecture.html#migration-notes"},{"type":"extras","title":"Performance Characteristics - Persistence Architecture","doc":"#### Before (Synchronous)\n- Event append time: ~5+ seconds (with fence operation)\n- Frequent timeouts under load\n- Poor user experience\n\n#### After (Asynchronous)\n- Event append time: ~10-50ms (memory write only)\n- No timeouts\n- Smooth user experience\n- Configurable persistence latency","ref":"persistence-architecture.html#performance-characteristics"},{"type":"extras","title":"Monitoring - Persistence Architecture","doc":"The system provides comprehensive logging:\n\n```\n[info] PersistenceWorker[my_store] is UP\n[debug] PersistenceWorker[my_store] persisting 3 stores\n[debug] Successfully persisted store my_store\n```","ref":"persistence-architecture.html#monitoring"},{"type":"extras","title":"Security Considerations - Persistence Architecture","doc":"- Data is stored in memory immediately, so it's not lost on process restart\n- Khepri handles in-memory to disk persistence reliably\n- Graceful shutdown ensures no data loss during system shutdown","ref":"persistence-architecture.html#security-considerations"},{"type":"extras","title":"Conclusion - Persistence Architecture","doc":"The asynchronous persistence architecture eliminates timeout issues while maintaining data durability. The system is now responsive, scalable, and provides a better user experience while ensuring data integrity through reliable background persistence.","ref":"persistence-architecture.html#conclusion"},{"type":"extras","title":"Implementation Guidelines","doc":"# General Implementation Guidelines for Reckon_* Applications","ref":"implementation-guidelines.html"},{"type":"extras","title":"Introduction: Why Vertical Slicing and Screaming Architecture? - Implementation Guidelines","doc":"Traditional layered architectures organize code by **technical concerns**controllers, services, repositories, models. This approach creates several problems:","ref":"implementation-guidelines.html#introduction-why-vertical-slicing-and-screaming-architecture"},{"type":"extras","title":"Problems with Layered Architecture - Implementation Guidelines","doc":"1. **Scattered Business Logic**: A single business operation spans multiple layers and directories\n2. **Cognitive Overhead**: Developers must navigate between layers to understand one feature\n3. **Tight Coupling**: Changes often require modifications across multiple layers\n4. **Testing Complexity**: Integration tests become necessary to verify simple business operations\n5. **Team Conflicts**: Multiple developers working on the same layers create merge conflicts\n6. **Hidden Business Intent**: The codebase doesn't reveal what the system actually does","ref":"implementation-guidelines.html#problems-with-layered-architecture"},{"type":"extras","title":"Why We Chose Vertical Slicing - Implementation Guidelines","doc":"**Vertical slicing** organizes code by **business capabilities** instead of technical layers. Each business operation becomes a self-contained \"slice\" with everything it needs:\n\n- **Single Source of Truth**: All code for one business operation lives together\n- **Reduced Cognitive Load**: Developers see the complete feature in one place\n- **Independent Evolution**: Features can change without affecting others\n- **Simplified Testing**: Each slice can be tested in isolation\n- **Team Autonomy**: Different teams can own different slices\n- **Business Alignment**: Code structure mirrors how the business thinks","ref":"implementation-guidelines.html#why-we-chose-vertical-slicing"},{"type":"extras","title":"Why We Chose Screaming Architecture - Implementation Guidelines","doc":"**Screaming Architecture** (Uncle Bob's term) means your codebase \"screams\" its business intent. When someone looks at the folder structure, they immediately understand:\n\n- **What the system does** (not how it's implemented)\n- **What business operations exist** (initialize_account, cast_vote)\n- **What the domain is about** (voting, accounts, profiles)\n\nInstead of seeing generic technical folders like `controllers/`, `services/`, `models/`, you see business-focused folders like `initialize_poll/`, `cast_vote/`, `activate_account/`.","ref":"implementation-guidelines.html#why-we-chose-screaming-architecture"},{"type":"extras","title":"Real-World Benefits - Implementation Guidelines","doc":"This architecture has proven benefits in production systems:\n\n- **Faster Onboarding**: New developers understand the system quickly\n- **Reduced Bugs**: Business logic stays cohesive and isolated\n- **Better Documentation**: The code structure IS the documentation\n- **Easier Refactoring**: Changes are localized to specific slices\n- **Business Conversations**: Non-technical stakeholders can navigate the codebase","ref":"implementation-guidelines.html#real-world-benefits"},{"type":"extras","title":"Architecture Philosophy - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#architecture-philosophy"},{"type":"extras","title":"Vertical Slicing Architecture - Implementation Guidelines","doc":"The reckon_* applications follow a **vertical slicing architecture** where each business operation (use case) is implemented as a self-contained slice that includes all necessary components:\n\n- **Command**: Input structure defining the operation\n- **Event**: Output structure representing what happened\n- **Handler**: Business logic processing the command and emitting events\n- **Projections**: Read models updated by events (when needed)\n\nThis approach ensures that each business capability is:\n- **Isolated**: Changes to one slice don't affect others\n- **Cohesive**: All related code is co-located\n- **Testable**: Each slice can be tested independently\n- **Discoverable**: Business operations are obvious from the folder structure","ref":"implementation-guidelines.html#vertical-slicing-architecture"},{"type":"extras","title":"Screaming Architecture - Implementation Guidelines","doc":"The codebase \"screams\" its business intent through:\n\n1. **Directory Structure**: Business operations are immediately visible as top-level folders\n2. **Module Names**: Clearly express business concepts, not technical layers\n3. **File Organization**: Business logic is organized by use case, not by technical pattern","ref":"implementation-guidelines.html#screaming-architecture"},{"type":"extras","title":" MANDATORY Event Storming Requirement - Implementation Guidelines","doc":"**RULE**: Each new project MUST start with an initial Event Storming session, of which the result will be written in the apps `./design_docs/event-storming.md` document.\n\nThis document is a timestamped log of event storming sessions and will contain the following sections:\n\n1. **Executive Summary** (a description of the process under design)\n2. **ASCII diagram** that depicts the commands, the policies they are called from, the events they emit\n3. **ASCII code structure diagram**\n4. **Description of each slice**\n\nThe event storming document serves as the foundation for implementing the vertical slicing architecture and ensures all stakeholders understand the business processes before development begins.","ref":"implementation-guidelines.html#mandatory-event-storming-requirement"},{"type":"extras","title":" STRICT VERSIONING RULES - Implementation Guidelines","doc":"**These rules are MANDATORY and MUST be followed without exception:**","ref":"implementation-guidelines.html#strict-versioning-rules"},{"type":"extras","title":"1. All Commands and Events MUST Have Versions - Implementation Guidelines","doc":"- **Commands**: `InitializePollV1`, `CastVoteV1`\n- **Events**: `PollInitializedV1`, `VoteCastedV1`\n- **Event Type Strings**: `\"poll_initialized:v1\"`, `\"vote_casted:v1\"`","ref":"implementation-guidelines.html#1-all-commands-and-events-must-have-versions"},{"type":"extras","title":"2. Directory Names Are Clean (No Version Suffix) - Implementation Guidelines","doc":"- **Format**: `[command_name]/` (no version in directory name)\n- **Examples**: `initialize_poll/`, `cast_vote/`, `activate_account/`\n- **Rationale**: Prevents unwieldy module names like `InitializePollV1.CommandV1`","ref":"implementation-guidelines.html#2-directory-names-are-clean-no-version-suffix"},{"type":"extras","title":"3. File Names MUST Include Version - Implementation Guidelines","doc":"- **Files**: `command_v1.ex`, `event_v1.ex`, `maybe_initialize_poll_v1.ex`\n- **Event Handlers**: `initialized_to_state_v1.ex`, `casted_to_summary_v1.ex`\n- **Projections**: `initialized_to_summary_v1.ex`\n- **Evolution**: V2 files coexist in same directory: `command_v2.ex`, `event_v2.ex`","ref":"implementation-guidelines.html#3-file-names-must-include-version"},{"type":"extras","title":"4.  FORBIDDEN: CRUD Events - Implementation Guidelines","doc":"**CRUD events have NO business meaning and are STRICTLY FORBIDDEN:**\n\n** FORBIDDEN CRUD Events:**\n- `PollCreated`, `AccountUpdated`, `UserDeleted`\n- `ProfileCreated`, `MembershipUpdated`\n- Any event ending in `-Created`, `-Updated`, `-Deleted`\n\n** REQUIRED: Business-Meaningful Events:**\n- `PollInitialized`, `AccountActivated`, `UserSuspended`\n- `ProfileEstablished`, `MembershipGranted`, `MembershipExpired`\n- Events that express **business intent and meaning**","ref":"implementation-guidelines.html#4-forbidden-crud-events"},{"type":"extras","title":"Project Structure - Implementation Guidelines","doc":"```\nreckon_[domain]/\n lib/\n    reckon_[domain]/\n       application.ex           # OTP application bootstrap\n       repo.ex                  # Ecto repository for read models\n       router.ex                # Commanded router for command dispatch\n       shared/\n          [aggregate].ex       # Domain aggregate root\n      \n       domain/                  #  LITERAL \"domain\" directory (invariant)\n          [command_name]/      #  VERTICAL SLICE (named after COMMAND)\n             command.ex       # Command structure (input)\n             event.ex         # Event structure (output)\n             handler.ex       # Command handler (business logic)\n             [event]_to_state.ex                    # Event handler (aggregate updates)\n             [event]_to_[readmodel].ex              # Projection (optional)\n             when_[event]_then_[command].ex         # Policy (optional)\n         \n          [another_command]/   # Another vertical slice\n              command.ex\n              event.ex\n              handler.ex\n              [event]_to_state.ex\n              ...\n      \n       projections/             # Cross-cutting read models (if needed)\n           [projection_name].ex\n   \n    reckon_[domain].ex           # Public API (business facade)\n    mix/\n        tasks/\n            reckon_[domain].setup.ex\n            reckon_[domain].reset.ex\n config/\n    config.exs                   # Main config with ExESDB setup\n    dev.exs\n    test.exs\n    prod.exs\n test/\n priv/\n mix.exs\n```","ref":"implementation-guidelines.html#project-structure"},{"type":"extras","title":"Core Principles - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#core-principles"},{"type":"extras","title":"1. One Capability Per Module Strategy - Implementation Guidelines","doc":"**FUNDAMENTAL RULE**: We follow a strict 1-capability-per-module strategy across all components:\n\n- **1 Command per Module**: Each command gets its own dedicated module\n- **1 Event per Module**: Each event gets its own dedicated module  \n- **1 Handler per Module**: Each handler processes one command type\n- **1 Projection per Module**: Each projection handles one event type to one read model\n- **1 Policy per Module**: Each policy (process manager) handles one event-to-command translation\n\nThis ensures maximum cohesion, testability, and discoverability.","ref":"implementation-guidelines.html#1-one-capability-per-module-strategy"},{"type":"extras","title":"2. Each Command/Event Must Reside in Its Own Module - Implementation Guidelines","doc":"** WRONG - Grouping multiple commands/events in one module:**\n```elixir\ndefmodule ReckonProfiles.Commands do\n  defmodule CreateProfile do\n    # ...\n  end\n  \n  defmodule UpdateProfile do\n    # ...\n  end\nend\n```\n\n** CORRECT - Each command/event in its own dedicated module:**\n```elixir\n# lib/reckon_profiles/create_profile/command.ex\ndefmodule ReckonProfiles.CreateProfile.Command do\n  # ...\nend\n\n# lib/reckon_profiles/update_profile/command.ex\ndefmodule ReckonProfiles.UpdateProfile.Command do\n  # ...\nend\n```","ref":"implementation-guidelines.html#2-each-command-event-must-reside-in-its-own-module"},{"type":"extras","title":"3. Vertical Slice Structure - Implementation Guidelines","doc":"Each business operation follows this exact pattern:\n\n```\ndomain/\n [command_name]/                      #  SLICE named after COMMAND (no version)\n     command_v1.ex                    # Input structure - what the user wants to do\n     event_v1.ex                      # Output structure - what actually happened\n     maybe_[command]_v1.ex            # Command handler - business logic\n     [event]_to_state_v1.ex           # Event handler - aggregate state updates\n     [event]_to_[readmodel]_v1.ex     # Projection (optional)\n     when_[event]_then_[command]_v1.ex # Policy (optional)\n    \n    # V2 Evolution (coexists in same directory)\n     command_v2.ex                    # Updated command structure\n     event_v2.ex                      # Updated event structure\n     maybe_[command]_v2.ex            # Updated command handler\n```\n\n**Key Points:**\n- All slices go under the literal `domain/` directory\n- Slices are named after the **command** they contain (not the business operation)\n- `maybe_[command].ex` = **Command Handler** (processes commands, emits events)\n- `[event]_to_state.ex` = **Event Handler** (applies events to aggregate state)\n- Policies go in the slice where the **command** they trigger is located","ref":"implementation-guidelines.html#3-vertical-slice-structure"},{"type":"extras","title":"4. Strict Projection Naming Convention - Implementation Guidelines","doc":"**MANDATORY FORMAT**: Projections MUST be named using the exact format ` _to_ .ex`\n\n**Rules:**\n- Projections belong in the same slice as the event they process\n- File name format: `[lowercase_event_name]_to_[readmodel_name].ex`\n- Module name format: `Domain.BusinessOperation.EventToReadmodel`\n- Each projection handles exactly ONE event type to ONE read model\n\n**Examples:**\n```\n# File: lib/reckon_accounts/initialize_account/initialized_to_summary.ex\n# Module: ReckonAccounts.InitializeAccount.InitializedToSummary\n# Processes: AccountInitialized  AccountSummary\n\n# File: lib/reckon_profiles/establish_profile/established_to_directory.ex\n# Module: ReckonProfiles.EstablishProfile.EstablishedToDirectory\n# Processes: ProfileEstablished  ProfileDirectory\n```","ref":"implementation-guidelines.html#4-strict-projection-naming-convention"},{"type":"extras","title":"5. Policy (Process Manager) Pattern - Implementation Guidelines","doc":"**Purpose**: Policies translate events into commands and dispatch them through CommandedApp.\n\n**Rules:**\n- Policies belong in the slice where the **command** they trigger is located\n- File name format: `when_ _then_ .ex`\n- Module name format: `Domain.CommandName.When Then `\n- Each policy handles ONE event type and dispatches ONE command type\n- **MANDATORY**: All command dispatching MUST go through `CommandedApp.dispatch/1`\n\n**Policy Structure:**\n```elixir\n# File: lib/reckon_profiles/domain/establish_profile/when_account_activated_then_establish_profile.ex\ndefmodule ReckonProfiles.Domain.EstablishProfile.WhenAccountActivatedThenEstablishProfile do\n  @moduledoc \"\"\"\n  Policy that triggers profile establishment when an account is activated.\n  \n  This policy listens to AccountActivated events and automatically\n  dispatches an EstablishProfile command to the profiles domain.\n  \"\"\"\n  \n  use Commanded.Event.Handler,\n    application: ReckonProfiles.CommandedApp,\n    name: \"when_account_activated_then_establish_profile\"\n  \n  alias ReckonAccounts.Domain.ActivateAccount.Event, as: AccountActivated\n  alias ReckonProfiles.Domain.EstablishProfile.Command, as: EstablishProfileCommand\n  alias ReckonProfiles.CommandedApp\n  \n  def handle(%AccountActivated{} = event, _metadata) do\n    command = %EstablishProfileCommand{\n      account_id: event.account_id,\n      email: event.email,\n      requested_at: DateTime.utc_now()\n    }\n    \n    # MANDATORY: Dispatch through CommandedApp\n    CommandedApp.dispatch(command)\n  end\nend\n```\n\n**Key Policy Placement Rule:**\n- The policy goes in the slice of the **command** it triggers, not the event it listens to\n- This makes sense because the policy is about **causing** that command to be executed","ref":"implementation-guidelines.html#5-policy-process-manager-pattern"},{"type":"extras","title":"Concrete Examples - Implementation Guidelines","doc":"To make the new structure crystal clear, here are concrete examples:","ref":"implementation-guidelines.html#concrete-examples"},{"type":"extras","title":"Example: ReckonProfiles Domain Structure - Implementation Guidelines","doc":"```\nreckon_profiles/\n lib/\n    reckon_profiles/\n       application.ex\n       repo.ex\n       router.ex\n       shared/\n          profile.ex                        # Profile aggregate\n      \n       domain/                              #  LITERAL \"domain\" directory\n          establish_profile/              # Command slice\n             command.ex                   # EstablishProfile command\n             event.ex                     # ProfileEstablished event\n             handler.ex                   # Command handler\n             established_to_state.ex     # Event handler (ProfileEstablished  Profile aggregate)\n             established_to_directory.ex # Projection (ProfileEstablished  ProfileDirectory)\n             when_account_activated_then_establish_profile.ex  # Policy\n         \n          update_profile/                 # Another command slice\n             command.ex                   # UpdateProfile command\n             event.ex                     # ProfileUpdated event\n             handler.ex                   # Command handler\n             updated_to_state.ex         # Event handler\n             updated_to_directory.ex     # Projection\n         \n          deactivate_profile/\n              command.ex\n              event.ex\n              handler.ex\n              deactivated_to_state.ex\n      \n       projections/                        # Cross-cutting projections (if needed)\n           profile_search_projection.ex\n   \n    reckon_profiles.ex                      # Public API\n    ...\n```","ref":"implementation-guidelines.html#example-reckonprofiles-domain-structure"},{"type":"extras","title":"Example: File Names and Module Names - Implementation Guidelines","doc":"**Command:**\n- File: `lib/reckon_profiles/domain/establish_profile/command.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Command`\n\n**Event:**\n- File: `lib/reckon_profiles/domain/establish_profile/event.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Event`\n\n**Command Handler:**\n- File: `lib/reckon_profiles/domain/establish_profile/handler.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.Handler`\n\n**Event Handler (to State):**\n- File: `lib/reckon_profiles/domain/establish_profile/established_to_state.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.EstablishedToState`\n\n**Projection:**\n- File: `lib/reckon_profiles/domain/establish_profile/established_to_directory.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.EstablishedToDirectory`\n\n**Policy:**\n- File: `lib/reckon_profiles/domain/establish_profile/when_account_activated_then_establish_profile.ex`\n- Module: `ReckonProfiles.Domain.EstablishProfile.WhenAccountActivatedThenEstablishProfile`","ref":"implementation-guidelines.html#example-file-names-and-module-names"},{"type":"extras","title":"Key Naming Rules Summary - Implementation Guidelines","doc":"1. **Directory Structure**: `reckon_[domain]/lib/reckon_[domain]/domain/[command_name]/`\n2. **Slice Names**: Named after the **command** (not the business operation)\n3. **Handler Types**:\n   - `maybe_[command].ex` = Command Handler\n   - `[event]_to_state.ex` = Event Handler (aggregate updates)\n4. **Projections**: `[event]_to_[readmodel].ex` (in slice with event)\n5. **Policies**: `when_[event]_then_[command].ex` (in slice with command they trigger)\n6. **Module Naming**: `ReckonDomain.Domain.CommandName.FileType`","ref":"implementation-guidelines.html#key-naming-rules-summary"},{"type":"extras","title":"6. Screaming Business Intent - Implementation Guidelines","doc":"Folder names should express **business operations**, not technical concepts:\n\n** GOOD - Business-focused names:**\n- `initialize_account/`\n- `verify_email/`\n- `create_profile/`\n- `update_profile_picture/`\n- `close_account/`\n\n** BAD - Technical-focused names:**\n- `commands/`\n- `events/`\n- `handlers/`\n- `controllers/`\n- `services/`","ref":"implementation-guidelines.html#6-screaming-business-intent"},{"type":"extras","title":"Module Naming Conventions - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#module-naming-conventions"},{"type":"extras","title":"Command Modules - Implementation Guidelines","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Command do\n  @moduledoc \"\"\"\n  Command to create a new user profile.\n  \n  This command is triggered when a user completes their initial\n  profile setup after account verification.\n  \"\"\"\n  \n  defstruct [\n    :account_id,\n    :display_name,\n    :bio,\n    :requested_at\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    display_name: String.t(),\n    bio: String.t() | nil,\n    requested_at: DateTime.t()\n  }\nend\n```","ref":"implementation-guidelines.html#command-modules"},{"type":"extras","title":"Event Modules - Implementation Guidelines","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Event do\n  @moduledoc \"\"\"\n  Event emitted when a profile is successfully created.\n  \n  This event triggers read model updates and may trigger\n  other domain reactions.\n  \"\"\"\n  \n  @derive Jason.Encoder\n  defstruct [\n    :account_id,\n    :display_name,\n    :bio,\n    :created_at,\n    :version\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    display_name: String.t(),\n    bio: String.t() | nil,\n    created_at: DateTime.t(),\n    version: integer()\n  }\nend\n```","ref":"implementation-guidelines.html#event-modules"},{"type":"extras","title":"Handler Modules - Implementation Guidelines","doc":"```elixir\ndefmodule ReckonProfiles.CreateProfile.Handler do\n  @moduledoc \"\"\"\n  Command handler for CreateProfile command.\n  \n  Business rules:\n  - Profile can only be created once per account\n  - Display name must be unique\n  - Bio is optional but limited to 500 characters\n  \"\"\"\n  \n  alias ReckonProfiles.Shared.Profile\n  alias ReckonProfiles.CreateProfile.{Command, Event}\n  \n  def execute(%Profile{account_id: nil}, %Command{} = command) do\n    # Business logic here\n    %Event{\n      account_id: command.account_id,\n      display_name: command.display_name,\n      bio: command.bio,\n      created_at: command.requested_at,\n      version: 1\n    }\n  end\n  \n  def execute(%Profile{}, %Command{}) do\n    {:error, :profile_already_exists}\n  end\n  \n  def apply(%Profile{} = profile, %Event{} = event) do\n    # State updates here\n    %Profile{profile |\n      account_id: event.account_id,\n      display_name: event.display_name,\n      bio: event.bio,\n      created_at: event.created_at,\n      updated_at: event.created_at\n    }\n  end\nend\n```","ref":"implementation-guidelines.html#handler-modules"},{"type":"extras","title":"Technical Configuration - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#technical-configuration"},{"type":"extras","title":"Dependencies (mix.exs) - Implementation Guidelines","doc":"```elixir\ndefp deps do\n  [\n    {:dns_cluster, \"~> 0.1.1\"},\n    {:phoenix_pubsub, \"~> 2.1\"},\n    {:ecto_sql, \"~> 3.10\"},\n    {:ecto_sqlite3, \">= 0.0.0\"},\n    {:jason, \"~> 1.2\"},\n    {:ex_esdb, \"~> 0.1.4\"},\n    {:ex_esdb_commanded, \"0.1.3\"}\n  ]\nend\n```","ref":"implementation-guidelines.html#dependencies-mix-exs"},{"type":"extras","title":"ExESDB Configuration (config/config.exs) - Implementation Guidelines","doc":"```elixir\n# Configure ExESDB for ReckonProfiles\nconfig :ex_esdb, :khepri,\n  data_dir: \"tmp/reckon_profiles\",\n  store_id: :reckon_profiles,  #  UNIQUE per domain\n  timeout: 10_000,\n  db_type: :single,\n  pub_sub: :ex_esdb_pubsub,\n  store_description: \"Reckon Profiles Event Store\",\n  store_tags: [\"reckon\", \"profiles\", \"event-sourcing\", \"development\"]\n\n# Configure the Commanded application\nconfig :reckon_profiles, ReckonProfiles.CommandedApp,\n  event_store: [\n    adapter: ExESDB.Commanded.Adapter,\n    store_id: :reckon_profiles,\n    stream_prefix: \"reckon_profiles_\",  #  UNIQUE per domain\n    serializer: Jason,\n    event_type_mapper: ReckonProfiles.EventTypeMapper\n  ]\n```","ref":"implementation-guidelines.html#exesdb-configuration-config-config-exs"},{"type":"extras","title":"LibCluster Configuration (User Preference) - Implementation Guidelines","doc":"```elixir\n# Configure libcluster (preferred over seed_nodes)\nconfig :libcluster,\n  topologies: [\n    reckon_profiles: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45_894,  #  UNIQUE per domain\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"255.255.255.255\",\n        broadcast_only: true,\n        secret: System.get_env(\"RECKON_PROFILES_CLUSTER_SECRET\") || \"reckon_profiles_cluster_secret\"\n      ]\n    ]\n  ]\n```","ref":"implementation-guidelines.html#libcluster-configuration-user-preference"},{"type":"extras","title":"Public API Pattern - Implementation Guidelines","doc":"The main module provides a business-focused API:\n\n```elixir\ndefmodule ReckonProfiles do\n  @moduledoc \"\"\"\n  ReckonProfiles domain - User profile management and personalization.\n  \n  This module provides the public API for profile operations including:\n  - Profile creation and updates\n  - Profile picture management\n  - Privacy settings\n  \n  Uses vertical slicing architecture where each command has its own slice\n  containing command, events, and handlers.\n  \"\"\"\n  \n  alias ReckonProfiles.CommandedApp\n  \n  @doc \"\"\"\n  Creates a new user profile.\n  \n  This is typically called after account verification is complete.\n  \"\"\"\n  def create_profile(account_id, display_name, bio \\\\ nil) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    CommandedApp.dispatch(command)\n  end\n  \n  # ... other business operations\nend\n```","ref":"implementation-guidelines.html#public-api-pattern"},{"type":"extras","title":"Key Benefits of This Architecture - Implementation Guidelines","doc":"1. **Discoverability**: New developers can immediately understand what the system does by looking at folder names\n2. **Maintainability**: Changes to one business operation don't affect others\n3. **Testability**: Each slice can be unit tested independently\n4. **Scalability**: Teams can work on different slices without conflicts\n5. **Business Alignment**: Code structure mirrors business processes","ref":"implementation-guidelines.html#key-benefits-of-this-architecture"},{"type":"extras","title":"Projection Naming and Organization - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#projection-naming-and-organization"},{"type":"extras","title":"Event-to-Projection Naming Pattern - Implementation Guidelines","doc":"Projections should use the `event_to_projection_type` naming pattern to immediately communicate their intent and relationship, following screaming architecture principles:\n\n```\nreckon_[domain]/\n lib/\n    reckon_[domain]/\n       initialize_account/\n          command.ex\n          event.ex\n          handler.ex\n          initialized_to_summary.ex    # Projection: AccountInitialized  AccountSummary\n       activate_account/\n          command.ex\n          event.ex\n          handler.ex\n          activated_to_summary.ex      # Projection: AccountActivated  AccountSummary\n       close_account/\n           command.ex\n           event.ex\n           handler.ex\n           closed_to_summary.ex         # Projection: AccountClosed  AccountSummary\n```","ref":"implementation-guidelines.html#event-to-projection-naming-pattern"},{"type":"extras","title":"Projection Module Naming - Implementation Guidelines","doc":"Projection modules should follow the same naming pattern:\n\n```elixir\n# lib/reckon_accounts/initialize_account/initialized_to_summary.ex\ndefmodule ReckonAccounts.InitializeAccount.InitializedToSummary do\n  @moduledoc \"\"\"\n  Projection that handles AccountInitialized events and updates the AccountSummary read model.\n  \n  This projection creates new entries in the account_summaries table when accounts are initialized.\n  \"\"\"\n  \n  use Commanded.Projections.Ecto,\n    application: ReckonAccounts.CommandedApp,\n    repo: ReckonAccounts.Repo,\n    name: \"ReckonAccounts.InitializeAccount.InitializedToSummary\"\n  \n  alias ReckonAccounts.InitializeAccount.Event, as: AccountInitialized\n  alias ReckonAccounts.Schemas.AccountSummary\n  \n  project(%AccountInitialized{} = event, _metadata, fn multi ->\n    # Projection logic here\n  end)\nend\n```","ref":"implementation-guidelines.html#projection-module-naming"},{"type":"extras","title":"Testing Guidelines - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#testing-guidelines"},{"type":"extras","title":"Vertical Slicing in Tests - Implementation Guidelines","doc":"Tests should mirror the vertical slicing architecture and include projections within their related slices:\n\n```\nreckon_[domain]/\n test/\n    reckon_[domain]/\n       initialize_account/              # Tests for initialize_account slice\n          command_test.exs\n          handler_test.exs\n          event_test.exs\n          initialized_to_summary_test.exs  # Projection test within slice\n       activate_account/                # Tests for activate_account slice\n          command_test.exs\n          handler_test.exs\n          event_test.exs\n          activated_to_summary_test.exs    # Projection test within slice\n       close_account/\n           command_test.exs\n           handler_test.exs\n           event_test.exs\n           closed_to_summary_test.exs       # Projection test within slice\n    support/\n        test_helper.ex\n```","ref":"implementation-guidelines.html#vertical-slicing-in-tests"},{"type":"extras","title":"Test Isolation Principles - Implementation Guidelines","doc":"1. **Domain Tests Should Be Isolated**: Each domain's tests should run independently without requiring other domains to be running.\n\n2. **Use Ecto.Adapters.SQL.Sandbox**: For database tests, use sandbox mode to ensure test isolation:\n\n```elixir\n# test/test_helper.exs\nEcto.Adapters.SQL.Sandbox.mode(ReckonAccounts.Repo, :manual)\n\n# In test modules\nsetup do\n  :ok = Ecto.Adapters.SQL.Sandbox.checkout(ReckonAccounts.Repo)\nend\n```\n\n3. **Test Each Slice Independently**: Write unit tests for commands, handlers, events, and projections separately within each slice.\n\n4. **Integration Tests for Cross-Slice Interactions**: Use integration tests sparingly and only when testing interactions between slices within the same domain.","ref":"implementation-guidelines.html#test-isolation-principles"},{"type":"extras","title":"Projection Testing Guidelines - Implementation Guidelines","doc":"1. **Projections Stay With Their Events**: Projections should be located in the same slice as their related events, using the `event_to_projection_type` naming pattern.\n\n2. **Test Projections Separately**: Write dedicated tests for projections that verify:\n   - Event handling and state updates\n   - Database persistence\n   - Error handling and recovery\n\n3. **Use Test Fixtures**: Create test fixtures for events to ensure consistent testing:\n\n```elixir\n# test/support/fixtures.ex\ndefmodule ReckonAccounts.Fixtures do\n  def account_initialized_event(attrs \\\\ %{}) do\n    %ReckonAccounts.InitializeAccount.Event{\n      account_id: attrs[:account_id] || \"test-account-123\",\n      email: attrs[:email] || \"test@example.com\",\n      initialized_at: attrs[:initialized_at] || DateTime.utc_now() |> DateTime.truncate(:second)\n    }\n  end\nend\n```","ref":"implementation-guidelines.html#projection-testing-guidelines"},{"type":"extras","title":"Integration Between Domains - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#integration-between-domains"},{"type":"extras","title":"Web Service Proxy Pattern - Implementation Guidelines","doc":"Integrations between domain services (`reckon_*` apps) and the website (`landing_site_web`) must go through the umbrella's web service proxy (`landing_site`). This ensures:\n\n1. **Loose Coupling**: Domains don't directly depend on web concerns\n2. **Consistent API**: All web interactions go through a single, well-defined interface\n3. **Testability**: Web and domain logic can be tested independently\n4. **Scalability**: Domains can be extracted to separate services later\n\n```elixir\n#  Correct: Web interactions through proxy\nLandingSite.Accounts.initialize_account(email, password)\n\n#  Incorrect: Direct domain access from web\nReckonAccounts.initialize_account(email, password)\n```","ref":"implementation-guidelines.html#web-service-proxy-pattern"},{"type":"extras","title":"Integration Events (Facts) Pattern - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#integration-events-facts-pattern"},{"type":"extras","title":"Facts vs Domain Events - Implementation Guidelines","doc":"**Domain Events** are internal to a domain and handle business logic within that domain.\n\n**Facts** are integration events that communicate between domains. They represent immutable facts about what happened in one domain that other domains might care about.","ref":"implementation-guidelines.html#facts-vs-domain-events"},{"type":"extras","title":"Facts Structure - Implementation Guidelines","doc":"Facts are defined in the `reckon_shared` application, organized by originating domain:\n\n```\nreckon_shared/\n lib/\n    reckon_shared/\n       accounts/                    # Facts from reckon_accounts domain\n          account_initialized_fact.ex\n          account_activated_fact.ex\n          account_closed_fact.ex\n       profiles/                    # Facts from reckon_profiles domain\n          profile_created_fact.ex\n          profile_updated_fact.ex\n       memberships/                 # Facts from reckon_memberships domain\n           membership_created_fact.ex\n           membership_expired_fact.ex\n    reckon_shared.ex\n```","ref":"implementation-guidelines.html#facts-structure"},{"type":"extras","title":"Fact Module Pattern - Implementation Guidelines","doc":"```elixir\n# lib/reckon_shared/accounts/account_activated_fact.ex\ndefmodule ReckonShared.Accounts.AccountActivatedFact do\n  @moduledoc \"\"\"\n  Integration event (Fact) emitted when an account is activated.\n  \n  This fact is consumed by other domains that need to react to\n  account activation, such as:\n  - ReckonProfiles (to enable profile creation)\n  - ReckonMemberships (to activate trial memberships)\n  \"\"\"\n  \n  @derive Jason.Encoder\n  defstruct [\n    :account_id,\n    :email,\n    :activated_at,\n    :fact_id,\n    :fact_version\n  ]\n  \n  @type t :: %__MODULE__{\n    account_id: String.t(),\n    email: String.t(),\n    activated_at: DateTime.t(),\n    fact_id: String.t(),\n    fact_version: integer()\n  }\n  \n  @doc \"\"\"\n  Creates a new account activated fact.\n  \"\"\"\n  def new(account_id, email, activated_at \\\\ DateTime.utc_now()) do\n    %__MODULE__{\n      account_id: account_id,\n      email: email,\n      activated_at: activated_at,\n      fact_id: UUID.uuid4(),\n      fact_version: 1\n    }\n  end\nend\n```","ref":"implementation-guidelines.html#fact-module-pattern"},{"type":"extras","title":"Fact Projection Pattern - Implementation Guidelines","doc":"Each domain has projections that convert domain events into facts and publish them:\n```elixir\n# lib/reckon_accounts/projections/account_facts_projection.ex\ndefmodule ReckonAccounts.Projections.AccountFactsProjection do\n  @moduledoc \"\"\"\n  Projection that converts domain events into integration facts.\n  \n  Publishes facts to PubSub topics for consumption by other domains.\n  \"\"\"\n  \n  use Commanded.Projections.Ecto,\n    application: ReckonAccounts.CommandedApp,\n    repo: ReckonAccounts.Repo,\n    name: \"account_facts_projection\"\n  \n  alias ReckonAccounts.VerifyEmail.Event, as: EmailVerifiedEvent\n  alias ReckonShared.Accounts.AccountActivatedFact\n  \n  project(%EmailVerifiedEvent{} = event, _metadata) do\n    # Convert domain event to integration fact\n    fact = AccountActivatedFact.new(\n      event.account_id,\n      event.email,\n      event.verified_at\n    )\n    \n    # Publish to event-specific topic\n    Phoenix.PubSub.publish(\n      :ex_esdb_pubsub,\n      \"accounts:facts:account_activated\",\n      fact\n    )\n    \n    :ok\n  end\nend\n```","ref":"implementation-guidelines.html#fact-projection-pattern"},{"type":"extras","title":"Event-Specific Topics Communication - Implementation Guidelines","doc":"**Design Rule**: We communicate via event-specific topics, not generic domain topics.\n\n** GOOD - Event-specific topics:**\n- `\"accounts:facts:account_activated\"`\n- `\"accounts:facts:account_closed\"`\n- `\"profiles:facts:profile_created\"`\n- `\"profiles:facts:profile_updated\"`\n- `\"memberships:facts:membership_created\"`\n\n** BAD - Generic domain topics:**\n- `\"accounts:events\"`\n- `\"profiles:all\"`\n- `\"domain:updates\"`","ref":"implementation-guidelines.html#event-specific-topics-communication"},{"type":"extras","title":"Cross-Domain Event Handling - Implementation Guidelines","doc":"Other domains subscribe to specific fact topics:\n```elixir\n# lib/reckon_profiles/event_handlers/account_event_handler.ex\ndefmodule ReckonProfiles.EventHandlers.AccountEventHandler do\n  @moduledoc \"\"\"\n  Handles account-related facts from other domains.\n  \"\"\"\n  \n  use GenServer\n  \n  alias ReckonShared.Accounts.AccountActivatedFact\n  alias ReckonProfiles.CreateProfile.Command\n  \n  def start_link(opts) do\n    GenServer.start_link(__MODULE__, opts, name: __MODULE__)\n  end\n  \n  def init(_opts) do\n    # Subscribe to specific account facts\n    Phoenix.PubSub.subscribe(:ex_esdb_pubsub, \"accounts:facts:account_activated\")\n    Phoenix.PubSub.subscribe(:ex_esdb_pubsub, \"accounts:facts:account_closed\")\n    \n    {:ok, %{}}\n  end\n  \n  def handle_info(%AccountActivatedFact{} = fact, state) do\n    # React to account activation by enabling profile creation\n    # (Business logic here)\n    {:noreply, state}\n  end\nend\n```","ref":"implementation-guidelines.html#cross-domain-event-handling"},{"type":"extras","title":"Topic Naming Convention - Implementation Guidelines","doc":"```\n[source_domain]:facts:[event_name]\n```\n\n**Examples:**\n- `accounts:facts:account_initialized`\n- `accounts:facts:account_activated`\n- `accounts:facts:account_closed`\n- `profiles:facts:profile_created`\n- `profiles:facts:profile_picture_updated`\n- `memberships:facts:membership_created`\n- `memberships:facts:membership_expired`","ref":"implementation-guidelines.html#topic-naming-convention"},{"type":"extras","title":"Benefits of This Pattern - Implementation Guidelines","doc":"1. **Decoupling**: Domains don't know about each other, only about facts\n2. **Versioning**: Facts can be versioned independently\n3. **Selective Consumption**: Domains subscribe only to events they care about\n4. **Auditability**: All integration events are explicitly defined as facts\n5. **Testability**: Fact publishing and consumption can be tested independently","ref":"implementation-guidelines.html#benefits-of-this-pattern"},{"type":"extras","title":"Web UI Architecture - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#web-ui-architecture"},{"type":"extras","title":"Phoenix LiveView Preference - Implementation Guidelines","doc":"**Design Rule**: Use Phoenix LiveView instead of classic MVC architecture for interactive web interfaces.\n\n** PREFERRED - LiveView architecture:**\n- Real-time interactivity without JavaScript\n- Stateful user interfaces\n- Server-side rendering with client-side updates\n- Built-in handling of form validation and user feedback\n- Simplified state management\n\n** AVOID - Classic MVC where LiveView is suitable:**\n- Controllers for simple form handling\n- Multiple request/response cycles for interactive features\n- Client-side JavaScript for basic interactivity\n- Complex form validation handling","ref":"implementation-guidelines.html#phoenix-liveview-preference"},{"type":"extras","title":"LiveView Module Structure - Implementation Guidelines","doc":"```elixir\n# lib/landing_site_web/live/auth_live.ex\ndefmodule LandingSiteWeb.AuthLive do\n  use LandingSiteWeb, :live_view\n  \n  # LiveView callbacks\n  def mount(_params, _session, socket) do\n    # Initialize socket state\n  end\n  \n  def handle_event(\"register\", %{\"user\" => user_params}, socket) do\n    # Handle user registration\n  end\n  \n  def handle_event(\"login\", %{\"user\" => user_params}, socket) do\n    # Handle user authentication\n  end\n  \n  def render(assigns) do\n    # Render the LiveView template\n  end\nend\n```","ref":"implementation-guidelines.html#liveview-module-structure"},{"type":"extras","title":"When to Use Classic MVC - Implementation Guidelines","doc":"**Acceptable use cases for Controllers:**\n- API endpoints (JSON responses)\n- Simple redirects or downloads\n- Authentication callbacks (OAuth, etc.)\n- Webhook handlers\n- Static page rendering without interactivity","ref":"implementation-guidelines.html#when-to-use-classic-mvc"},{"type":"extras","title":"Migration from MVC to LiveView - Implementation Guidelines","doc":"Refactoring from classic MVC to LiveView is generally straightforward:\n\n1. **Controller actions**  **LiveView event handlers**\n2. **Form submissions**  **LiveView events**\n3. **Flash messages**  **LiveView assigns**\n4. **Redirects**  **LiveView navigation**\n5. **Template rendering**  **LiveView render function**\n\n**Example migration:**\n```elixir\n# Before: Classic MVC\ndef create_account(conn, %{\"user\" => user_params}) do\n  case UserContext.register_user(user_params) do\n    {:ok, user} ->\n      conn\n      |> put_flash(:info, \"Account created successfully\")\n      |> redirect(to: ~p\"/auth/login\")\n    {:error, changeset} ->\n      render(conn, :register, changeset: changeset)\n  end\nend\n\n# After: LiveView\ndef handle_event(\"register\", %{\"user\" => user_params}, socket) do\n  case UserContext.register_user(user_params) do\n    {:ok, user} ->\n      socket\n      |> put_flash(:info, \"Account created successfully\")\n      |> push_navigate(to: ~p\"/auth/login\")\n      |> noreply()\n    {:error, changeset} ->\n      socket\n      |> assign(:changeset, changeset)\n      |> noreply()\n  end\nend\n```","ref":"implementation-guidelines.html#migration-from-mvc-to-liveview"},{"type":"extras","title":"Reckon_App Integration Pattern - Implementation Guidelines","doc":"","ref":"implementation-guidelines.html#reckon_app-integration-pattern"},{"type":"extras","title":"DomainAPI Architecture - Implementation Guidelines","doc":"**Design Rule**: Each reckon_app owns a DomainAPI GenServer that provides the interface for external communication.\n\n**Architecture Components:**\n\n1. **DomainAPI GenServer**: Each reckon_app implements its own DomainAPI that:\n   - Registers itself in Swarm using `Swarm.register_name(api_name(), self())`\n   - Offers a user-friendly set of API functions for sending commands to the Domain\n   - Handles `GenServer.cast` and `GenServer.call` callbacks\n   - Uses pattern matching like `GenServer.cast(api_pid(), message)` to route messages\n\n2. **Service Registration**: \n   ```elixir\n   # In each reckon_app's DomainAPI\n   defmodule ReckonProfiles.DomainAPI do\n     use GenServer\n\n     \n     @domain_name :my_domain\n     \n     def api_name(), do: {:domain_api, @domain_name}\n     def api_pid(), do: Swarm.whereis_name(api_name())\n     \n     def start_link(opts) do\n       GenServer.start_link(__MODULE__, opts, name: __MODULE__)\n     end\n     \n     def init(opts) do\n       # Register this API with Swarm for discovery\n       Swarm.register_name({:domain_api, :reckon_profiles}, __MODULE__, [])\n       {:ok, opts}\n     end\n     \n     # User-friendly API functions\n     def establish_profile(account_id, display_name, bio \\\\ nil) do\n       command = %ReckonProfiles.EstablishProfile.Command{\n         account_id: account_id,\n         display_name: display_name,\n         bio: bio,\n         requested_at: DateTime.utc_now()\n       }\n       \n       GenServer.call(api_pid(), {:dispatch_command, command})\n     end\n     \n     def handle_call({:dispatch_command, command}, _from, state) do\n       result = ReckonProfiles.CommandedApp.dispatch(command)\n       {:reply, result, state}\n     end\n   end\n   ```\n\n3. **Landing Site Integration**: \n   ```elixir\n   # In landing_site mix.exs\n   defp deps do\n     [\n       # Add each reckon_app as dependency without starting the application\n       {:reckon_accounts, path: \"../reckon_apps/reckon_accounts/\", application: false},\n       {:reckon_profiles, path: \"../reckon_apps/reckon_profiles/\", application: false},\n       {:reckon_memberships, path: \"../reckon_apps/reckon_memberships/\", application: false}\n     ]\n   end\n   ```\n\n4. **LibCluster Configuration**: All services use the same cluster configuration to enable service discovery:\n   ```elixir\n   # In each reckon_app's config/config.exs\n   config :libcluster,\n     topologies: [\n       reckon_cluster: [\n         strategy: Cluster.Strategy.Gossip,\n         config: [\n           port: 45_890,  # Shared port for all reckon services\n           if_addr: \"0.0.0.0\",\n           multicast_addr: \"230.1.1.251\",\n           multicast_ttl: 1,\n           secret: System.get_env(\"RECKON_CLUSTER_SECRET\") || \"reckon_cluster_dev_secret\"\n         ]\n       ]\n     ]\n   ```","ref":"implementation-guidelines.html#domainapi-architecture"},{"type":"extras","title":"Benefits of This Pattern - Implementation Guidelines","doc":"1. **Domain Ownership**: Each reckon_app owns its API boundary and communication protocol\n2. **Service Discovery**: Swarm provides automatic service discovery across the cluster\n3. **Fault Tolerance**: Services can be started/stopped independently\n4. **Clean Dependencies**: Landing site depends on reckon_apps for compilation but not runtime\n5. **Scalability**: Multiple instances of each reckon_app can be deployed\n6. **LibCluster Integration**: Follows the preferred clustering approach over seed_nodes","ref":"implementation-guidelines.html#benefits-of-this-pattern-1"},{"type":"extras","title":"Implementation Checklist - Implementation Guidelines","doc":"**For each reckon_app:**\n- [ ] Implement DomainAPI GenServer with Swarm registration\n- [ ] Add user-friendly API functions that wrap CommandedApp.dispatch calls\n- [ ] Configure libcluster with shared cluster settings\n- [ ] Add libcluster and swarm dependencies\n- [ ] Update Application supervision tree to start DomainAPI\n\n**For landing_site:**\n- [ ] Add reckon_apps as `application: false` dependencies\n- [ ] Implement service discovery to find available DomainAPIs\n- [ ] Configure libcluster to join the same cluster\n- [ ] Create communication layer that uses Swarm.whereis_name to find services","ref":"implementation-guidelines.html#implementation-checklist"},{"type":"extras","title":"Anti-Patterns to Avoid - Implementation Guidelines","doc":"1. ** Grouping by technical layer** (controllers/, services/, models/)\n2. ** Shared command/event modules** (mixing multiple operations)\n3. ** Generic naming** (using technical terms instead of business terms)\n4. ** Horizontal slicing** (splitting one business operation across multiple technical layers)\n5. ** Anemic domain models** (putting business logic in \"service\" classes)\n6. ** Generic PubSub topics** (using broad topics instead of event-specific ones)\n7. ** Direct domain coupling** (one domain importing modules from another)\n8. ** Mixing domain events with integration facts** (using same struct for both)\n9. ** CRUD-based event naming** (using generic -created, -updated, -deleted instead of meaningful business events)\n10. ** Direct Router usage** (bypassing CommandedApp for command dispatch)\n11. ** Multiple capabilities per module** (violating 1-capability-per-module rule)\n12. ** Incorrect projection naming** (not following ` _to_ .ex` format)\n13. ** Policies bypassing CommandedApp** (direct command creation without proper dispatch)","ref":"implementation-guidelines.html#anti-patterns-to-avoid"},{"type":"extras","title":" FORBIDDEN: Direct Router Usage for Command Dispatch - Implementation Guidelines","doc":"**This anti-pattern is FORBIDDEN in all reckon_* applications.**\n\n** BAD - Direct Router usage:**\n```elixir\ndefmodule ReckonProfiles do\n  alias ReckonProfiles.Router  #  FORBIDDEN\n  \n  def create_profile(account_id, display_name, bio) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    Router.dispatch(command)  #  FORBIDDEN - bypasses CommandedApp\n  end\nend\n```\n\n** CORRECT - CommandedApp usage:**\n```elixir\ndefmodule ReckonProfiles do\n  alias ReckonProfiles.CommandedApp  #  CORRECT\n  \n  def create_profile(account_id, display_name, bio) do\n    command = %ReckonProfiles.CreateProfile.Command{\n      account_id: account_id,\n      display_name: display_name,\n      bio: bio,\n      requested_at: DateTime.utc_now()\n    }\n    \n    CommandedApp.dispatch(command)  #  CORRECT - goes through CommandedApp\n  end\nend\n```\n\n**Why this rule exists:**\n\n1. **Architectural Consistency**: CommandedApp is the proper entry point for commands in Commanded\n2. **Centralized Configuration**: CommandedApp handles event store config, middleware, and other application-level concerns\n3. **Middleware Support**: CommandedApp can add middleware for logging, validation, authentication, etc.\n4. **Error Handling**: Provides consistent error handling and retry logic across all commands\n5. **Testing**: Easier to mock CommandedApp than individual router calls\n6. **Future-Proofing**: Easier to add cross-cutting concerns like audit logging, metrics, etc.\n7. **Commanded Best Practices**: `CommandedApp.dispatch()` is the idiomatic way in Commanded\n\n**Command Flow:**\n```\nBusiness Logic (ReckonProfiles.ex)\n         \n  CommandedApp.dispatch()   CORRECT\n         \n    Router (internal routing)\n         \n  Handler (processes command)\n         \n    Events (domain events)\n```\n\n**NOT:**\n```\nBusiness Logic (ReckonProfiles.ex)\n         \n    Router.dispatch()   FORBIDDEN\n         \n  Handler (processes command)\n         \n    Events (domain events)\n```\n\n**The Router is an internal implementation detail that should NEVER be called directly from business logic.**","ref":"implementation-guidelines.html#forbidden-direct-router-usage-for-command-dispatch"},{"type":"extras","title":"CRUD Events vs Business Events - Implementation Guidelines","doc":"** BAD - CRUD-focused event names:**\n- `ProfileCreated`\n- `ProfileUpdated` \n- `ProfileDeleted`\n- `AccountCreated`\n- `AccountUpdated`\n- `MembershipCreated`\n\n** GOOD - Business-focused event names:**\n- `ProfileEstablished` (when user completes initial profile setup)\n- `ProfilePersonalized` (when user customizes their profile)\n- `ProfileDeactivated` (when user temporarily hides their profile)\n- `AccountInitialized` (when registration begins)\n- `AccountActivated` (when email is verified)\n- `AccountSuspended` (when account is temporarily disabled)\n- `AccountClosed` (when account is permanently closed)\n- `TrialMembershipGranted` (when free trial begins)\n- `PremiumMembershipUpgraded` (when user pays for premium)\n- `MembershipExpired` (when subscription ends)\n\n**Why this matters:**\n- Business events capture **intent and meaning**, not just state changes\n- They reflect the **ubiquitous language** of the domain\n- They enable **better event sourcing** by preserving business context\n- They make **event streams readable** as a business narrative\n- They support **better analytics** and business intelligence\n- They enable **temporal queries** that make business sense\n\n**Example of business context preservation:**\n```elixir\n#  BAD - Generic CRUD event\ndefmodule ReckonAccounts.AccountUpdated.Event do\n  defstruct [:account_id, :changes, :updated_at]\nend\n\n#  GOOD - Meaningful business events\ndefmodule ReckonAccounts.AccountActivated.Event do\n  defstruct [:account_id, :email, :activated_at, :verification_token]\nend\n\ndefmodule ReckonAccounts.AccountSuspended.Event do\n  defstruct [:account_id, :reason, :suspended_at, :suspended_by]\nend\n\ndefmodule ReckonAccounts.AccountClosed.Event do\n  defstruct [:account_id, :reason, :closed_at, :requested_by]\nend\n```\n\nRemember: The codebase should **scream** what it does, not how it's implemented!","ref":"implementation-guidelines.html#crud-events-vs-business-events"},{"type":"extras","title":"Debugging and Troubleshooting","doc":"# ExESDB.Debugger\n\nA comprehensive debugging and inspection tool for ExESDB Event Sourcing Database systems.","ref":"debugging.html"},{"type":"extras","title":"Overview - Debugging and Troubleshooting","doc":"The ExESDB Debugger provides REPL-friendly functions to investigate all aspects of your ExESDB system, including:\n\n-  **Process Supervision Tree Inspection**\n-  **Configuration Analysis**\n-  **Stream and Event Investigation**  \n-  **Performance Metrics**\n-  **Health Monitoring**\n-  **Emitter Pool Management**\n-  **Function Tracing**\n-  **Benchmarking Tools**","ref":"debugging.html#overview"},{"type":"extras","title":"Quick Start - Debugging and Troubleshooting","doc":"```elixir\n# In your IEx session\niex> ExESDB.Debugger.overview()\niex> ExESDB.Debugger.help()\n```","ref":"debugging.html#quick-start"},{"type":"extras","title":"Core Functions - Debugging and Troubleshooting","doc":"","ref":"debugging.html#core-functions"},{"type":"extras","title":"System Overview - Debugging and Troubleshooting","doc":"```elixir\n# Get a complete system overview\nExESDB.Debugger.overview()\nExESDB.Debugger.overview(:my_store)\n\n# Show help with all available commands\nExESDB.Debugger.help()\n```","ref":"debugging.html#system-overview"},{"type":"extras","title":"Process Management - Debugging and Troubleshooting","doc":"```elixir\n# List all ExESDB processes\nExESDB.Debugger.processes()\n\n# Display supervision tree\nExESDB.Debugger.supervision_tree()\n\n# Show emitter pools and workers\nExESDB.Debugger.emitters()\n```","ref":"debugging.html#process-management"},{"type":"extras","title":"Configuration - Debugging and Troubleshooting","doc":"```elixir\n# Show detailed configuration\nExESDB.Debugger.config()\n\n# Configuration shows sources (app config vs environment variables)\n```","ref":"debugging.html#configuration"},{"type":"extras","title":"Data Investigation - Debugging and Troubleshooting","doc":"```elixir\n# List all streams\nExESDB.Debugger.streams()\n\n# Show events in a specific stream\nExESDB.Debugger.events(\"user-123\", limit: 10)\nExESDB.Debugger.events(\"orders\", start_version: 50, direction: :backward)\n\n# List active subscriptions\nExESDB.Debugger.subscriptions()\n```","ref":"debugging.html#data-investigation"},{"type":"extras","title":"Health & Performance - Debugging and Troubleshooting","doc":"```elixir\n# Comprehensive health check\nExESDB.Debugger.health()\n\n# Performance metrics\nExESDB.Debugger.performance()\n\n# Show top processes by memory/CPU\nExESDB.Debugger.top()\nExESDB.Debugger.top(limit: 5, sort_by: :reductions)\n```","ref":"debugging.html#health-performance"},{"type":"extras","title":"Debugging Tools - Debugging and Troubleshooting","doc":"```elixir\n# Start Erlang Observer GUI\nExESDB.Debugger.observer()\n\n# Trace function calls\nExESDB.Debugger.trace(ExESDB.StreamsWriter, :append_events, duration: 5000)\n\n# Benchmark functions\nExESDB.Debugger.benchmark(fn -> expensive_operation() end, times: 100)\n```","ref":"debugging.html#debugging-tools"},{"type":"extras","title":"Multi-Store Support - Debugging and Troubleshooting","doc":"All functions work with multiple stores:\n\n```elixir\n# Auto-discover the store (works with single stores)\nExESDB.Debugger.overview()\n\n# Specify a particular store\nExESDB.Debugger.overview(:orders_store)\nExESDB.Debugger.processes(:inventory_store)\nExESDB.Debugger.health(:users_store)\n```","ref":"debugging.html#multi-store-support"},{"type":"extras","title":"Example Output - Debugging and Troubleshooting","doc":"","ref":"debugging.html#example-output"},{"type":"extras","title":"System Overview - Debugging and Troubleshooting","doc":"```\n ExESDB System Overview\n========================================\n Store: :my_store\n System:  Running\n PID: #PID<0.1234.0>\n Node: :node@localhost\n System healthy\n  Processes: 12/12 alive\n  Config: cluster mode, data: /tmp/data\n\n Use ExESDB.Debugger.help() for available commands\n```","ref":"debugging.html#system-overview-1"},{"type":"extras","title":"Health Check - Debugging and Troubleshooting","doc":"```\n ExESDB Health Check for :my_store\n==================================================\n System Process       : OK\n Configuration        : OK\n Gateway Workers      : 2 gateway worker(s) running\n Store Accessibility  : OK\n Memory Usage        : High memory usage: 156.7 MB\n Process Supervision  : OK\n\n Summary: 6 checks, 0 errors, 1 warnings\n```","ref":"debugging.html#health-check"},{"type":"extras","title":"Process Listing - Debugging and Troubleshooting","doc":"```\n  ExESDB Processes for :my_store\n==================================================\n\n System (1 processes)\n   exesdb_system_my_store         #PID<0.1234.0> 2.1MB msgs:0\n\n Gateway (2 processes)  \n   gateway_worker_my_store_1      #PID<0.1235.0> 1.2MB msgs:0\n   gateway_worker_my_store_2      #PID<0.1236.0> 1.1MB msgs:0\n\n Emitter (3 processes)\n   emitter_pool_subscription_1    #PID<0.1237.0> 512KB msgs:0\n\n Total Memory: 4.8MB\n```","ref":"debugging.html#process-listing"},{"type":"extras","title":"Health Checks - Debugging and Troubleshooting","doc":"The health check performs the following validations:\n\n-  **System Process**: Main supervisor is running\n-  **Configuration**: All config values are accessible\n-  **Gateway Workers**: At least one gateway worker is available\n-  **Store Accessibility**: Can communicate with the store\n-  **Memory Usage**: Monitors total memory consumption\n-  **Process Supervision**: All supervised processes are alive","ref":"debugging.html#health-checks"},{"type":"extras","title":"Performance Monitoring - Debugging and Troubleshooting","doc":"```elixir\nExESDB.Debugger.performance()\n```\n\nShows:\n- System memory and CPU info\n- Total ExESDB process count and memory usage\n- Top memory-consuming processes\n- System uptime","ref":"debugging.html#performance-monitoring"},{"type":"extras","title":"Tracing and Debugging - Debugging and Troubleshooting","doc":"","ref":"debugging.html#tracing-and-debugging"},{"type":"extras","title":"Function Tracing - Debugging and Troubleshooting","doc":"```elixir\n# Trace all calls to a function for 5 seconds\nExESDB.Debugger.trace(ExESDB.StreamsWriter, :append_events)\n\n# Custom tracing duration\nExESDB.Debugger.trace(MyModule, :my_function, duration: 10_000)\n```","ref":"debugging.html#function-tracing"},{"type":"extras","title":"Benchmarking - Debugging and Troubleshooting","doc":"```elixir\n# Benchmark a function\nExESDB.Debugger.benchmark(fn -> \n  ExESDB.StreamsWriter.append_events(:my_store, \"test\", [%{type: \"test\"}])\nend, times: 100)\n```","ref":"debugging.html#benchmarking"},{"type":"extras","title":"Observer GUI - Debugging and Troubleshooting","doc":"```elixir\n# Opens the Erlang Observer for real-time monitoring\nExESDB.Debugger.observer()\n```","ref":"debugging.html#observer-gui"},{"type":"extras","title":"Dependencies - Debugging and Troubleshooting","doc":"The debugger uses several built-in and external libraries:\n\n- **:recon** - Process inspection and system information\n- **:observer** - Erlang Observer GUI (included in OTP)\n- **:dbg** - Function tracing (included in OTP)\n- **:sys** - System process inspection (included in OTP)","ref":"debugging.html#dependencies"},{"type":"extras","title":"Troubleshooting - Debugging and Troubleshooting","doc":"","ref":"debugging.html#troubleshooting"},{"type":"extras","title":"Common Issues - Debugging and Troubleshooting","doc":"1. **No processes found**\n   ```elixir\n   # Make sure ExESDB is running\n   ExESDB.Debugger.health()  # Will show what's missing\n   ```\n\n2. **Gateway worker not available**\n   ```elixir\n   # Check if the system is properly started\n   ExESDB.Debugger.supervision_tree()\n   ```\n\n3. **Memory warnings**\n   ```elixir\n   # Investigate top memory consumers\n   ExESDB.Debugger.top(sort_by: :memory)\n   ExESDB.Debugger.observer()  # For detailed analysis\n   ```","ref":"debugging.html#common-issues"},{"type":"extras","title":"Tips - Debugging and Troubleshooting","doc":"- Use `help()` to see all available commands\n- All functions work without store_id (auto-discovery)\n- Health checks will identify most common issues\n- Observer GUI provides real-time monitoring\n- Tracing is helpful for performance debugging\n- Performance monitoring shows system resource usage","ref":"debugging.html#tips"},{"type":"extras","title":"Integration - Debugging and Troubleshooting","doc":"The debugger is designed to be used in development and production REPL sessions. It's safe to use in production as all operations are read-only by default.\n\nAdd it to your application by ensuring ExESDB is in your dependencies, then use it directly in IEx:\n\n```elixir\n# In your IEx session after starting your app\niex> ExESDB.Debugger.overview()\n```","ref":"debugging.html#integration"},{"type":"extras","title":"Examples - Debugging and Troubleshooting","doc":"","ref":"debugging.html#examples"},{"type":"extras","title":"Development Workflow - Debugging and Troubleshooting","doc":"```elixir\n# 1. Start your app\niex -S mix\n\n# 2. Check system health\nExESDB.Debugger.health()\n\n# 3. Look at your data\nExESDB.Debugger.streams()\nExESDB.Debugger.events(\"user-123\", limit: 5)\n\n# 4. Monitor performance\nExESDB.Debugger.performance()\nExESDB.Debugger.top()\n\n# 5. Debug issues\nExESDB.Debugger.trace(MyModule, :problematic_function)\n```","ref":"debugging.html#development-workflow"},{"type":"extras","title":"Production Debugging - Debugging and Troubleshooting","doc":"```elixir\n# Quick health check\nExESDB.Debugger.health()\n\n# Check memory usage\nExESDB.Debugger.performance()\n\n# Investigate specific issues\nExESDB.Debugger.processes()\nExESDB.Debugger.supervision_tree()\n```\n\nThe ExESDB Debugger makes it easy to understand, monitor, and debug your Event Sourcing system!","ref":"debugging.html#production-debugging"},{"type":"extras","title":"Read Me","doc":"# ExESDB - A BEAM-native Event Store\n\n`ExESDB` is a BEAM-native Event Store, built on top of the [khepri](https://github.com/rabbitmq/khepri) and [ra](https://github.com/rabbitmq/ra) subsystems.","ref":"readme.html"},{"type":"extras","title":"Motivation - Read Me","doc":"One of the arguments for BEAM development is that it comes \"batteries included\". Be it caching, storage, pub/sub, observability etc... the Erlang ecosystem always has the option to avoid external dependencies.\n\nFor Event Sourcing use cases however, the Event Store is often a separate service.\n\nThis project is an attempt at addressing this point, by building further upon the work of the `rabbitmq/khepri` and `rabbitmq/ra` subsystems.","ref":"readme.html#motivation"},{"type":"extras","title":"Features - Read Me","doc":"ExESDB is a distributed, BEAM-native Event Store that provides high-availability event sourcing capabilities with automatic cluster formation and coordination. Built on top of Khepri and Ra (Raft consensus), it offers enterprise-grade reliability and performance.","ref":"readme.html#features"},{"type":"extras","title":"Core Event Store Functionality - Read Me","doc":"#### Event Stream Management\n- **Stream Creation**: Automatic stream creation on first event append\n- **Event Appending**: Atomic append operations with optimistic concurrency control\n- **Event Retrieval**: Query events with forward/backward traversal support\n- **Stream Versioning**: Track stream versions for conflict detection and resolution\n- **Stream Listing**: Enumerate all streams in the store\n\n#### Event Storage\n- **Persistent Storage**: Durable event storage using Khepri's distributed key-value store\n- **ACID Compliance**: Atomic, consistent, isolated, and durable operations\n- **Conflict Resolution**: Built-in optimistic concurrency control\n- **Data Integrity**: Checksum validation and corruption detection\n\n#### Subscription System\n- **Multiple Subscription Types**:\n  - `:by_stream` - Subscribe to specific event streams\n  - `:by_event_type` - Subscribe to events by type classification\n  - `:by_event_pattern` - Pattern-based event matching\n  - `:by_event_payload` - Content-based subscription filtering\n- **Persistent Subscriptions**: Durable subscriptions that survive node restarts\n- **Transient Subscriptions**: Temporary subscriptions for real-time processing\n- **Event Replay**: Start subscriptions from any stream version\n- **Acknowledgment System**: Reliable event delivery with ACK/NACK support\n- **\"Follow-the-Leader\"**: Subscription processes automatically migrate to cluster leader\n\n#### Snapshot Management\n- **Aggregate Snapshots**: Store and retrieve aggregate state snapshots\n- **Version-based Snapshots**: Snapshots tied to specific stream versions\n- **Snapshot Lifecycle**: Create, read, update, and delete snapshot operations\n- **Performance Optimization**: Reduce replay time for large aggregates\n- **Distributed Storage**: Snapshots stored across the cluster for availability","ref":"readme.html#core-event-store-functionality"},{"type":"extras","title":"Distributed Architecture & Clustering - Read Me","doc":"#### LibCluster Integration\nExESDB uses LibCluster for automatic cluster discovery and formation:\n\n- **Strategy**: Gossip-based multicast discovery\n- **Protocol**: UDP multicast on configurable port (default: 45892)\n- **Network**: Automatic node discovery on shared networks\n- **Security**: Shared secret authentication for cluster membership\n- **Broadcast Discovery**: Configurable multicast addressing\n\n#### Cluster Formation Process\n1. **Initialization**: Node starts and initializes LibCluster topology\n2. **Discovery**: Uses gossip multicast to discover peer nodes\n3. **Authentication**: Validates cluster membership using shared secrets\n4. **Coordination**: ClusterCoordinator manages join/leave operations\n5. **Consensus**: Khepri cluster formation using Raft consensus\n6. **Monitoring**: Continuous health monitoring and leader election\n\n#### High Availability Features\n- **Automatic Clustering**: Nodes automatically discover and join clusters\n- **Split-Brain Prevention**: ClusterCoordinator prevents network partition issues\n- **Leader Election**: Automatic leader election using Raft consensus\n- **Failover**: Seamless handling of node failures\n- **Data Replication**: Events replicated across cluster nodes\n- **Consensus Protocol**: Ra/Raft ensures data consistency","ref":"readme.html#distributed-architecture-clustering"},{"type":"extras","title":"Storage Engine - Read Me","doc":"#### Khepri Integration\n- **Distributed Tree Store**: Hierarchical key-value storage\n- **MVCC**: Multi-version concurrency control\n- **Transactions**: ACID transaction support\n- **Schema Evolution**: Support for data structure changes\n- **Triggers**: Event-driven data processing\n\n#### Ra (Raft) Consensus\n- **Strong Consistency**: Linearizable read/write operations\n- **Partition Tolerance**: Operates correctly during network partitions\n- **Leader-based Replication**: Single leader for write operations\n- **Log Compaction**: Automatic cleanup of old log entries\n- **Snapshot Support**: Efficient state transfer for new nodes","ref":"readme.html#storage-engine"},{"type":"extras","title":"Configuration & Deployment - Read Me","doc":"#### Environment Configuration\n- `EX_ESDB_STORE_ID`: Unique identifier for the store instance\n- `EX_ESDB_DB_TYPE`: Deployment type (`:single` or `:cluster`)\n- `EX_ESDB_DATA_DIR`: Data directory for persistent storage\n- `EX_ESDB_TIMEOUT`: Operation timeout configuration\n- `EX_ESDB_CLUSTER_SECRET`: Shared secret for cluster authentication\n- `EX_ESDB_COOKIE`: Erlang distribution cookie\n- `EX_ESDB_PUB_SUB`: PubSub configuration for event broadcasting\n\n#### LibCluster Configuration\n```elixir\nconfig :libcluster,\n  topologies: [\n    ex_esdb_cluster: [\n      strategy: Cluster.Strategy.Gossip,\n      config: [\n        port: 45_892,\n        if_addr: \"0.0.0.0\",\n        multicast_addr: \"255.255.255.255\",\n        broadcast_only: true,\n        secret: System.get_env(\"EX_ESDB_CLUSTER_SECRET\")\n      ]\n    ]\n  ]\n```","ref":"readme.html#configuration-deployment"},{"type":"extras","title":"Gateway API Integration - Read Me","doc":"#### High-Availability Proxy\n- **Load Balancing**: Distribute requests across gateway workers\n- **Service Discovery**: Automatic discovery of available gateway workers\n- **Fault Tolerance**: Handle worker failures gracefully\n- **Request Routing**: Smart routing based on operation type\n\n#### Worker Distribution\n- **Swarm Integration**: Distributed worker management\n- **Process Migration**: Workers can move between cluster nodes\n- **Resource Management**: Efficient resource utilization across cluster\n- **Monitoring**: Real-time worker health and performance tracking","ref":"readme.html#gateway-api-integration"},{"type":"extras","title":"Operational Features - Read Me","doc":"#### Monitoring & Observability\n- **Cluster Status**: Real-time cluster membership and health\n- **Leader Tracking**: Monitor current cluster leader\n- **Performance Metrics**: Operation latency and throughput\n- **Error Tracking**: Comprehensive error logging and reporting\n- **Health Checks**: Built-in health check endpoints\n\n#### Development Tools\n- **Cluster Manager**: Interactive cluster management script\n- **Docker Compose**: Multi-node development environment\n- **Chaos Engineering**: Built-in chaos testing capabilities\n- **Validation Scripts**: Automated cluster validation tools","ref":"readme.html#operational-features"},{"type":"extras","title":"Network Topology - Read Me","doc":"```\n        \n   ExESDB Node          ExESDB Node          ExESDB Node   \n    (Leader)        (Follower)       (Follower)    \n   Khepri + Ra          Khepri + Ra          Khepri + Ra   \n        \n                                                       \n                                                       \n              Gossip Multicast Network (UDP:45892)     \n                                                       \n                                                       \n    Raft Consensus           Event Storage           Subscription\n     & Replication          & Retrieval             Management\n```","ref":"readme.html#network-topology"},{"type":"extras","title":"Deployment Scenarios - Read Me","doc":"#### Single Node Deployment\n- **Development**: Local development and testing\n- **Small Applications**: Simple event sourcing needs\n- **Embedded Usage**: Integration within existing applications\n\n#### Multi-Node Cluster\n- **Production**: High-availability production deployments\n- **Horizontal Scaling**: Scale read/write capacity\n- **Geographic Distribution**: Multi-region deployments\n- **Fault Tolerance**: Survive individual node failures\n\n#### Container Orchestration\n- **Docker Compose**: Development and testing environments\n- **Kubernetes**: Production container orchestration\n- **Docker Swarm**: Simplified container clustering\n- **Health Checks**: Container-level health monitoring","ref":"readme.html#deployment-scenarios"},{"type":"extras","title":"Performance Characteristics - Read Me","doc":"#### Throughput\n- **Write Performance**: Optimized for high-volume event appending\n- **Read Performance**: Efficient event retrieval and streaming\n- **Concurrent Operations**: Handle multiple simultaneous operations\n- **Batch Processing**: Support for batch event operations\n\n#### Scalability\n- **Horizontal Scaling**: Add nodes to increase capacity\n- **Storage Scalability**: Distributed storage across cluster\n- **Subscription Scaling**: Distribute subscription load\n- **Resource Utilization**: Efficient use of available resources","ref":"readme.html#performance-characteristics"},{"type":"extras","title":"Integration Capabilities - Read Me","doc":"#### BEAM Ecosystem\n- **Phoenix Integration**: Real-time web applications\n- **LiveView Support**: Real-time UI updates\n- **GenServer Integration**: Native BEAM process integration\n- **OTP Supervision**: Fault-tolerant supervision trees\n\n#### External Systems\n- **REST APIs**: HTTP-based integration\n- **Message Queues**: Integration with external queuing systems\n- **Databases**: Projection and read model support\n- **Monitoring Systems**: Metrics and alerting integration","ref":"readme.html#integration-capabilities"},{"type":"extras","title":"Installation - Read Me","doc":"","ref":"readme.html#installation"},{"type":"extras","title":"Docker Installation - Read Me","doc":"ExESDB is available as a Docker image on Docker Hub with automatic versioning based on the `mix.exs` version.\n\n#### Available Tags\n- `beamcampus/ex_esdb:latest` - Latest build from master branch\n- `beamcampus/ex_esdb:0.0.18` - Specific version (current version)\n- `beamcampus/ex_esdb:0.0.x` - Any specific version tag\n\n#### Quick Start\n\n**Single Node:**\n```bash\ndocker run -d \\\n  --name ex-esdb \\\n  -p 4369:4369 \\\n  -p 9000-9100:9000-9100 \\\n  -p 45892:45892/udp \\\n  -e EX_ESDB_STORE_ID=\"my-store\" \\\n  -e EX_ESDB_DB_TYPE=\"single\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -v ex-esdb-data:/data \\\n  beamcampus/ex_esdb:latest\n```\n\n**Multi-Node Cluster:**\n```bash\n# Node 1 (seed node)\ndocker run -d \\\n  --name ex-esdb-node1 \\\n  --network ex-esdb-net \\\n  -p 4369:4369 \\\n  -p 9001:9000 \\\n  -p 45892:45892/udp \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node1-data:/data \\\n  beamcampus/ex_esdb:latest\n\n# Node 2\ndocker run -d \\\n  --name ex-esdb-node2 \\\n  --network ex-esdb-net \\\n  -p 9002:9000 \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node2-data:/data \\\n  beamcampus/ex_esdb:latest\n\n# Node 3\ndocker run -d \\\n  --name ex-esdb-node3 \\\n  --network ex-esdb-net \\\n  -p 9003:9000 \\\n  -e EX_ESDB_STORE_ID=\"cluster-store\" \\\n  -e EX_ESDB_DB_TYPE=\"cluster\" \\\n  -e EX_ESDB_DATA_DIR=\"/data\" \\\n  -e EX_ESDB_CLUSTER_SECRET=\"your-secret-key\" \\\n  -e EX_ESDB_COOKIE=\"your-erlang-cookie\" \\\n  -v ex-esdb-node3-data:/data \\\n  beamcampus/ex_esdb:latest\n```\n\n#### Docker Compose\n\nFor development and testing, use the provided Docker Compose setup:\n\n```bash\n# Clone the repository\ngit clone https://github.com/beam-campus/ex-esdb.git\ncd ex-esdb/dev-env\n\n# Start a 3-node cluster\n./start-core-only.sh\n\n# Or use the interactive cluster manager\n./ez-cluster.sh\n```\n\nThe Docker Compose setup includes:\n- **Core Cluster**: 3-node ExESDB cluster (ex-esdb0, ex-esdb1, ex-esdb2)\n- **Extended Tier**: Additional 2 nodes (ex-esdb10, ex-esdb11)\n- **Massive Tier**: Additional 8 nodes (ex-esdb20-27)\n- **Automatic Networking**: Configured Docker networks for cluster communication\n- **Data Persistence**: Named volumes for data persistence\n- **Health Checks**: Built-in container health monitoring\n\n#### Environment Variables\n\n> ** Important**: When using the dev-env Docker Compose configurations, you must export the `EX_ESDB_COOKIE` environment variable on your host machine. This single environment variable is used for all cluster authentication purposes (cookies, secrets, etc.).\n> \n> ```bash\n> export EX_ESDB_COOKIE=\"your-secure-cookie-value\"\n> ```\n\n| Variable | Description | Default | Required |\n|----------|-------------|---------|----------|\n| `EX_ESDB_STORE_ID` | Unique store identifier | - | Yes |\n| `EX_ESDB_DB_TYPE` | Deployment type (`single` or `cluster`) | `single` | No |\n| `EX_ESDB_DATA_DIR` | Data directory path | `/data` | No |\n| `EX_ESDB_TIMEOUT` | Operation timeout (ms) | `5000` | No |\n| `EX_ESDB_CLUSTER_SECRET` | Cluster authentication secret | - | Yes (cluster) |\n| `EX_ESDB_COOKIE` | Erlang distribution cookie | - | Yes (cluster) |\n| `EX_ESDB_PUB_SUB` | PubSub process name | `:ex_esdb_pubsub` | No |\n\n#### Ports\n\n| Port | Protocol | Description |\n|------|----------|-------------|\n| `4369` | TCP | EPMD (Erlang Port Mapper Daemon) |\n| `9000-9100` | TCP | Erlang distribution ports |\n| `45892` | UDP | LibCluster gossip multicast |\n\n#### Health Checks\n\nThe Docker image includes a built-in health check script:\n\n```bash\n# Check container health\ndocker exec ex-esdb ./check-ex-esdb.sh\n\n# View health status\ndocker inspect --format='{{.State.Health.Status}}' ex-esdb\n```\n\n#### Production Considerations\n\n1. **Security**: Use strong, unique values for `EX_ESDB_CLUSTER_SECRET` and `EX_ESDB_COOKIE`\n2. **Networking**: Ensure proper firewall rules for cluster communication\n3. **Storage**: Use named volumes or bind mounts for data persistence\n4. **Monitoring**: Implement external monitoring for cluster health\n5. **Backups**: Regular backup of data volumes\n6. **Resource Limits**: Set appropriate CPU and memory limits","ref":"readme.html#docker-installation"},{"type":"extras","title":"Hex Installation - Read Me","doc":"ExESDB is also available as a Hex package for direct integration:\n\n```elixir\ndef deps do\n  [\n    {:ex_esdb, \"~> 0.0.18\"}\n  ]\nend\n```","ref":"readme.html#hex-installation"},{"type":"extras","title":"Contents - Read Me","doc":"- [Getting Started](system/guides/getting_started.md)","ref":"readme.html#contents"},{"type":"extras","title":"Releases - Read Me","doc":"- [On Hex](https://hex.pm/packages/ex_esdb)\n- [Release Documentation](https://hexdocs.pm/ex_esdb/index.html)","ref":"readme.html#releases"}],"proglang":"elixir","content_type":"text/markdown","producer":{"name":"ex_doc","version":"0.38.2"}}