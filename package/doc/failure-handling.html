<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="ExDoc v0.38.2">
    <meta name="project" content="ex_esdb v0.5.0">


    <title>Failure Handling — ex_esdb v0.5.0</title>

    <link rel="stylesheet" href="dist/html-elixir-KV3YOVJ3.css" />

      <link rel="canonical" href="https://hexdocs.pm/ex_esdb/failure-handling.html" />

    <script defer src="dist/sidebar_items-BAB1635F.js"></script>
    <script defer src="docs_config.js"></script>
    <script defer src="dist/html-DPJLHKSM.js"></script>

  </head>
  <body>
    <script>(()=>{var t="ex_doc:settings",e="dark";var o="dark",s="light";var E="sidebar_state",n="closed";var r="sidebar_width";var a="sidebar-open";var i=new URLSearchParams(window.location.search),S=i.get("theme")||JSON.parse(localStorage.getItem(t)||"{}").theme;(S===o||S!==s&&window.matchMedia("(prefers-color-scheme: dark)").matches)&&document.body.classList.add(e);var d=sessionStorage.getItem(E),A=d!==n&&!window.matchMedia(`screen and (max-width: ${768}px)`).matches;document.body.classList.toggle(a,A);var c=sessionStorage.getItem(r);c&&document.body.style.setProperty("--sidebarWidth",`${c}px`);var p=/(Macintosh|iPhone|iPad|iPod)/.test(window.navigator.userAgent);document.documentElement.classList.toggle("apple-os",p);})();
</script>

<div class="body-wrapper">

<button id="sidebar-menu" class="sidebar-button sidebar-toggle" aria-label="toggle sidebar" aria-controls="sidebar">
  <i class="ri-menu-line ri-lg" title="Collapse/expand sidebar"></i>
</button>

<nav id="sidebar" class="sidebar">

  <div class="sidebar-header">
    <div class="sidebar-projectInfo">

      <div>
        <a href="readme.html" class="sidebar-projectName" translate="no">
ex_esdb
        </a>
        <div class="sidebar-projectVersion" translate="no">
          v0.5.0
        </div>
      </div>
    </div>
    <ul id="sidebar-list-nav" class="sidebar-list-nav" role="tablist" data-extras="guides"></ul>
  </div>
</nav>

<output role="status" id="toast"></output>

<main class="content page-extra" id="main" data-type="extras">
  <div id="content" class="content-inner">
    <div class="top-search">
      <div class="search-settings">
        <form class="search-bar" action="search.html">
          <label class="search-label">
            <span class="sr-only">Search documentation of ex_esdb</span>
            <input name="q" type="text" class="search-input" placeholder="Press / to search" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
          </label>
          <button type="submit" class="search-button" aria-label="Submit Search" tabindex="-1">
            <i class="ri-search-2-line ri-lg" aria-hidden="true"></i>
          </button>
          <button type="button" tabindex="-1" class="search-close-button" aria-hidden="true">
            <i class="ri-close-line ri-lg" title="Cancel search"></i>
          </button>
        </form>
        <div class="autocomplete">
        </div>
        <button class="icon-settings display-settings">
          <i class="ri-settings-3-line"></i>
          <span class="sr-only">Settings</span>
        </button>
      </div>
    </div>

<div id="top-content">
  <div class="heading-with-actions top-heading">
    <h1>Failure Handling in ExESDB</h1>


  </div>


<p>This guide provides a comprehensive overview of all failure handling strategies implemented in ExESDB, from individual process failures to cluster-wide outages. ExESDB is built on the BEAM's &quot;let it crash&quot; philosophy while providing robust recovery mechanisms at every level.</p><h2 id="table-of-contents" class="section-heading"><a href="#table-of-contents" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Table of Contents</span></h2><ol><li><a href="#failure-categories">Failure Categories</a></li><li><a href="#supervision-strategies">Supervision Strategies</a></li><li><a href="#node-level-failures">Node-Level Failures</a></li><li><a href="#network-partitions">Network Partitions</a></li><li><a href="#data-consistency">Data Consistency</a></li><li><a href="#worker-process-failures">Worker Process Failures</a></li><li><a href="#storage-failures">Storage Failures</a></li><li><a href="#configuration-and-monitoring">Configuration and Monitoring</a></li><li><a href="#recovery-procedures">Recovery Procedures</a></li><li><a href="#testing-failure-scenarios">Testing Failure Scenarios</a></li></ol><h2 id="failure-categories" class="section-heading"><a href="#failure-categories" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Failure Categories</span></h2><p>ExESDB handles failures across multiple dimensions:</p><h3 id="1-process-level-failures" class="section-heading"><a href="#1-process-level-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">1. Process-Level Failures</span></h3><ul><li><strong>Worker crashes</strong>: Individual GenServer processes failing</li><li><strong>Supervisor crashes</strong>: Supervisor trees failing</li><li><strong>Application crashes</strong>: Entire OTP applications going down</li></ul><h3 id="2-node-level-failures" class="section-heading"><a href="#2-node-level-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">2. Node-Level Failures</span></h3><ul><li><strong>Hard crashes</strong>: Sudden node termination (power loss, kill -9)</li><li><strong>Soft crashes</strong>: Graceful shutdowns and restarts</li><li><strong>Network isolation</strong>: Node becomes unreachable but continues running</li></ul><h3 id="3-cluster-level-failures" class="section-heading"><a href="#3-cluster-level-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">3. Cluster-Level Failures</span></h3><ul><li><strong>Split-brain scenarios</strong>: Network partitions causing multiple leaders</li><li><strong>Quorum loss</strong>: Insufficient nodes for consensus</li><li><strong>Data corruption</strong>: Storage-level integrity issues</li></ul><h3 id="4-storage-level-failures" class="section-heading"><a href="#4-storage-level-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">4. Storage-Level Failures</span></h3><ul><li><strong>Disk failures</strong>: Storage becoming unavailable</li><li><strong>Corruption</strong>: Data integrity violations</li><li><strong>Performance degradation</strong>: Slow storage affecting operations</li></ul><h2 id="supervision-strategies" class="section-heading"><a href="#supervision-strategies" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Supervision Strategies</span></h2><p>ExESDB uses a hierarchical supervision tree with different restart strategies for different components:</p><h3 id="root-supervision-tree" class="section-heading"><a href="#root-supervision-tree" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Root Supervision Tree</span></h3><pre><code class="makeup elixir" translate="no"><span class="nc">ExESDB.App</span><span class="w"> </span><span class="p" data-group-id="8071264923-1">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-1">)</span><span class="w">
</span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.System</span><span class="w"> </span><span class="p" data-group-id="8071264923-2">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-2">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">Phoenix.PubSub</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">Cluster.Supervisor</span><span class="w"> </span><span class="p" data-group-id="8071264923-3">(</span><span class="nc">LibCluster</span><span class="p" data-group-id="8071264923-3">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">PartitionSupervisor</span><span class="w"> </span><span class="p" data-group-id="8071264923-4">(</span><span class="nc">EmitterPools</span><span class="p" data-group-id="8071264923-4">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.Store</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.ClusterSystem</span><span class="w"> </span><span class="p" data-group-id="8071264923-5">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-5">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.Streams</span><span class="w"> </span><span class="p" data-group-id="8071264923-6">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-6">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.Snapshots</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.Subscriptions</span><span class="w"> </span><span class="p" data-group-id="8071264923-7">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-7">)</span><span class="w">
    </span><span class="err">├</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.GatewaySupervisor</span><span class="w"> </span><span class="p" data-group-id="8071264923-8">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-8">)</span><span class="w">
    </span><span class="err">└</span><span class="err">─</span><span class="err">─</span><span class="w"> </span><span class="nc">ExESDB.LeaderSystem</span><span class="w"> </span><span class="p" data-group-id="8071264923-9">(</span><span class="n">one_for_one</span><span class="p" data-group-id="8071264923-9">)</span></code></pre><h3 id="restart-strategies-by-component" class="section-heading"><a href="#restart-strategies-by-component" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Restart Strategies by Component</span></h3><table><thead><tr><th style="text-align: left;">Component</th><th style="text-align: left;">Strategy</th><th style="text-align: left;">Restart</th><th style="text-align: left;">Reason</th></tr></thead><tbody><tr><td style="text-align: left;"><strong>System</strong></td><td style="text-align: left;"><code class="inline">:one_for_one</code></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;">Core system components</td></tr><tr><td style="text-align: left;"><strong>ClusterSystem</strong></td><td style="text-align: left;"><code class="inline">:one_for_one</code></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;">Independent cluster services</td></tr><tr><td style="text-align: left;"><strong>Streams</strong></td><td style="text-align: left;"><code class="inline">:one_for_one</code></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;">Stream readers/writers are independent</td></tr><tr><td style="text-align: left;"><strong>GatewaySupervisor</strong></td><td style="text-align: left;"><code class="inline">:one_for_one</code></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;">Gateway API and workers</td></tr><tr><td style="text-align: left;"><strong>StreamsWriterWorker</strong></td><td style="text-align: left;"><code class="inline">:temporary</code></td><td style="text-align: left;"><code class="inline">:temporary</code></td><td style="text-align: left;">TTL-based lifecycle</td></tr><tr><td style="text-align: left;"><strong>StreamsReaderWorker</strong></td><td style="text-align: left;"><code class="inline">:temporary</code></td><td style="text-align: left;"><code class="inline">:temporary</code></td><td style="text-align: left;">On-demand workers</td></tr><tr><td style="text-align: left;"><strong>GatewayWorker</strong></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;"><code class="inline">:permanent</code></td><td style="text-align: left;">Core gateway functionality</td></tr></tbody></table><h3 id="worker-lifecycle-management" class="section-heading"><a href="#worker-lifecycle-management" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Worker Lifecycle Management</span></h3><p><strong>Stream Workers</strong>:</p><ul><li>Use <code class="inline">:temporary</code> restart to prevent infinite restart loops</li><li>Implement TTL-based shutdown for resource management</li><li>Automatically clean up Swarm registrations on exit</li></ul><p><strong>Gateway Workers</strong>:</p><ul><li>Use <code class="inline">:permanent</code> restart for high availability</li><li>Register with Swarm for distributed load balancing</li><li>Handle graceful shutdown with cleanup</li></ul><h2 id="node-level-failures" class="section-heading"><a href="#node-level-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Node-Level Failures</span></h2><h3 id="hard-crash-detection-and-recovery" class="section-heading"><a href="#hard-crash-detection-and-recovery" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Hard Crash Detection and Recovery</span></h3><p>When a node crashes unexpectedly, multiple systems work together to detect and recover:</p><h4>1. NodeMonitor Service (Fast Detection)</h4><p><strong>Problem</strong>: Traditional Raft consensus timeouts can take 10-30 seconds
<strong>Solution</strong>: Proactive health monitoring with 6-second detection</p><p>This solution implements a comprehensive approach to quickly detect and handle node failures:</p><h3 id="1-nodemonitor-service" class="section-heading"><a href="#1-nodemonitor-service" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">1. NodeMonitor Service</span></h3><p><strong>Location</strong>: <code class="inline">lib/ex_esdb/node_monitor.ex</code></p><p><strong>Features</strong>:</p><ul><li><strong>Active Health Probing</strong>: Probes cluster nodes every 2 seconds</li><li><strong>Multi-Layer Health Checks</strong>: Verifies node connectivity + application health</li><li><strong>Fast Failure Detection</strong>: Marks nodes as failed after 3 consecutive probe failures (6 seconds total)</li><li><strong>Automatic Cleanup</strong>: Removes stale Swarm registrations from failed nodes</li><li><strong>Event-Driven Updates</strong>: Responds to <code class="inline">:nodeup</code>/<code class="inline">:nodedown</code> events</li></ul><p><strong>Configuration</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Default settings (configurable)</span><span class="w">
</span><span class="ss">probe_interval</span><span class="p">:</span><span class="w"> </span><span class="mi">2_000</span><span class="p">,</span><span class="w">     </span><span class="c1"># 2 seconds between probes</span><span class="w">
</span><span class="ss">failure_threshold</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">      </span><span class="c1"># 3 failures = node considered down</span><span class="w">
</span><span class="ss">probe_timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">1_000</span><span class="w">       </span><span class="c1"># 1 second timeout per probe</span></code></pre><h3 id="2-integration-with-existing-architecture" class="section-heading"><a href="#2-integration-with-existing-architecture" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">2. Integration with Existing Architecture</span></h3><p>The NodeMonitor integrates seamlessly with your current LibCluster setup:</p><p><strong>Modified Files</strong>:</p><ul><li><code class="inline">lib/ex_esdb/cluster_system.ex</code> - Added NodeMonitor to supervision tree</li><li>Uses existing <code class="inline">ClusterCoordinator</code> for split-brain prevention</li><li>Leverages current <a href="https://hexdocs.pm/swarm/3.4.0/Swarm.html"><code class="inline">Swarm</code></a> registrations for worker cleanup</li></ul><h3 id="3-how-it-works" class="section-heading"><a href="#3-how-it-works" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">3. How It Works</span></h3><h4>Health Probe Cycle (Every 2 seconds):</h4><ol><li><strong>Discover Nodes</strong>: Get cluster members from Khepri</li><li><strong>Probe Health</strong>: Test each node with RPC calls</li><li><strong>Track Failures</strong>: Increment failure count for unresponsive nodes</li><li><strong>Trigger Cleanup</strong>: Handle nodes that exceed failure threshold</li><li><strong>Update State</strong>: Maintain monitoring state for next cycle</li></ol><h4>Failure Detection:</h4><pre><code class="makeup elixir" translate="no"><span class="c1"># Multi-layer health check</span><span class="w">
</span><span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="nc">Node</span><span class="w"> </span><span class="ss">connectivity</span><span class="p">:</span><span class="w"> </span><span class="nc">:rpc</span><span class="o">.</span><span class="n">call</span><span class="p" data-group-id="6556146235-1">(</span><span class="n">node</span><span class="p">,</span><span class="w"> </span><span class="ss">:erlang</span><span class="p">,</span><span class="w"> </span><span class="ss">:node</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="6556146235-2">[</span><span class="p" data-group-id="6556146235-2">]</span><span class="p" data-group-id="6556146235-1">)</span><span class="w">
</span><span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="nc">Application</span><span class="w"> </span><span class="ss">health</span><span class="p">:</span><span class="w"> </span><span class="nc">Check</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">running</span><span class="w">
</span><span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="nc">Failure</span><span class="w"> </span><span class="ss">tracking</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">consecutive</span><span class="w"> </span><span class="n">failures</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="n">down</span></code></pre><h4>Automatic Cleanup:</h4><pre><code class="makeup elixir" translate="no"><span class="c1"># When a node is detected as failed:</span><span class="w">
</span><span class="mi">1</span><span class="o">.</span><span class="w"> </span><span class="nc">Remove</span><span class="w"> </span><span class="nc">Swarm</span><span class="w"> </span><span class="n">worker</span><span class="w"> </span><span class="n">registrations</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="n">node</span><span class="w">
</span><span class="mi">2</span><span class="o">.</span><span class="w"> </span><span class="nc">Update</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="n">state</span><span class="w"> </span><span class="p" data-group-id="8114065855-1">(</span><span class="n">notify</span><span class="w"> </span><span class="n">other</span><span class="w"> </span><span class="n">components</span><span class="p" data-group-id="8114065855-1">)</span><span class="w">
</span><span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="nc">Clean</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">subscriptions</span><span class="w"> </span><span class="n">tied</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="n">node</span><span class="w">
</span><span class="mi">4</span><span class="o">.</span><span class="w"> </span><span class="nc">Log</span><span class="w"> </span><span class="n">failure</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">monitoring</span><span class="o">/</span><span class="n">alerting</span></code></pre><h3 id="4-benefits" class="section-heading"><a href="#4-benefits" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">4. Benefits</span></h3><p><strong>Fast Detection</strong>: </p><ul><li>Traditional Raft consensus timeout: 10-30 seconds</li><li>This solution: 6 seconds (3 failures × 2 second intervals)</li></ul><p><strong>Proactive Cleanup</strong>:</p><ul><li>Prevents requests to unavailable workers</li><li>Maintains cluster integrity</li><li>Enables faster recovery</li></ul><p><strong>Graceful Degradation</strong>:</p><ul><li>System continues operating with remaining nodes</li><li>Workers redistribute automatically via Swarm</li><li>No single point of failure</li></ul><h3 id="5-usage" class="section-heading"><a href="#5-usage" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">5. Usage</span></h3><h4>Check Cluster Health:</h4><pre><code class="makeup elixir" translate="no"><span class="c1"># Get current health status</span><span class="w">
</span><span class="nc">ExESDB.NodeMonitor</span><span class="o">.</span><span class="n">health_status</span><span class="p" data-group-id="7981888003-1">(</span><span class="p" data-group-id="7981888003-1">)</span><span class="w">
</span><span class="c1"># Returns: %{</span><span class="w">
</span><span class="c1">#   monitored_nodes: [:node1@host, :node2@host],</span><span class="w">
</span><span class="c1">#   node_failures: %{},</span><span class="w">
</span><span class="c1">#   last_seen: %{:node1@host =&gt; 1625567890123},</span><span class="w">
</span><span class="c1">#   threshold: 3</span><span class="w">
</span><span class="c1"># }</span></code></pre><h4>Manual Node Probe:</h4><pre><code class="makeup elixir" translate="no"><span class="c1"># Force probe a specific node</span><span class="w">
</span><span class="nc">ExESDB.NodeMonitor</span><span class="o">.</span><span class="n">probe_node</span><span class="p" data-group-id="1631648428-1">(</span><span class="ss">:node1@host</span><span class="p" data-group-id="1631648428-1">)</span><span class="w">
</span><span class="c1"># Returns: :healthy or :unhealthy</span></code></pre><h3 id="6-configuration-options" class="section-heading"><a href="#6-configuration-options" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">6. Configuration Options</span></h3><p><strong>Environment Variables</strong> (can be added to your config):</p><pre><code class="makeup elixir" translate="no"><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:node_monitor</span><span class="p">,</span><span class="w">
  </span><span class="ss">probe_interval</span><span class="p">:</span><span class="w"> </span><span class="mi">2_000</span><span class="p">,</span><span class="w">      </span><span class="c1"># How often to probe (ms)</span><span class="w">
  </span><span class="ss">failure_threshold</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">       </span><span class="c1"># Failures before marking as down</span><span class="w">
  </span><span class="ss">probe_timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">1_000</span><span class="p">,</span><span class="w">       </span><span class="c1"># Timeout per probe (ms)</span><span class="w">
  </span><span class="ss">cleanup_stale_workers</span><span class="p">:</span><span class="w"> </span><span class="no">true</span><span class="w"> </span><span class="c1"># Auto-cleanup Swarm registrations</span></code></pre><h3 id="7-monitoring-and-observability" class="section-heading"><a href="#7-monitoring-and-observability" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">7. Monitoring and Observability</span></h3><p><strong>Log Messages</strong>:</p><ul><li><code class="inline">NodeMonitor started with 2000ms intervals</code></li><li><code class="inline">Health probe failed for node1@host (2/3)</code></li><li><code class="inline">Node node1@host detected as failed, initiating cleanup</code></li><li><code class="inline">Cleaning up Swarm registration: {:gateway_worker, node1@host, 1234}</code></li></ul><p><strong>Integration Points</strong>:</p><ul><li>Logs can be forwarded to your monitoring system</li><li>Health status can be exposed via HTTP endpoints</li><li>Alerts can be triggered on node failures</li></ul><h3 id="8-advanced-features-future-extensions" class="section-heading"><a href="#8-advanced-features-future-extensions" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">8. Advanced Features (Future Extensions)</span></h3><p>The solution is designed to be extensible:</p><p><strong>Planned Enhancements</strong>:</p><ul><li><strong>Forced Khepri Node Removal</strong>: Actively remove failed nodes from consensus</li><li><strong>Worker Redistribution</strong>: Trigger immediate rebalancing of Swarm workers</li><li><strong>Leader Election</strong>: Handle leader failures more aggressively</li><li><strong>Custom Health Checks</strong>: Application-specific health validation</li></ul><h3 id="9-deployment" class="section-heading"><a href="#9-deployment" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">9. Deployment</span></h3><p><strong>No Breaking Changes</strong>: </p><ul><li>Fully backward compatible with existing cluster</li><li>Can be deployed incrementally (node by node)</li><li>Falls back gracefully if monitoring fails</li></ul><p><strong>Resource Usage</strong>:</p><ul><li>Minimal CPU overhead (RPC calls every 2 seconds)</li><li>Low memory footprint (tracks failure state only)</li><li>Network traffic: ~1KB per node per probe</li></ul><h3 id="10-testing-the-solution" class="section-heading"><a href="#10-testing-the-solution" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">10. Testing the Solution</span></h3><p><strong>Chaos Engineering</strong>:</p><pre><code class="makeup bash" translate="no"><span class=""># Simulate hard crash
</span><span class="">docker kill ex-esdb-node1
</span><span class="">
</span><span class=""># Monitor logs for detection
</span><span class="">docker logs ex-esdb-node2 | grep NodeMonitor
</span><span class="">
</span><span class=""># Verify cleanup
</span><span class=""># Should see Swarm registrations removed within 6 seconds
</span></code></pre><p><strong>Expected Timeline</strong>:</p><ul><li>T+0: Node crashes</li><li>T+2s: First probe failure detected</li><li>T+4s: Second probe failure  </li><li>T+6s: Third probe failure, node marked as failed</li><li>T+6s: Cleanup initiated (Swarm registrations removed)</li><li>T+8s: Next probe cycle (failed node no longer monitored)</li></ul><h4>2. LibCluster Integration</h4><p><strong>Built-in Node Detection</strong>:</p><ul><li>Uses <code class="inline">:net_kernel.monitor_nodes(true, [:nodedown_reason])</code> for immediate notification</li><li>Gossip-based discovery helps detect network issues</li><li>Automatic cluster formation and healing</li></ul><h4>3. ClusterCoordinator</h4><p><strong>Split-Brain Prevention</strong>:</p><ul><li>Deterministic leader election (lowest node name)</li><li>Prevents multiple clusters from forming</li><li>Coordinates safe cluster joining</li></ul><p><strong>Features</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Prevent split-brain during network partitions</span><span class="w">
</span><span class="kd">def</span><span class="w"> </span><span class="nf">should_be_cluster_coordinator</span><span class="p" data-group-id="7537873235-1">(</span><span class="n">connected_nodes</span><span class="p" data-group-id="7537873235-1">)</span><span class="w"> </span><span class="k" data-group-id="7537873235-2">do</span><span class="w">
  </span><span class="n">all_nodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p" data-group-id="7537873235-3">[</span><span class="n">node</span><span class="p" data-group-id="7537873235-4">(</span><span class="p" data-group-id="7537873235-4">)</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">connected_nodes</span><span class="p" data-group-id="7537873235-3">]</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">sort</span><span class="p" data-group-id="7537873235-5">(</span><span class="p" data-group-id="7537873235-5">)</span><span class="w">
  </span><span class="n">node</span><span class="p" data-group-id="7537873235-6">(</span><span class="p" data-group-id="7537873235-6">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nc">List</span><span class="o">.</span><span class="n">first</span><span class="p" data-group-id="7537873235-7">(</span><span class="n">all_nodes</span><span class="p" data-group-id="7537873235-7">)</span><span class="w">
</span><span class="k" data-group-id="7537873235-2">end</span></code></pre><h3 id="graceful-shutdown-handling" class="section-heading"><a href="#graceful-shutdown-handling" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Graceful Shutdown Handling</span></h3><p>ExESDB handles graceful shutdowns through multiple mechanisms:</p><p><strong>Signal Handling</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># SIGTERM and SIGQUIT handling</span><span class="w">
</span><span class="nc">:os</span><span class="o">.</span><span class="n">set_signal</span><span class="p" data-group-id="5322542028-1">(</span><span class="ss">:sigterm</span><span class="p">,</span><span class="w"> </span><span class="ss">:handle</span><span class="p" data-group-id="5322542028-1">)</span><span class="w">
</span><span class="nc">:os</span><span class="o">.</span><span class="n">set_signal</span><span class="p" data-group-id="5322542028-2">(</span><span class="ss">:sigquit</span><span class="p">,</span><span class="w"> </span><span class="ss">:handle</span><span class="p" data-group-id="5322542028-2">)</span></code></pre><p><strong>Process Cleanup</strong>:</p><ul><li>Workers unregister from Swarm before termination</li><li>Subscription state is persisted</li><li>Transactions are completed or rolled back</li></ul><h2 id="network-partitions" class="section-heading"><a href="#network-partitions" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Network Partitions</span></h2><h3 id="split-brain-prevention" class="section-heading"><a href="#split-brain-prevention" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Split-Brain Prevention</span></h3><p>ExESDB implements multiple layers to prevent split-brain scenarios:</p><h4>1. ClusterCoordinator Logic</h4><ul><li><strong>Deterministic Election</strong>: Uses sorted node names for consistent leader selection</li><li><strong>Existing Cluster Detection</strong>: Searches for active clusters before forming new ones</li><li><strong>Coordinated Joining</strong>: Prevents multiple simultaneous cluster formations</li></ul><h4>2. Raft Consensus (Ra/Khepri)</h4><ul><li><strong>Majority Quorum</strong>: Requires majority of nodes for write operations</li><li><strong>Leader Election</strong>: Automatic failover when leader becomes unavailable</li><li><strong>Log Replication</strong>: Ensures consistency across cluster members</li></ul><h4>3. Partition Tolerance</h4><p><strong>Minority Partition Behavior</strong>:</p><ul><li>Nodes in minority partition become read-only</li><li>No new events can be written without quorum</li><li>Automatic healing when partition resolves</li></ul><p><strong>Majority Partition Behavior</strong>:</p><ul><li>Continues normal operations</li><li>Elects new leader if needed</li><li>Accepts new writes and maintains consistency</li></ul><h3 id="network-partition-recovery" class="section-heading"><a href="#network-partition-recovery" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Network Partition Recovery</span></h3><p><strong>Automatic Healing Process</strong>:</p><ol><li><strong>Detection</strong>: Nodes detect network connectivity restoration</li><li><strong>State Synchronization</strong>: Minority nodes sync with majority</li><li><strong>Conflict Resolution</strong>: Raft log reconciliation</li><li><strong>Service Restoration</strong>: Workers redistribute across all nodes</li></ol><h2 id="data-consistency" class="section-heading"><a href="#data-consistency" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Data Consistency</span></h2><h3 id="transaction-handling" class="section-heading"><a href="#transaction-handling" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Transaction Handling</span></h3><p>ExESDB provides ACID guarantees through Khepri transactions:</p><p><strong>Optimistic Concurrency Control</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="kd">def</span><span class="w"> </span><span class="nf">try_append_events</span><span class="p" data-group-id="8953885209-1">(</span><span class="n">store</span><span class="p">,</span><span class="w"> </span><span class="n">stream_id</span><span class="p">,</span><span class="w"> </span><span class="n">expected_version</span><span class="p">,</span><span class="w"> </span><span class="n">events</span><span class="p" data-group-id="8953885209-1">)</span><span class="w"> </span><span class="k" data-group-id="8953885209-2">do</span><span class="w">
  </span><span class="n">current_version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">get_current_version</span><span class="p" data-group-id="8953885209-3">(</span><span class="n">store</span><span class="p">,</span><span class="w"> </span><span class="n">stream_id</span><span class="p" data-group-id="8953885209-3">)</span><span class="w">
  
  </span><span class="k">if</span><span class="w"> </span><span class="n">current_version</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">expected_version</span><span class="w"> </span><span class="k" data-group-id="8953885209-4">do</span><span class="w">
    </span><span class="c1"># Proceed with append</span><span class="w">
    </span><span class="n">append_events_atomically</span><span class="p" data-group-id="8953885209-5">(</span><span class="n">store</span><span class="p">,</span><span class="w"> </span><span class="n">stream_id</span><span class="p">,</span><span class="w"> </span><span class="n">events</span><span class="p">,</span><span class="w"> </span><span class="n">current_version</span><span class="p" data-group-id="8953885209-5">)</span><span class="w">
  </span><span class="k" data-group-id="8953885209-4">else</span><span class="w">
    </span><span class="p" data-group-id="8953885209-6">{</span><span class="ss">:error</span><span class="p">,</span><span class="w"> </span><span class="ss">:wrong_expected_version</span><span class="p" data-group-id="8953885209-6">}</span><span class="w">
  </span><span class="k" data-group-id="8953885209-4">end</span><span class="w">
</span><span class="k" data-group-id="8953885209-2">end</span></code></pre><p><strong>Transaction Isolation</strong>:</p><ul><li>Uses Khepri's MVCC for concurrent access</li><li>Transactions are atomic and isolated</li><li>Automatic rollback on failures</li></ul><h3 id="conflict-resolution" class="section-heading"><a href="#conflict-resolution" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Conflict Resolution</span></h3><p><strong>Version Conflicts</strong>:</p><ul><li>Expected version mismatches return <code class="inline">:wrong_expected_version</code></li><li>Clients must retry with updated expected version</li><li>No automatic conflict resolution (explicit client handling)</li></ul><p><strong>Concurrent Writes</strong>:</p><ul><li>Only one writer per stream at a time</li><li>Serialized access through stream-specific workers</li><li>Queue management for high-throughput scenarios</li></ul><h2 id="worker-process-failures" class="section-heading"><a href="#worker-process-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Worker Process Failures</span></h2><h3 id="stream-worker-failure-handling" class="section-heading"><a href="#stream-worker-failure-handling" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Stream Worker Failure Handling</span></h3><h4>Writers (StreamsWriterWorker)</h4><p><strong>Lifecycle Management</strong>:</p><ul><li><code class="inline">:temporary</code> restart strategy prevents restart loops</li><li>TTL-based shutdown for resource management</li><li>Automatic Swarm cleanup on termination</li></ul><p><strong>Failure Scenarios</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># TTL-based shutdown</span><span class="w">
</span><span class="kd">def</span><span class="w"> </span><span class="nf">handle_info</span><span class="p" data-group-id="7094034584-1">(</span><span class="ss">:check_idle</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7094034584-2">%{</span><span class="ss">idle_since</span><span class="p">:</span><span class="w"> </span><span class="n">idle_since</span><span class="p" data-group-id="7094034584-2">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">state</span><span class="p" data-group-id="7094034584-1">)</span><span class="w"> </span><span class="k" data-group-id="7094034584-3">do</span><span class="w">
  </span><span class="n">writer_ttl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nc">Options</span><span class="o">.</span><span class="n">writer_idle_ms</span><span class="p" data-group-id="7094034584-4">(</span><span class="p" data-group-id="7094034584-4">)</span><span class="w">
  
  </span><span class="k">if</span><span class="w"> </span><span class="n">idle_since</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">writer_ttl</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">epoch_time_ms</span><span class="p" data-group-id="7094034584-5">(</span><span class="p" data-group-id="7094034584-5">)</span><span class="w"> </span><span class="k" data-group-id="7094034584-6">do</span><span class="w">
    </span><span class="nc">Process</span><span class="o">.</span><span class="n">exit</span><span class="p" data-group-id="7094034584-7">(</span><span class="n">self</span><span class="p" data-group-id="7094034584-8">(</span><span class="p" data-group-id="7094034584-8">)</span><span class="p">,</span><span class="w"> </span><span class="ss">:ttl_reached</span><span class="p" data-group-id="7094034584-7">)</span><span class="w">
  </span><span class="k" data-group-id="7094034584-6">end</span><span class="w">
  
  </span><span class="p" data-group-id="7094034584-9">{</span><span class="ss">:noreply</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p" data-group-id="7094034584-9">}</span><span class="w">
</span><span class="k" data-group-id="7094034584-3">end</span><span class="w">

</span><span class="c1"># Graceful cleanup on exit</span><span class="w">
</span><span class="kd">def</span><span class="w"> </span><span class="nf">handle_info</span><span class="p" data-group-id="7094034584-10">(</span><span class="p" data-group-id="7094034584-11">{</span><span class="ss">:EXIT</span><span class="p">,</span><span class="w"> </span><span class="c">_pid</span><span class="p">,</span><span class="w"> </span><span class="n">reason</span><span class="p" data-group-id="7094034584-11">}</span><span class="p">,</span><span class="w"> </span><span class="p" data-group-id="7094034584-12">%{</span><span class="ss">worker_name</span><span class="p">:</span><span class="w"> </span><span class="n">name</span><span class="p" data-group-id="7094034584-12">}</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">state</span><span class="p" data-group-id="7094034584-10">)</span><span class="w"> </span><span class="k" data-group-id="7094034584-13">do</span><span class="w">
  </span><span class="nc">Swarm</span><span class="o">.</span><span class="n">unregister_name</span><span class="p" data-group-id="7094034584-14">(</span><span class="n">name</span><span class="p" data-group-id="7094034584-14">)</span><span class="w">
  </span><span class="p" data-group-id="7094034584-15">{</span><span class="ss">:noreply</span><span class="p">,</span><span class="w"> </span><span class="n">state</span><span class="p" data-group-id="7094034584-15">}</span><span class="w">
</span><span class="k" data-group-id="7094034584-13">end</span></code></pre><h4>Readers (StreamsReaderWorker)</h4><ul><li>Similar TTL-based lifecycle</li><li>On-demand creation for read operations</li><li>Automatic cleanup when no longer needed</li></ul><h3 id="gateway-worker-failures" class="section-heading"><a href="#gateway-worker-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Gateway Worker Failures</span></h3><p><strong>High Availability Design</strong>:</p><ul><li><code class="inline">:permanent</code> restart strategy for critical gateway functions</li><li>Load balancing through random worker selection</li><li>Graceful failover to other gateway workers</li></ul><p><strong>Worker Distribution</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Random load balancing with fallback</span><span class="w">
</span><span class="kd">defp</span><span class="w"> </span><span class="nf">random_gateway_worker</span><span class="w"> </span><span class="k" data-group-id="9027558633-1">do</span><span class="w">
  </span><span class="k">case</span><span class="w"> </span><span class="nc">Swarm</span><span class="o">.</span><span class="n">members</span><span class="p" data-group-id="9027558633-2">(</span><span class="ss">:gateway_workers</span><span class="p" data-group-id="9027558633-2">)</span><span class="w"> </span><span class="k" data-group-id="9027558633-3">do</span><span class="w">
    </span><span class="p" data-group-id="9027558633-4">[</span><span class="p" data-group-id="9027558633-4">]</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> 
      </span><span class="c1"># Fallback to local gateway if no distributed workers</span><span class="w">
      </span><span class="nc">ExESDB.GatewayWorker</span><span class="w">
    </span><span class="n">workers</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> 
      </span><span class="n">workers</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nc">Enum</span><span class="o">.</span><span class="n">random</span><span class="p" data-group-id="9027558633-5">(</span><span class="p" data-group-id="9027558633-5">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">elem</span><span class="p" data-group-id="9027558633-6">(</span><span class="mi">1</span><span class="p" data-group-id="9027558633-6">)</span><span class="w">
  </span><span class="k" data-group-id="9027558633-3">end</span><span class="w">
</span><span class="k" data-group-id="9027558633-1">end</span></code></pre><h3 id="subscription-worker-failures" class="section-heading"><a href="#subscription-worker-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Subscription Worker Failures</span></h3><p><strong>&quot;Follow-the-Leader&quot; Pattern</strong>:</p><ul><li>Subscription workers automatically migrate to leader node</li><li>Persistent subscription state survives worker failures</li><li>Automatic restart and state recovery</li></ul><h2 id="storage-failures" class="section-heading"><a href="#storage-failures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Storage Failures</span></h2><h3 id="khepri-ra-storage-resilience" class="section-heading"><a href="#khepri-ra-storage-resilience" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Khepri/Ra Storage Resilience</span></h3><p><strong>Data Directory Management</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Configurable data directory</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:khepri</span><span class="p">,</span><span class="w">
  </span><span class="ss">data_dir</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;/data&quot;</span><span class="p">,</span><span class="w">
  </span><span class="ss">store_id</span><span class="p">:</span><span class="w"> </span><span class="ss">:reg_gh</span><span class="p">,</span><span class="w">
  </span><span class="ss">timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">2_000</span></code></pre><p><strong>Failure Scenarios</strong>:</p><h4>Disk Space Exhaustion</h4><ul><li><strong>Detection</strong>: Monitor disk usage in production</li><li><strong>Mitigation</strong>: Implement log compaction and cleanup</li><li><strong>Recovery</strong>: Restore from snapshots if available</li></ul><h4>Corruption Detection</h4><ul><li><strong>Checksums</strong>: Ra maintains data integrity checks</li><li><strong>Verification</strong>: Periodic consistency checks</li><li><strong>Recovery</strong>: Restore from cluster peers or backups</li></ul><h4>Performance Degradation</h4><ul><li><strong>Monitoring</strong>: Track operation latencies</li><li><strong>Alerting</strong>: Set thresholds for response times</li><li><strong>Mitigation</strong>: Scale storage or redistribute load</li></ul><h3 id="backup-and-recovery" class="section-heading"><a href="#backup-and-recovery" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Backup and Recovery</span></h3><p><strong>Snapshot Management</strong>:</p><ul><li>Regular snapshots of aggregate state</li><li>Version-based snapshot storage</li><li>Distributed snapshot replication</li></ul><p><strong>Disaster Recovery</strong>:</p><ol><li><strong>Data Loss Prevention</strong>: Multi-node replication</li><li><strong>Point-in-Time Recovery</strong>: Event replay from snapshots</li><li><strong>Cross-Region Backup</strong>: External backup strategies</li></ol><h2 id="configuration-and-monitoring" class="section-heading"><a href="#configuration-and-monitoring" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Configuration and Monitoring</span></h2><h3 id="health-check-endpoints" class="section-heading"><a href="#health-check-endpoints" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Health Check Endpoints</span></h3><p><strong>Cluster Health</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Check overall cluster status</span><span class="w">
</span><span class="nc">ExESDB.NodeMonitor</span><span class="o">.</span><span class="n">health_status</span><span class="p" data-group-id="9851611499-1">(</span><span class="p" data-group-id="9851611499-1">)</span><span class="w">

</span><span class="c1"># Check specific node</span><span class="w">
</span><span class="nc">ExESDB.NodeMonitor</span><span class="o">.</span><span class="n">probe_node</span><span class="p" data-group-id="9851611499-2">(</span><span class="ss">:node1@host</span><span class="p" data-group-id="9851611499-2">)</span><span class="w">

</span><span class="c1"># Get cluster members</span><span class="w">
</span><span class="nc">ExESDB.Cluster</span><span class="o">.</span><span class="n">members</span><span class="p" data-group-id="9851611499-3">(</span><span class="n">store_id</span><span class="p" data-group-id="9851611499-3">)</span></code></pre><p><strong>Performance Metrics</strong>:</p><ul><li>Operation latencies</li><li>Throughput measurements</li><li>Resource utilization</li><li>Error rates</li></ul><h3 id="logging-and-alerting" class="section-heading"><a href="#logging-and-alerting" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Logging and Alerting</span></h3><p><strong>Structured Logging</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="n">config</span><span class="w"> </span><span class="ss">:logger</span><span class="p">,</span><span class="w"> </span><span class="ss">:console</span><span class="p">,</span><span class="w">
  </span><span class="ss">format</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;$time $metadata[$level] $message</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w">
  </span><span class="ss">metadata</span><span class="p">:</span><span class="w"> </span><span class="p" data-group-id="2545689415-1">[</span><span class="ss">:mfa</span><span class="p" data-group-id="2545689415-1">]</span></code></pre><p><strong>Alert Categories</strong>:</p><ul><li><strong>Critical</strong>: Node failures, data corruption</li><li><strong>Warning</strong>: Performance degradation, high error rates</li><li><strong>Info</strong>: Normal operations, state changes</li></ul><h3 id="configuration-best-practices" class="section-heading"><a href="#configuration-best-practices" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Configuration Best Practices</span></h3><p><strong>Production Settings</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Timeouts</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:khepri</span><span class="p">,</span><span class="w">
  </span><span class="ss">timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">5_000</span><span class="w">  </span><span class="c1"># Increase for production</span><span class="w">

</span><span class="c1"># Node monitoring</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:node_monitor</span><span class="p">,</span><span class="w">
  </span><span class="ss">probe_interval</span><span class="p">:</span><span class="w"> </span><span class="mi">2_000</span><span class="p">,</span><span class="w">
  </span><span class="ss">failure_threshold</span><span class="p">:</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w">
  </span><span class="ss">probe_timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">1_000</span><span class="w">

</span><span class="c1"># Worker TTL</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:worker_idle_ms</span><span class="p">,</span><span class="w"> </span><span class="mi">300_000</span><span class="w">  </span><span class="c1"># 5 minutes</span></code></pre><p><strong>Development Settings</strong>:</p><pre><code class="makeup elixir" translate="no"><span class="c1"># Faster timeouts for development</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:khepri</span><span class="p">,</span><span class="w">
  </span><span class="ss">timeout</span><span class="p">:</span><span class="w"> </span><span class="mi">10_000</span><span class="w">

</span><span class="c1"># Shorter TTL for resource management</span><span class="w">
</span><span class="n">config</span><span class="w"> </span><span class="ss">:ex_esdb</span><span class="p">,</span><span class="w"> </span><span class="ss">:worker_idle_ms</span><span class="p">,</span><span class="w"> </span><span class="mi">60_000</span><span class="w">  </span><span class="c1"># 1 minute</span></code></pre><h2 id="recovery-procedures" class="section-heading"><a href="#recovery-procedures" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Recovery Procedures</span></h2><h3 id="manual-recovery-steps" class="section-heading"><a href="#manual-recovery-steps" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Manual Recovery Steps</span></h3><h4>Single Node Recovery</h4><ol><li><strong>Identify Issue</strong>: Check logs and monitoring</li><li><strong>Isolate Node</strong>: Remove from load balancer if needed</li><li><strong>Restart Service</strong>: Use graceful restart procedures</li><li><strong>Verify Health</strong>: Confirm cluster membership</li><li><strong>Restore Traffic</strong>: Gradually return to service</li></ol><h4>Cluster Recovery</h4><ol><li><strong>Assess Damage</strong>: Determine scope of failure</li><li><strong>Quorum Check</strong>: Ensure majority of nodes available</li><li><strong>Leader Election</strong>: Verify or trigger new leader election</li><li><strong>Data Integrity</strong>: Check for any corruption</li><li><strong>Service Validation</strong>: Test critical operations</li></ol><h3 id="automated-recovery" class="section-heading"><a href="#automated-recovery" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Automated Recovery</span></h3><p><strong>Self-Healing Mechanisms</strong>:</p><ul><li>Automatic process restarts via supervision</li><li>Worker redistribution through Swarm</li><li>Cluster reformation after partitions</li><li>Leader election on failures</li></ul><p><strong>Monitoring Integration</strong>:</p><ul><li>Health check failures trigger alerts</li><li>Automatic scaling based on load</li><li>Proactive maintenance scheduling</li></ul><h2 id="testing-failure-scenarios" class="section-heading"><a href="#testing-failure-scenarios" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Testing Failure Scenarios</span></h2><h3 id="chaos-engineering" class="section-heading"><a href="#chaos-engineering" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Chaos Engineering</span></h3><p><strong>Node Failures</strong>:</p><pre><code class="makeup bash" translate="no"><span class=""># Hard crash simulation
</span><span class="">docker kill ex-esdb-node1
</span><span class="">
</span><span class=""># Network partition simulation
</span><span class="">iptables -A INPUT -s &lt;node2_ip&gt; -j DROP
</span><span class="">iptables -A OUTPUT -d &lt;node2_ip&gt; -j DROP
</span><span class="">
</span><span class=""># Resource exhaustion
</span><span class="">stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 60s
</span></code></pre><p><strong>Service Failures</strong>:</p><pre><code class="makeup bash" translate="no"><span class=""># Stop specific services
</span><span class="">systemctl stop ex_esdb
</span><span class="">
</span><span class=""># Simulate disk failures
</span><span class="">fill-disk.sh /data 95%
</span><span class="">
</span><span class=""># Network latency injection
</span><span class="">tc qdisc add dev eth0 root netem delay 500ms
</span></code></pre><h3 id="test-scenarios" class="section-heading"><a href="#test-scenarios" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Test Scenarios</span></h3><h4>Scenario 1: Single Node Failure</h4><p><strong>Steps</strong>:</p><ol><li>Start 3-node cluster</li><li>Kill one node abruptly</li><li>Verify cluster continues operating</li><li>Check client request success rates</li><li>Restart failed node</li><li>Verify automatic rejoin</li></ol><p><strong>Expected Results</strong>:</p><ul><li>Cluster maintains quorum (2/3 nodes)</li><li>No data loss</li><li>Client operations continue</li><li>Failed node rejoins automatically</li></ul><h4>Scenario 2: Network Partition</h4><p><strong>Steps</strong>:</p><ol><li>Start 3-node cluster</li><li>Create network partition (2 vs 1 node)</li><li>Verify majority partition continues</li><li>Check minority partition becomes read-only</li><li>Heal partition</li><li>Verify automatic reconciliation</li></ol><p><strong>Expected Results</strong>:</p><ul><li>Majority partition elects new leader</li><li>Minority partition rejects writes</li><li>Healing triggers state synchronization</li><li>No data inconsistencies</li></ul><h4>Scenario 3: Leader Failure</h4><p><strong>Steps</strong>:</p><ol><li>Identify current leader</li><li>Kill leader node</li><li>Verify new leader election</li><li>Check subscription migration</li><li>Validate continued operations</li></ol><p><strong>Expected Results</strong>:</p><ul><li>New leader elected within timeout</li><li>Subscriptions migrate to new leader</li><li>Client operations resume</li><li>Worker redistribution occurs</li></ul><h3 id="monitoring-during-tests" class="section-heading"><a href="#monitoring-during-tests" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Monitoring During Tests</span></h3><p><strong>Key Metrics</strong>:</p><ul><li>Response times</li><li>Error rates</li><li>Memory usage</li><li>CPU utilization</li><li>Network connectivity</li><li>Disk I/O</li></ul><p><strong>Log Analysis</strong>:</p><pre><code class="makeup bash" translate="no"><span class=""># Monitor cluster events
</span><span class="">docker logs -f ex-esdb-node1 | grep -E &quot;(NodeMonitor|Cluster|Leader)&quot;
</span><span class="">
</span><span class=""># Check for errors
</span><span class="">docker logs ex-esdb-node1 | grep -i error
</span><span class="">
</span><span class=""># Monitor worker redistribution
</span><span class="">docker logs ex-esdb-node1 | grep -i swarm
</span></code></pre><h2 id="conclusion" class="section-heading"><a href="#conclusion" class="hover-link"><i class="ri-link-m" aria-hidden="true"></i></a><span class="text">Conclusion</span></h2><p>ExESDB provides comprehensive failure handling across all system levels, from individual process failures to cluster-wide outages. The system is designed with the BEAM's &quot;let it crash&quot; philosophy while ensuring data consistency and high availability through:</p><p><strong>Key Strengths</strong>:</p><ul><li><strong>Fast Failure Detection</strong>: 6-second node failure detection vs 10-30 second consensus timeouts</li><li><strong>Automatic Recovery</strong>: Self-healing mechanisms at every level</li><li><strong>Data Consistency</strong>: ACID guarantees through Raft consensus</li><li><strong>High Availability</strong>: No single points of failure</li><li><strong>Graceful Degradation</strong>: System continues operating with reduced capacity</li></ul><p><strong>Operational Benefits</strong>:</p><ul><li><strong>Reduced Downtime</strong>: Automatic failover and recovery</li><li><strong>Operational Simplicity</strong>: Minimal manual intervention required</li><li><strong>Predictable Behavior</strong>: Well-defined failure modes and recovery procedures</li><li><strong>Monitoring Integration</strong>: Comprehensive observability and alerting</li></ul><p><strong>Production Readiness</strong>:</p><ul><li>Battle-tested BEAM supervision principles</li><li>Proven Raft consensus implementation (Ra)</li><li>Comprehensive testing scenarios</li><li>Clear operational procedures</li></ul><p>This failure handling strategy ensures that ExESDB can operate reliably in production environments while maintaining the flexibility and resilience that makes BEAM-based systems ideal for distributed, fault-tolerant applications.</p>

</div>

<div class="bottom-actions" id="bottom-actions">
  <div class="bottom-actions-item">

      <a href="testing.html" class="bottom-actions-button" rel="prev">
        <span class="subheader">
          ← Previous Page
        </span>
        <span class="title">
Testing
        </span>
      </a>

  </div>
  <div class="bottom-actions-item">

      <a href="logger-filtering.html" class="bottom-actions-button" rel="next">
        <span class="subheader">
          Next Page →
        </span>
        <span class="title">
Logger Filtering
        </span>
      </a>

  </div>
</div>
    <footer class="footer">
      <p>

          <span class="line">
            <a href="https://hex.pm/packages/ex_esdb/0.5.0" class="footer-hex-package">Hex Package</a>

            <a href="https://preview.hex.pm/preview/ex_esdb/0.5.0">Hex Preview</a>

              (<a href="https://preview.hex.pm/preview/ex_esdb/0.5.0/show/guides/failure_handling.md">current file</a>)

          </span>

        <span class="line">
          <button class="a-main footer-button display-quick-switch" title="Search HexDocs packages">
            Search HexDocs
          </button>

            <a href="ex_esdb.epub" title="ePub version">
              Download ePub version
            </a>

        </span>
      </p>

      <p class="built-using">
        Built using
        <a href="https://github.com/elixir-lang/ex_doc" title="ExDoc" target="_blank" rel="help noopener" translate="no">ExDoc</a> (v0.38.2) for the

          <a href="https://elixir-lang.org" title="Elixir" target="_blank" translate="no">Elixir programming language</a>

      </p>

    </footer>
  </div>
</main>
</div>

  </body>
</html>
